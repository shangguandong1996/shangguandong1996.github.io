<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part19]]></title>
    <url>%2F2019%2F09%2F02%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part19%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part18]]></title>
    <url>%2F2019%2F09%2F02%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part18%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part17]]></title>
    <url>%2F2019%2F09%2F02%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part17%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part16]]></title>
    <url>%2F2019%2F09%2F02%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part16%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part15]]></title>
    <url>%2F2019%2F09%2F02%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part15%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part14]]></title>
    <url>%2F2019%2F09%2F02%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part14%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part13]]></title>
    <url>%2F2019%2F09%2F02%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part13%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part12]]></title>
    <url>%2F2019%2F06%2F05%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part12%2F</url>
    <content type="text"><![CDATA[我感觉到后面应该会讲的比较省略了，公式模板什么的套的比较多，因为主要是用来做考试复习和速查的。大家如果有什么疑问，可以在下面提出来。在这一部分，我也会注重把解题的步骤写出来（好像写出来是有分的）。其实我们假设检验的步骤就是，建立H0和H1，然后确定分布，然后确定我们的样本值以及更极端值所占有的比率，如果比例太小，说明这个样本值不常见，就可以拒绝H0，接受H1。 样本均值比较 样本均值的比较我们一般会涉及到 Z检验 和 t检验。Z检验针对的是总体方差已知的情况，t检验针对的总体方差未知的情况。一般来说，t检验更为的常见。 使用t检验，前体条件必须是样本均值的抽样分布符合正态分布。如果总体是正态分布，那么小样本的样本均值抽样分布也可以符合正态分布。如果总体不是正态分布，那么只有样本量达到一定大小，才可以符合正态分布。但一般来说，我们的考试生物学数据是符合正态分布的，而且课上也不提检验正态性，所以我这里不说检验正态分布了。后面ANOVA就提到了检验正态性。。。。。 当然，某些生物学也是不符合正态性的，就要考虑用非参数检验了。 Z检验 Z检验就是根据样本值，得到样本值的Z-score，然后计算概率。 单样本均值比较，即与某个数字进行比较的话，就是 \[ z= \frac{\bar{x}-\mu}{\sigma/\sqrt{n}} \] 两样本的均值比较的话，就是 \[ z=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}} \] 举个例子： 假设我们已知总体身高符合正态分布，且标准差已知为5，那么我们的样本数据为……（这里放上一堆数字，总共为20）。那么我们想要检验总体均值是否8。 步骤为： 我们建立原假设和备则假设，并设置显著性\(\alpha=0.05\) \[ H_0:\mu=8\quad H_1:\mu \neq8 \] 然后计算p-value 123456789101112131415161718192021# 模拟数字&gt; data &lt;- rnorm(20,mean = 8,sd = 5)&gt; data [1] 10.688820 7.462011 6.457040 6.146526 20.790506 9.610317 3.614535 5.224481 [9] 16.044720 8.231625 5.929559 13.817802 8.168671 3.331038 7.902722 7.818987[17] -4.585604 5.304461 3.261386 11.483466# 计算样本均值和标准差&gt; mean(data)[1] 7.835154&gt; sd(data)[1] 5.286252# 计算z-score&gt; (mean(data)-8)*sqrt(20)/(sd(data))[1] -0.1394591# 计算p-value# 因为z-score &lt; 0，所以计算p-value是&gt; 2*pnorm(-0.1394591)[1] 0.8890874 由于p-value &gt; 0.05，所以接受H0。即认为总体均值是等于8的。 首先要注意单尾和双尾的问题，如果H1是不等于，就是双尾。H1是大于或者小于，就是单尾。单尾的话，p-value不用乘以2了。 z-score这里手算的话，要注意z-score的正负，如果是负的话是2*pnorm(z-score)。如果是正的话，就是2*（1-pnorm(z-score)） t检验 单样本的t检验 \[ t=\frac{\bar{x}-\mu}{s/\sqrt{n}} \] 配对样本的t检验 配对样本的t检验，本质上就是配对样本对应值之差的单样本检验。所以也是一样的公式 独立两样本的t检验——方差相等 \[ t = \frac{(x_1-x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_p^2}{n_1}+\frac{s_p^2}{n_2}}} \] \[ s_p^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2} \] t分布的自由度为 \[ df=n_1+n_2-2 \] 独立两样本的t检验——方差不相等 \[ t=\frac{(x_1-x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}} \] t分布的自由度为： \[ df = \frac{(s_1^2/n_1+s_2^2/n2)^2}{\frac{(s_1^2/n_1)^2}{n_1-1}+\frac{(s_2^2/n_2)^2}{n_2-1}} \] t分布的步骤还是跟z差不多的，只要注意写上H0和H1就行了。不过不同的是，需要记得去检验方差齐性。方差齐性的R函数是var.test。方差齐性检验完了，如果是齐性的，就在t.test 里面设置 var.equal=T。 举个例子（这里我不写H0，H1了） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 生成数据&gt; t_data1 &lt;- rnorm(20)&gt; t_data2 &lt;- rnorm(20)# 先确定是不是配对数据，我们先假设是配对的&gt; t.test(t_data1,t_data2,paired = T) Paired t-testdata: t_data1 and t_data2t = 0.82025, df = 19, p-value = 0.4222alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: -0.3972871 0.9093616sample estimates:mean of the differences 0.2560372 # 也可以假设是不配对的# 然后确定你的H1假设是单尾还是双尾，然后调整# 我们假设是双尾，即两者均值不等——双尾其实是默认值t.test(t_data1,t_data2,alternative = "two.sided")# 然后要做方差齐性检验（这里也要写H0和H1，即假设方差是否相等）&gt; var.test(t_data1,t_data2) F test to compare two variancesdata: t_data1 and t_data2F = 1.6712, num df = 19, denom df = 19, p-value = 0.2719alternative hypothesis: true ratio of variances is not equal to 195 percent confidence interval: 0.6614761 4.2221719sample estimates:ratio of variances 1.671187 # 做完方差齐性之后，根据结果，设置var.equal参数&gt; t.test(t_data1,t_data2,var.equal = T) Two Sample t-testdata: t_data1 and t_data2t = 0.77421, df = 38, p-value = 0.4436alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: -0.4134474 0.9255219sample estimates: mean of x mean of y 0.21364009 -0.04239716 样本方差比较 单样本方差比较 对于单样本的方差比较，我们用卡方分布。卡方统计量为 \[ \chi^2=\frac{(n-1)s^2}{\sigma^2} \] 例子就用PPT上这张图 PPT这里的零假设是方差等于35 只不过这里p值的计算可以利用R来做，不用查表 123# 还是双端&gt; 2*pchisq(2.103,9)[1] 0.02053599 关于不同情况下的双端计算，可以看这张PPT。 两样本方差比较 对于两样本的方差比较，我们用F检验。 F分布的定义为设随机变量 \(X_1 \sim \chi^2(m)\)，\(X_2 \sim \chi^2(n)\)，X1与X2独立。则称 \(F=\frac{X1/m}{X2/n}\)的分布是自由度为m与n的F分布，记为 \(F\sim F(m,n)\)。这个定义恰好适用与我们的两样本比较。 检验过程用PPT表示： 当然，在R里面，你直接用var.test就可以了。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part11]]></title>
    <url>%2F2019%2F06%2F05%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part11%2F</url>
    <content type="text"><![CDATA[中心极限定理 中心极限定理 假设我们有一个总体，我们从总体中取出一个大小为5的样本。我们可以利用这个样本均值、方差来估计总体的均值，方差。而如果我们不断地从总体中取出 n=5 的样本，然后每次都计算抽取样本的均值，就可以形成一个样本均值的抽样分布。 中心极限定理告诉我们，如果总体是呈现正态分布，或者样本的大小足够大，那么样本均值的抽样分布就会呈现正态分布。 可以见下图（图来源于Y叔的统计学笔记，文末给出链接） 可以看到，如果样本量足够大，哪怕总体是来源于一个再疯狂的分布，样本均值的抽样分布都会呈现一个正态分布，但如果样本量不够，则总体必须是正态分布，样本均值的抽样分布才是正态分布的。 多大算大呢，一般的thumb认为是30。当然，这也只是个经验。具体的大小还得依赖于你总体的分布。如果你总体的分布很像正态分布，自然样本量小点也可以达到效果。 标准误 对于单个样本而言，比如说我们取了5株苗。我们就会用标准差（standard deviation, SD）来衡量样本的离散程度。但对于我们上面提到的多个均值得到的分布（样本均值的抽样分布），我们也需要衡量分布的离散程度，这时候我们就会用标准误（Stand error，SE 或者说 standard error of the means，SEM）来衡量。样本均值抽样分布的标准误计算为： \[ \sigma_\bar{x}=SE=\frac{\sigma}{\sqrt{n}} \] 即用总体的标准差除以样本量的平方根。但通常来说，我们是不知道总体的方差的，所以通常会用样本的方差来估计，那么 \[ s_{\bar{x}}=\frac{s}{\sqrt{n}} \] 置信区间（confidence interval） 通常来说，我们会用总体参数的点估计（比如参数的均值）来代表我们对总体参数的估计。但实际上，度量一个点估计的精读更直观的方法就是给出未知参数的一个区间。我们通常会设定一个 \(\alpha\) 值，把 \(1-\alpha\) 叫做置信水平。比如我们通常会设置 95% 的置信水平。置信水平的频率解释就是，我们利用我们构建置信区间的方法，不断地重复构建置信区间，比如说构建1000次。这样我们就得到了1000个置信区间，每次得到的区间都是不一样的。置信区间是否包含我们总体参数（即真值）的结果是一个二元的，即包含或者不包含。这样最终差不多就会有950个置信区间包含了我们的真值，另外50个不包含真值。 可以看下面的图，每根线都是我们构建的置信区间。绿线代表包含了真值，红线代表没有包含真值。（图片来自：Data Analysis for the Life Sciences） 均值的置信区间 先放一段我个人认为的均值置信区间构建的原理，不保证正确。可以不看直接看后面置信区间的公式（公式考试的时候好像还是要写的）： 我们设置95%为置信水平。我们从总体中得到的了一个样本均值，这个样本均值是样本均值抽样分布（假设是正态分布）的一个点。我们可以认为这个样本均值点应该是在距离真值95%范围内的。双侧95%的那个阈值点就是1.96，所以样本均值距离真值应该是1.96个标准误之内的 \[ -1.96\le\frac{\mu-\bar{x}}{\sigma_{x}}\le1.96 \] \[ -1.96{\sigma_{x}} \le \mu-\bar{x} \le 1.96{\sigma_{x}} \] \[ -1.96{\sigma_{x}}+\bar{x} \le \mu \le 1.96{\sigma_{x}}+\bar{x} \] 现在讲公式： 样本均值分布呈现正态分布的情况下，可以使用正态分布和t分布来估计置信区间，用哪种方法，取决于总体参数 σ 是否已知。 知道总体标准差的情况下，我们使用正态分布 \[ (\bar{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) &lt; \mu &lt; (\bar{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) \] \(Z_{\alpha/2}\)就是所谓的critical value，跟你设置的置信水平有关系，比如你是95%的置信水平，双侧的话，就是一边在2.5%，一边在97.5%。那么就是-1.96和1.96了。 用R算critical value就是应用我们之前讲过的dpqr中的q了。 1234&gt; qnorm(0.975)[1] 1.959964&gt; qnorm(0.025)[1] -1.959964 不知道总体标准差的情况下，我们使用 t 分布 \[ (\bar{x}-t_{\alpha/2}\frac{s}{\sqrt{n}}) &lt; \mu &lt; (\bar{x}+t_{\alpha/2}\frac{s}{\sqrt{n}}) \] 之所以t分布的使用要满足样本均值分布呈现正态分布，是因为t分布的建立要求之一就是正态分布。具体可以去看概率论与数理统计的书。如果不满足这个条件，就不能使用t分布。 用 t 分布来计算置信区间的话，可以用R的t.test，会直接输出置信区间。 123456789101112131415&gt; data &lt;- rnorm(20)# 改下置信区间为90%&gt; t.test(data,conf.level = 0.9) One Sample t-testdata: datat = -0.59555, df = 19, p-value = 0.5585alternative hypothesis: true mean is not equal to 090 percent confidence interval: -0.5063434 0.2469069sample estimates: mean of x -0.1297183 有可能还会要你算边际误差（margin of error，E，也称误差幅度） 边际误差就是所谓的 \[ E=z_{\alpha/2}\sigma_{\bar{x}}=z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \] 其实就是用来构建置信区间的。 当然，如果总体标准差不知道的话，就用样本标准差代替，分布变成t分布。 \[ E=t_{\alpha/2}\frac{s}{\sqrt{n}} \] 可以看到，如果想要降低E，即缩短置信区间，最稳妥的方法就是增大n。即提高样本容量。 比例的置信区间 这部分来自于Y叔的统计笔记。我感觉写的很直观，就直接放了。 比例的置信区间也差不多，公式在下面 \[ (\hat{p} - E) &lt; p &lt; (\hat{p} + E) \] 其中 E 是边际误差，\(\hat{p}\) 是算出来的比例，而p是总体比例。E通过下面的式子 \[ E = z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \] 因为这种情况是符合二项分布的，而n有比较大，所以用正态分布来估计。 比如我们检测了829个成年人，51%反对修铁路。问总体上有大约多少是反对修铁路的。 首先，我们先检查这个二项分布是否符合正态分布的近似。发现，\(n\hat{p}=422.79 &gt; 5\)，\(n\hat{q}=406.21 &gt; 5\)。的确是可以用来近似的。 然后就计算 E： \[ E =1.96\sqrt{\frac{0.51*0.49}{829}} \] 就可以算出置信区间了。 方差和标准差的置信区间 我们如果是拿样本的标准差来估计总体的标准差的置信区间，就要用到卡方分布。 假设我们从正态分布的总体中每次抽出样本量为n的样本，计算样本的方差\(s^2\)。那么每次计算得到的\(\frac{(n-1)s^2}{\sigma^2}\)就会符合卡方分布。 \[ \chi^2 = \frac{(n-1)s^2}{\sigma^2} \] 因为卡方是不对称分布，所以置信区间也是不对称的，所以需要分别找出左侧和右侧的临界值。 假设我们抽取的n是100，那么自由度（degree of freedom）是99，我们要计算95%的置信区间，需要分别计算左侧0.025和右侧0.025的临界值： 1234&gt; qchisq(0.975,99)[1] 128.422&gt; qchisq(0.025,99)[1] 73.36108 这两个值被称为卡方左右值，\(\chi_{L}^2\)和\(\chi_{R}^2\)。那么标准差置信区间的计算就是 \[ \sqrt{\frac{(n-1)s^2}{\chi_{R}^2}} &lt; \sigma &lt; \sqrt{\frac{(n-1)s^2}{\chi_{L}^2}} \] 参考资料 Y叔的统计笔记]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part10]]></title>
    <url>%2F2019%2F06%2F03%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part10%2F</url>
    <content type="text"><![CDATA[前面一部分讲了些概率论的知识，这部分我们来讲讲课上讲过的随机变量及其分布。 可以把这一部分当作速查。 离散型随机变量 二项分布（Binomial Distributions） 如果记 X 为 n 重伯努利实验中成功（记为事件 A ）的次数，则 X 的可能取值为0，1……，n。记 p 为每次试验中 A 发生的概率，即 \(P(A)=p\)，则 \(P(\bar{A})=1-p\)。这个分布称为二项分布，记为\(X\sim{b(n,p)}\) 那么事件成功 k 次的概率就是 \[ P(X=k)=C_{n}^{k}p^k(1-p)^{n-k}\quad,k=0,1,……,n \] 关于组合数符号\(C_n^{k}和C_{k}^{n}\)写法一直有点争议，只要知道是怎么算就好。当然还有\(\binom{n}{k}\) 二项分布是一种常用的离散分布，比如： 检查 10 件产品，10 件产品中不合格的个数X服从二项分布 b（10，p），其中p为不合格率。 射击5次，5次命中次数Z服从二项分布b（5，p），其中p为射手的命中率。 二项分布的均值、方差（variance）、标准差为（Standard Deviation）： \[ \mu=np\\ \sigma^2=npq\\ \sigma=\sqrt{npq} \] 泊松分布（Poisson distribution） 泊松分布的概率分布列为： \[ P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda} \] 泊松分布只有一个参数，即 \(\lambda\)，\(\lambda &gt;0\)。记为 \(X\sim P(\lambda)\)。 泊松分布常与单位时间（或者单位面积、单位产品）等上的计数过程相联系，比如 一天中，来到某商场的顾客数目 某一服务设施在一定时间内受到的服务请求的次数 汽车站台的等候人数 泊松分布的均值（数学期望）和方差均是 \[ \mu=\sigma^2=\lambda \] 泊松分布还有一个非常实用的特性，即可以用泊松分布作为二项分布的一种近似。当二项分布n很大，p很小，而乘积 \(\lambda=np\) 的大小适中时候，可以用泊松分布近似。 根据课件上来说，一般是 \[ n &gt;= 100\\ np &lt;=10 \] 超几何分布（Hypergeometric distribution） 从一个有限总体中，进行不放回抽样往往会遇到超几何分布。 设有 N 件产品，其中有 M 件不合格品。若从中不放回地随机抽取 n 件，则其中含有的不合格的件数 X 服从超几何分布，记为 \(X\sim{h(n,N,M)}\)。超几何分布的概率分布列为 \[ P(X=k)=\frac{C_M^k C_{N-M}^{n-k}}{C_N^n} \] 超几何分布的数学期望和方差为： \[ \mu=n\frac{M}{N}\\ \sigma^2=\frac{nM(N-M)(N-n)}{N^2(N-1)} \] 当抽取个数远小于产品总数的时候，每次抽取后，总体中的不合格率 \(p=\frac{M}{N}\) 改变甚微，所以不放回的抽样就可以近似变成放回抽样。这时候超几何分布就可以用二项分布近似了。 跟我们相关的超几何分布的应用就是基因富集分析（enrichment analysis）。 基因富集常见的方法有 Fisher精确检验 超几何分布 二项分布 卡方检验 …… 关于基因富集的部分我们后面再讲。这里我只提下利用超几何分布来检验富集分析。 比如我们对根再生这个通路很感兴趣。我们想要知道这个通路在我们的差异基因中是不是显著富集的。我们得到了 2000 个差异基因，跟根再生通路相关的基因有50个。拟南芥全体基因有 25000 个，其中跟根再生通路相关的有 100 个。那么这里 25000 就是 N，100就是M。2000就是n，50就是k。然后我们就可以计算 p 值了。 当然，p值应该是要考虑加上极端值的累积概率。可能不单单是一个点的值。即应该用1-phyper而不是dphyper。后面差异富集部分一起讲吧。 连续型随机变量 正态分布（Normal Distributions） 一个随机变量如果是由大量微小的、独立的随机因素的叠加结果，那么这个变量一般都可以认为服从正态分布。比如人的身高、测量误差等。 正态分布的密度函数和分布函数太长了，就不写了，可以自己去翻阅PPT。正态分布记为 \(X\sim{N(\mu,\sigma^2)}\) 正态分布还可以转换成标准正态分布： 若随机变量 \(X\sim{N(\mu,\sigma^2)}\)，则 \(U=(X-\mu)/\sigma \sim {N(0,1)}\) 正态分布的应用应该就是后面要讲到的 t-test，所以这里就不讲了。 还有一点就是当前面的二项分布的 \[ np\ge5\\ nq\ge5 \] 就是用正态分布来近似二项分布， \[ \mu=np \\ \sigma=\sqrt{npq} \] 计算 分布函数、概率分布列、概率密度函数 分布函数就是累积分布函数（Cumulative Distribution Function，CDF），指的就是小于等于 a 的值出现概率的和。具有累积特性。比如对于标准正态分布而言，到负无穷到 0 为止的概率和就是0.5。常表示为： \[ F(a)=P(X \le a) \] 就像下图就是正态分布的累积分布函数图 概率分布列，或者说概率质量函数（probability mass function，PMF），就是针对离散型变量而言，离散型变量在特定取值上的概率。 概率密度函数（probability density function，PDF）就是针对连续型变量而言。因为连续型变量在特定取值上的概率肯定是0，所以对连续型变量使用概率分布列是没有意义的。概率密度函数曲线上的面积就是概率值。 下图就是正态分布的概率密度函数图 这个短暂篇幅不太好讲，如果想再深入，建议看书。 R实现 R的实现可以用《R语言实战》第二版的第90页的这张图表示： dpqr加上对应的分布缩写，就可以变成任一的概率函数了。让我们来稍微解释下dpqr d开头的密度函数应该是包含了离散型随机变量的概率分布列，连续型随机变量的概率密度函数。 比如我们想要算二项分布的概率分布列。以扔硬币为例，扔3次硬币，每次朝上的概率为0.5。 123456789# 0,1,2,3次朝上的概率&gt; dbinom(0,3,0.5)[1] 0.125&gt; dbinom(1,3,0.5)[1] 0.375&gt; dbinom(2,3,0.5)[1] 0.375&gt; dbinom(3,3,0.5)[1] 0.125 我们想要看看正态分布的概率密度，比如我们想要看标准正态分布，0那点的概率密度是多少。（看上面的图，应该是0.4左右） 12&gt; dnorm(0)[1] 0.3989423 p开头的分布函数就是我们之前提到过的累积分布函数。你可以想象成，在累计分布函数曲线上的x轴上，你设定一个值，那个值所对应y值（累积概率）是多少。还是前面两个例子。 我们想要看看，掷3次硬币，扔到小于等于1次正面的概率。 1234567# 就是0次正面+1次正面&gt; dbinom(0,3,0.5) + dbinom(1,3,0.5)[1] 0.5# 直接用p函数算。&gt; pbinom(1,3,0.5)[1] 0.5 我们想要看看从负无穷到0为止，总共的概率和。恰好就是一半的概率。 12&gt; pnorm(0)[1] 0.5 q开头的分位数函数可以想象成，在累积分布函数的y轴上，你设定一个y值，那个y值（累积概率）所对应的x值是多少。 感觉分位数对于连续型变量比较常见。比如我们想要知道标准正态分布的97.5%分位点是多少，即曲线下面积是0.975的时候，所对应的x值。我们也可以说，这个x值比97.5%的值都大。 12&gt; qnorm(0.975)[1] 1.959964 r开头的就是生成各种类型的随机数了。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part9]]></title>
    <url>%2F2019%2F06%2F01%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part9%2F</url>
    <content type="text"><![CDATA[从这一部分开始，我们正式开始进入生统课上的内容。我们首先先讲讲陈洛南老师的概率论的部分。 陈老师这一部分似乎考的不太多，我就提的少点了。 描述性统计 集中趋势 描述集中趋势的有平均数、中位数、众数、偏斜度。 对应的 R 代码为 123456789101112131415161718# 准备数据data &lt;- sample(1:10,4)&gt; data[1] 7 10 2 4# 均值&gt; mean(data)[1] 5.75# 中位数&gt; median(data)[1] 5.5# 众数（R里面似乎没有单独的众数，我觉得可以用table）&gt; table(data)data 2 4 7 10 1 1 1 1 当然，最方便还是用 summary 123&gt; summary(data) Min. 1st Qu. Median Mean 3rd Qu. Max. 2.00 3.50 5.50 5.75 7.75 10.00 离散程度 描述离散程度的有极差、方差、标准差、变异系数 代码为 123456789101112131415161718192021222324# 极差（最大值减去最小值）## 方法1&gt; max(data) - min(data)[1] 8## 方法2&gt; range(data)[1] 2 10&gt; diff(range(data))[1] 8# 方差&gt; var(data)[1] 12.25# 标准差&gt; sd(data)[1] 3.5# 变异系数# 变异系数是样本标准差除以样本均值再乘以100&gt; sd(data)*100/mean(data)[1] 60.86957 注意，R里面的 var 和 sd 对应计算的都是样本的方差和标准差，所以里面都是 n-1 。如果你想要计算总体的，可以再乘上 \(\frac{n-1}{n}\)。 概率 古典方法： 抽球问题看第一次作业就可以了。 概率性质： 当考虑概率的性质，比如加减的时候，只要画个维恩图就可以了。具体地看看第一次作业就可以了。 条件概率 条件概率： \[ P(A|B)=\frac{P(AB)}{P(B)} \] 全概率公式： \[ P(A)=\sum_{i=1}^{n}P(B_{i})P(A|B_{i}) \] 最简单的全概率公式： \[ P(A)=P(B)P(A|B)+P(\bar{B})P(A|\bar{B}) \] 比较偷懒，所以没打前提条件，具体地可以去看统计学的书。 独立性： 如果 A 和 Ｂ相互不影响，我们就说 A 和 B 是独立的。那么就会有 \[ P(A|B)=P(A)\\ P(B|A)=P(B)\\ P(AB)=P(A)P(B) \] 关于独立，可以去看看第一次作业的两个证明题。 条件概率计算的时候，只要注意把复杂的事件变成字母符号，然后按照公式一步步算就可以了。比如把下雨这个事件，称为 A 。带伞 这个事件称为 B，那么在下雨的情况下，带伞的概率就是 P(B|A) 了。 贝叶斯 贝叶斯公式： \[ P(B|A)=\frac{P(AB)}{P(A)}=\frac{P(A|B)P(B)}{P(B)P(A|B)+P(\bar{B})P(A|\bar{B})} \] 在贝叶斯公式中，我们称 P(B) 为 事件 B 的先验概率（priori probability），称 P(B|A) 为事件 B 的后验概率（ posterior probability），贝叶斯公式就是专门用来计算后验概率的。也是用来在已知结果的条件下，求出原因的概率。 我们可以举个例子。某地区居民的肝癌发病率为0.0004，现用甲胎蛋白法进行普查。医学研究表明，化验结果是存有错误的。已知患有肝癌的人，其化验结果 99% 呈现阳性，而没患肝癌的人其化验结果 99.9% 呈现阴性。现某人的检验结果呈现阳性，问其患肝癌的概率是多少。 记 B 为被检查者患有肝癌，A 为检查结果后为阳性。则 \[ P(B)=0.0004\quad P(\bar{B})=0.9996\\ P(A|B)=0.99\quad P(A|\bar{B})=1-0.999 \] 然后就可以利用贝叶斯公式： \[ P(B|A)=\frac{P(AB)}{P(A)}=\frac{P(A|B)P(B)}{P(B)P(A|B)+P(\bar{B})P(A|\bar{B})} \] 算出 P(B|A) = 0.284。可以看到真正患有癌症的患者只有不到 30%。那么我们该如何提高检测检验精度呢，一个方法就是复查，即提高人群中 P(B) 的比例。如果我们对首次检查得阳性的人群再进行复查，此时 P(B) = 0.284。再利用贝叶斯公式，就可以发现P(B|A) = 0.997了。 关于贝叶斯，陈老师上课还提到了一个有趣的点，即 假设我们的观测为 E（比如说我们的数据） 我们的假设为 R（比如说 基因 A 调控了 基因 B ） 我们通常做的 P-value 指的是 \(p(E|\bar{R})\) ，\(\bar{R}\) 是零假设。 但我们实际上想求得是，p(R|E)，所以真正的公式是 \[ p(R|E)=1-\frac{p(\bar{R})}{p(E)}p(E|\bar{R}) \] 所以，一定程度上，p-value越小，那么我们的 p(R|E) 就会越大了。 混淆矩阵 陈老师的课件里面出现了混淆矩阵，因为混淆矩阵在后面的统计检验、多重比较矫正、逻辑斯蒂回归等可能会用到，所以先提上来讲一讲。 先放上混淆矩阵： 混淆矩阵相关的定义为： 假阳性率（false positive rate）： 也称 I 型错误，即在没病的情况下，你能够预测出来病人是有病的比例，P（预测是有病|真实没病）。\(\frac{FP}{FP+TN}\) 假阴性率（false negative rate）： 即有病的情况下，你能够预测出来病人是没病的比例。\(\frac{FN}{TP+FN}\) 真阳性率： 即召回率（recall），称灵敏度，敏感性（sensitivity）。也称也是 1- II型错误，我感觉也可以称之为统计功效。即在有病的情况下，你能够预测出来病人有病的比例。\(\frac{TP}{TP+FN}\) 真阴性率： 即特异性（specificity），即没病的情况下，你能够预测出来病人是没病的比例。\(\frac{TN}{FP+TN}\) 准确度（accuracy）： 你预测都是对的比例，即病人有病的情况下，你成功预测他是有病的，病人没病的情况下，你成功预测他是没病的。\(\frac{TP+TN}{TP+TN+FP+FN}\) 精确度（precision ）： 也称预测阳性率。即你预测的有病病人中，有多少是真的有病的。\(\frac{TP}{FP+TP}\) 对于不同的机构，对于这些比例有着不同的要求。像对于医院来说，假阴性就必须非常非常低。 让我们带入数字来看下。假设我们我们用一种方法检验阿兹海默症，用的是 450 个病人，500 个正常人的样本。检验结果为。 假阳性率为：\(\frac{5}{500}\) 假阴性率：\(\frac{14}{450}\) 灵敏度：\(\frac{436}{450}\) 特异性：\(\frac{495}{500}\) 准确度：\(\frac{436+495}{950}\) 这里的灵敏度，就是对应我上面提到的：已知患有肝癌的人，其化验结果 99% 呈现阳性。而这里的特异性，就是对应我上面提到的：而没患肝癌的人其化验结果 99.9% 呈现阴性。 最后放两张我网上找来的图，以供大家自查： 参考资料： 概率论与数理统计教程第二版 特异度（specificity）与灵敏度（sensitivity） ROC和AUC]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part8]]></title>
    <url>%2F2019%2F05%2F23%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part8%2F</url>
    <content type="text"><![CDATA[我们前面讲完了数据读取、数据提取、数据读取、数据概述。这部分我们来讲讲跟剩下一些跟生统相关的函数与操作。 其实这部分我觉得掌握 order 函数就可以了，其他的 for、apply、function 我认为其实没啥必要掌握。因为生统数据一般都是才几行几列的数据，完全可以通过复制粘贴来完成这些操作。而这几个操作学习成本略大。反正生统考试也是代码抄到纸上。所以其他的函数我视情况再往上加。 排序函数 R 语言里面的排序相关的函数有三个，分别是 order、rank、sort。我们来看下这三个函数的使用。 1234567891011121314151617# 产生一个测试向量&gt; set.seed(19960521)&gt; test &lt;- sample(10, 5)&gt; test[1] 9 5 2 6 4# order&gt; order(test)[1] 3 5 2 4 1# rank&gt; rank(test)[1] 5 3 1 4 2# sort&gt; sort(test)[1] 2 4 5 6 9 大家看到结果可能会比较懵逼，让我们来一个个来看。 order order 实际上返回的是最小值，次小值，再次小值……次大值，最大值在原始数据中的位置。比如我们原始的数字是9,5,2,6,4。其中最小值是 2，那么 2 在原始数据中排在第 3 个位置。然后就可以 order 在结果中输出的第一个值就是 3，即代表原始数据中，最小值（2）在原始数据中的位置。次小值是4，在原始数据中排在第 5 个位置，那么order 函数输出的第二个值就是 5。 order 默认地排序是升序，即返回的是最小值，次小值，再次小值……次大值，最大值在原始数据中的位置。如果你在向量的前面加一个减号，那么就会变成降序，即返回的是最大值，次大值……次小值，最小值在原始数据中的位置。 12&gt; order(-test)[1] 1 4 2 5 3 大家看到在原始位置中的位置这个信息有没有感觉很有帮助。事实上，我们可以利用 order 返回给我们的信息来对数据进行排序。 12345&gt; test[order(test)][1] 2 4 5 6 9&gt; test[order(-test)][1] 9 6 5 4 2 order还可以对数据框、矩阵进行排序，还是拿我们之前病人的那个数据。不过我把 patientID 稍微更改一下。 1234567891011&gt; patientID &lt;- c(2, 2, 3, 1)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 2 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 1 52 Type1 Poor 我们可以根据病人的年龄进行排序。 1234567891011# 一步步来，先得到坐标索引&gt; order(patientdata$age)[1] 1 3 2 4# 再把坐标索引输进去&gt; patientdata[order(patientdata$age),] patientID age diabetes status1 2 25 Type1 Poor3 3 28 Type1 Excellent2 2 34 Type2 Improved4 1 52 Type1 Poor 我们还可以让数据框先根据病人的ID进行排序（升序）。排完序之后，然后再根据年龄进行排序（降序）。 1234567891011# 获取坐标索引&gt; order(patientdata$patientID,-patientdata$age)[1] 4 2 1 3# 再把坐标输进去&gt; patientdata[order(patientdata$patientID,-patientdata$age),] patientID age diabetes status4 1 52 Type1 Poor2 2 34 Type2 Improved1 2 25 Type1 Poor3 3 28 Type1 Excellent 我们可以看到，我们的病人数据框里面。ID是按升序进行排列的，然后再排age。可以看到ID 那边有 2 个数是相同的，即 2，2。单根据第一列排序是分不出先后的，那么我们就再根据 age 的顺序进行排列。 这种先根据什么，后根据什么排列对于处理生信相关的数据还是比较有用的。比如 peak 文件经常就是Chr，start，end，peakID这几列，我们就可以根据order来排列我们的Peak文件。但对于生统来说，可能只要掌握单列排序就行了。 之前有一题让我们提取出 p-value 最显著的前 10 个数据。我们就可以用 order 来做了。 12345678910111213141516171819202122# 有一大堆 p-value 值。&gt; head(test4_sig_result,20) gene12 gene37 gene49 gene62 gene95 gene101 gene104 gene129 0.0362923840 0.0121764651 0.0418676419 0.0329430116 0.0298363658 0.0105665102 0.0166156137 0.0017854211 gene131 gene135 gene152 gene177 gene178 gene186 gene217 gene239 0.0395431910 0.0439433653 0.0474609370 0.0340475106 0.0109452086 0.0170048804 0.0140824498 0.0087974170 gene250 gene277 gene279 gene282 0.0133802343 0.0007355859 0.0337127909 0.0280345683 ……茫茫多p-value# 提取出前10个## 先对这一大堆 p-value 进行排序test4_sig_result[order(test4_sig_result)]## 得到了有顺序的 p-value之后，利用 [] 提取&gt; test4_sig_result[order(test4_sig_result)][1:10] gene28801 gene27868 gene27438 gene21642 gene24019 gene12323 gene12962 gene28939 4.277344e-06 6.093302e-05 8.229606e-05 9.889894e-05 1.124717e-04 1.261658e-04 1.298200e-04 1.462493e-04 gene2387 gene18712 1.522920e-04 1.601297e-04 rank 12345&gt; test[1] 9 5 2 6 4&gt; rank(test)[1] 5 3 1 4 2 rank 实际上返回的是你原始数据中的每个值，在原始数据中的顺序（排名）。 打个比方，有 10 个人站成一排军训。order 函数会告诉教官，最矮的人站在哪里，次矮的人站在哪里，最高的人站在哪里。然后教官就可以根据 order 返回的信息，大声地说 XX号，站出来，排在第一位，XX号站出来，排在第二位。这样，教官一个个吼，最后这10个人就能从矮到高排好了。 而 rank 函数则是，有 10 个人站成一排军训，rank 函数则是会像一个检测器一样。每次经过一个人，就告诉教官，这个人在整个队列中，是排几矮的。就好比，检测器扫过第一个人，就会大声说出，这个人在这10个人中，是排在第2矮的。 比如我们这里数据，原始数据的顺序是9,5,2,6,4。9 在这 5 个数据中是最大的，就是第 5 小的。那么 rank 就会告诉我们 9，在原始数据中是排在第5位的。 sort 12345&gt; test[1] 9 5 2 6 4&gt; sort(test)[1] 2 4 5 6 9 sort 就相当于帮教官自动地排好了队列。 事实上，由于 order 不仅可以应用于向量，还可以应用于数据框，矩阵等等。所以应用场景也更多，我们生统也应该就用到它了。 for，function，apply 视情况而定。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part7]]></title>
    <url>%2F2019%2F05%2F19%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part7%2F</url>
    <content type="text"><![CDATA[到目前为止，我们已经讲完了如何读取数据，如何提取数据、如何整理数据，在这一部分，我们来讲讲如何对生统的数据有一个大致的描述。 这一部分参考了《R语言实战》的7.1部分 用数据的方式呈现 一般来说，常见的数据描述方式包括均值，方差，中位数，最大值，最小值等等。R 里面都有对应的函数来检验，比如 mean，var 等等。但为了快速地了解我们手头的数据，我们就可以用 summary 和 str 这两个函数。 我们以 test2 的数据集为例。 123456789101112131415161718192021222324252627282930# 读取test2 &lt;- read.table("rawdata/test2.txt",header = T)# head也是个查看数据的方式&gt; head(test2) control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.23# 使用Summary&gt; summary(test2) control low middle high Min. :17.22 Min. :18.64 Min. :22.29 Min. :24.07 1st Qu.:19.35 1st Qu.:20.39 1st Qu.:25.05 1st Qu.:29.21 Median :22.60 Median :22.69 Median :28.67 Median :32.63 Mean :21.98 Mean :23.23 Mean :28.13 Mean :32.84 3rd Qu.:23.96 3rd Qu.:25.69 3rd Qu.:29.95 3rd Qu.:36.14 Max. :27.21 Max. :29.67 Max. :35.12 Max. :39.76 # 使用str&gt; str(test2)'data.frame': 15 obs. of 4 variables: $ control: num 20.8 22.9 27.2 19.3 17.9 ... $ low : num 22.2 24.7 21.5 19.7 25.9 ... $ middle : num 28.6 28.7 25.3 30.3 23.1 ... $ high : num 31.9 37.9 39.8 27.9 29.6 ... 可以看到，summary 和 str 都是对每列的数据进行了描述。summary 偏向于描述数据的统计属性，比如最小值，最大值等等。str 偏向于描述数据的结构，比如我们可以看到数据总共是 15 行，4列。每列的数据都是数值型的数据。 让我们再来看看 test1 的数据集 123456789101112131415&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T)&gt; summary(test1) yield seed Min. :348 Min. :1.000 1st Qu.:362 1st Qu.:1.000 Median :376 Median :2.000 Mean :378 Mean :1.931 3rd Qu.:394 3rd Qu.:3.000 Max. :414 Max. :3.000 &gt; str(test1)'data.frame': 29 obs. of 2 variables: $ yield: int 383 406 351 400 390 361 394 395 414 382 ... $ seed : int 1 1 1 1 1 1 1 1 1 1 ... 当我们在用 Summary 和 str 看 test1 的数据集的时候，是不是感觉有那么一丝不太对。seed 那列的数据描述跟我们想的不太一样啊。事实上，还是因为没有把 seed 的那列变成因子型的变量导致的。让我们再次来做个转换。 12345678910111213&gt; test1$seed &lt;- factor(test1$seed)&gt; summary(test1) yield seed Min. :348 1:10 1st Qu.:362 2:11 Median :376 3: 8 Mean :378 3rd Qu.:394 Max. :414 &gt; str(test1)'data.frame': 29 obs. of 2 variables: $ yield: int 383 406 351 400 390 361 394 395 414 382 ... $ seed : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ... 这下子，是不是感觉好多了。 事实上，对于 test1 的数据集，还有一个问题就是在比较多组个体或观测时，关注的焦点经常是各组的描述性统计信息，而不是样本整体的描述性统计信息。 就像这里我们可能关注的是 test1 里面 seed1、seed2、seed3 这三种种子分别的数据类型，而不是整个 29 个观测的数据结果。关于分组变量，为了不加重大家的学习负担，我推荐可以用我们之前提到过的数据提取方法，从而分别地对数据进行描述。 因为生统的数据通常不会有多个变量这种复杂的数据结构，所以上面的方法已经足够了。如果碰到了多个变量，多个组，可以试试基本包的 aggregate() 函数 或者 tidyverse 包的 group_by 函数。 事实上，还有一种快速描述数据的函数，即 table 函数。 1234&gt; table(test1$seed) 1 2 3 10 11 8 可以看到table 可以快速地呈现各个处理所对应的重复数目。三种种子分别有10、11、8个重复。不过 table 似乎在列联表等中更为常见，这里先按下不表，后面在提到线性回归中可能会讲到。 用图画的方式呈现 人类是视觉动物，所以图画的呈现可能远比数据来的更直观。在生统中，我们通常遇见的还是 boxplot，即箱式图。箱式图对于数据格式的支持非常友好，其支持宽、长数据。 但值得注意的是，对于宽数据，直接把数据输入 boxplot 是没有问题的。因为你每列都代表一个单独的处理。虽然我不太推荐把宽数据作为画图的基本数据，因为其实在不符合各种画图包的格式，也不符合我们做线性回归、方差分析等的格式。这里 boxplot 能画可能只是一个个例。 对于长数据，我们就需要输入 formula，即告诉 boxplot 谁是 x 轴，谁是 y 轴。 让我们拿之前的 test1 和 test2数据来举例子。 12# test2的boxplotboxplot(test2) 123# test1的boxplot，我们以 yield 作为 y 轴，seed 作为 x 轴# 记得用 ？号看看 boxplot 的基本使用方法boxplot(yield ~ seed , data = test1) boxplot 的意义参见网上的解释 图片来源： Understanding Boxplots 你也可以画柱状图，用 hist 这个函数就行。这个上课应该都画过，不讲了。 顺便提一句，现在大家已经开始摈弃bar plot了，因为bar plot并不能给予我们太多的信息。相对应的，大家已经开始逐渐转向 box plot、dot plot（Scatter plot）、violin plot。如果对于画图有兴趣，大家可以去学学 ggplot2。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part6]]></title>
    <url>%2F2019%2F05%2F18%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part6%2F</url>
    <content type="text"><![CDATA[前面我们已经讲过了在生统课上会用到基本的数据结构以及怎么来提取我们想要的数据，这一部分我们来讲讲数据的清洗。 在生统课上，我们基本上会遇到两种数据结构。一种是我把其叫做宽数据，就比如我们在第五次生统作业中，碰到的第二题的数据。 12345678910&gt; test2 &lt;- read.table("rawdata/test2.txt",header = T)&gt; head(test2) control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.23 这种数据列名其实是不同的处理，每一列都对应不同处理下的值。 实际上，这种数据还不能称之为真正意义上的宽数据，这里拿来指代是为了方便与长数据区分。 另一种就是长数据，也就是我上一节翻来覆去提到过的数据结构。其每一行都是一个观测值，列名则是变量名。典型的就是我们在第五次生统作业中，碰到过的第一题的数据。 123456789&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T)&gt; head(test1) yield seed1 383 12 406 13 351 14 400 15 390 16 361 1 由于宽数据在线性回归，方差分析等分析中都无法被函数所识别，所以我们首先讲讲如何把宽数据转换成长数据。 gather函数转换 首先介绍的是 tidyr 包的 gather 函数。tidyr 包的安装就是我们之前讲过的方法 1install.packages("tidyr") 有点建议大家直接装 tidyverse 包，这是个各种包的合集，里面还包括了 ggplot2 等。不过有可能安不上 tidyverse ， 如果安装有问题，欢迎大家在下面提出问题。 gather 函数的使用非常方便，你只需要指定你转换后的 key 那列的列名，value 那列的列名，以及你需要转成长数据的那几列。我们以 test2 为例。 12345678910111213# 加载包library(tidyr)# 使用gahter&gt; test2_long &lt;- gather(test2, key = "Treatment_dose", value = "survive_time",control, low, middle, high)&gt; head(test2_long) Treatment_dose survive_time1 control 20.792 control 22.913 control 27.214 control 19.345 control 17.856 control 23.79 gather 函数你需要输入参数为 第一个参数是你的数据集 第二个参数是你新构建的关键列的名称（该列的内容由原先数据集的列名组成） 名字自己取，像我这里就取名为"Treatment_dose" 第三个参数是新构建的数值列的名称 名字还是自己取，我这里取名为"survive_time" 后面的几个参数都是你要用来构建关键列的那几个列名 这里就是control, low, middle, high，即原来的几个列名。 key 和 value 大家可能还是比较懵逼，但对于我们普通的生统数据，不需要太过于纠结其意义。 gather用法在转换的时候还要考虑uniq key的问题，但我们生统的数据应该也不需要考虑这一点。 也有人提到过用 reshape2 包的 melt 函数，或者基本包 transform 函数来转换。但我觉得没有 gather 这个函数直观，简单。还有，gather 转换的时候对于不等长数据的支持也比较好。就比如我们在第五次生统作业的第三题，药物 1 和 2 有 15 只小鼠，药物 3 只有 10 只老鼠。如果只是单纯地按我们之前的做法读入数据框，就会报错。 123&gt; test3 &lt;- read.table("rawdata/test3.txt",header = T)Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 11 did not have 3 elements 就是因为3列数据不等长，所以R才会报错。这时候，我们就可以设置一个参数 123456789101112131415161718&gt; test3 &lt;- read.table("rawdata/test3.txt",header = T,fill = T)&gt; test3 med1 med2 med31 40 50 602 10 20 303 35 45 1004 25 55 855 20 20 206 15 15 557 35 80 458 15 -10 309 -5 105 7710 30 75 10511 25 10 NA12 70 60 NA13 65 45 NA14 45 60 NA15 50 30 NA 然后就可以顺利地读入了，而且也可以顺利地用gather函数来整理成长数据。 1gather(test3,key = "different_med", value = "weight", med1,med2,med3) 会发现NA还是存在，但我们同样可以设置一个参数 1gather(test3,key = "different_med", value = "weight", med1,med2,med3,na.rm = T) 这样，就顺利地转换成我们需要的格式了。 基本包转换 其实，宽数据到长数据的转换，不一定需要特殊的包的函数，也可以用最基本的方法。尽管最基本的方法有些麻烦，但对于提高自己的数据转换能力还是很有帮助的。 rbind和cbind 在使用基本函数转换前，我们先来介绍两个我们以后可能会用到的函数，rbind 和 cbind。rbind是纵向合并，cbind是横向合并。具体操作我们来看一个例子 12345678910111213141516171819202122232425&gt; data1 &lt;- data.frame(A1 = sample(1:10,3),+ A2 = sample(1:10,3),+ A3 = sample(1:10,3))&gt; data1 A1 A2 A31 3 6 22 7 9 53 5 10 4&gt; data2 &lt;- data.frame(B1 = sample(10:20,3),+ B2 = sample(10:20,3),+ B3 = sample(10:20,3))&gt; data2 B1 B2 B31 15 17 142 13 14 113 17 15 12&gt; rbind(data1,data2)Error in match.names(clabs, names(xi)) : names do not match previous names&gt; cbind(data1,data2) A1 A2 A3 B1 B2 B31 3 6 2 15 17 142 7 9 5 13 14 113 5 10 4 17 15 12 可以看到rbind需要两个数据框有同样的变量（同样的列名），cbind则需要两个数据框的行数是一样的。 rbind尽管需要两个数据框有同样的变量，但顺序不一定要一样，比如 12345678910111213141516&gt; data3 &lt;- data.frame(A1 = sample(10:20,3),+ A3 = sample(10:20,3),+ A2 = sample(10:20,3))&gt; data3 A1 A3 A21 10 12 142 11 20 203 15 14 19&gt; rbind(data1,data3) A1 A2 A31 3 6 22 7 9 53 5 10 44 10 14 125 11 20 206 15 19 14 这里data1和data3的列名是一样的，但顺序是不一样的，但rbind还是可以合并。 关于rbind和cbind，《R语言实战》4.9也有提到，大家可以去看看 数据转换 介绍完了rbind和cbind，我们就可以来转换数据框了。我用test2做例子 1234567891011121314151617&gt; test2 control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.237 22.60 18.93 28.88 32.638 18.53 18.64 29.62 29.139 23.23 26.39 24.82 39.6210 20.14 25.49 34.64 36.1511 26.71 20.43 22.29 28.8512 19.36 22.69 29.22 24.0713 17.22 29.67 25.63 29.2914 24.13 20.36 35.12 35.2415 25.85 22.74 32.32 36.13 宽数据转换成长数据，本质上就像堆积木一样。你把每一列的数据拿出来，变成一块积木，然后你就一层层地堆起积木，最后就形成了长数据。是不是感觉特别像rbind干的事情？没错，我们这里就用rbind来构建长数据。 123456789control &lt;- data.frame(Treatment_dose = rep("control",15), survive_time = test2$control)low &lt;- data.frame(Treatment_dose = rep("low",15), survive_time = test2$low)middle &lt;- data.frame(Treatment_dose = rep("middle",15), survive_time = test2$middle)high &lt;- data.frame(Treatment_dose = rep("high",15), survive_time = test2$high)rbind(control,low,middle,high) 大家可能是一遍遍地打了以上的代码，机智的小伙伴可能还是复制粘贴的，然后把变量名改一下。不过，实际上我们可以用函数来解决这些重复操作的问题。函数这一部分就留待后面讲了。 双因素ANOVA的数据格式整理 在第五次生统作业的最后一题，我们需要考虑的是双因素ANOVA分析。但word文件里面的表格却不是一个双因素ANOVA的格式。让我们来看看如何利用上面讲过的内容，把它变成一个能让 aov 读入的双因素AONVA。 首先复制粘贴A1，A2，A3三列数据到txt文件中，然后读入R中。 123456789&gt; test4 &lt;- read.table("rawdata/test4.txt",header = T)&gt; head(test4) A1 A2 A31 282.1 296.7 300.12 264.2 318.0 307.53 274.2 295.3 294.24 276.4 292.8 312.05 283.7 304.5 300.26 288.0 305.9 292.6 然后转换成长数据框 1test4_long &lt;- gather(test4, key = temperature, value = weight, A1, A2, A3) 但这样还不够，这里只有单因素，即饲养温度这一列的信息，还没有饲料的信息。所以我们要自己加上一列饲料的信息。 123456789101112feed &lt;- c(rep("B1",10),rep("B2",10))test4_data &lt;- cbind(feed,test4_long)head(test4_data)&gt; head(test4_data) feed temperature weight1 B1 A1 282.12 B1 A1 264.23 B1 A1 274.24 B1 A1 276.45 B1 A1 283.76 B1 A1 288.0 这样就变成了双因素的ANOVA格式，可以顺利让aov函数读入了。 123456789&gt; test4_aov &lt;- aov(weight ~ feed * temperature, data = test4_data)&gt; summary(test4_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) feed 1 127 127 1.589 0.213 temperature 2 9080 4540 56.809 5.22e-14 ***feed:temperature 2 17 9 0.108 0.897 Residuals 54 4316 80 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part5]]></title>
    <url>%2F2019%2F05%2F11%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part5%2F</url>
    <content type="text"><![CDATA[之前我们已经了解了如何去提取行、列数据。这部分我们讲讲如何筛选自己想要的数据。生统常见的一个数据提取问题就是，提取某某处理的那部分数据进行一些检验。比如，在有3种种子的数据中，提取出1号种子对应的数据。这时候，尽管我们可以根据1号种子的数字索引来提取，但这终归不是一个好的方法，因为一旦数据是打乱的，我们就无法知道正确的数字索引，从而进行提取了。所以，这时候，我们就应该使用我们之前讲过的逻辑运算符来进行操作。 顺便提下，我们之前在介绍数据框的时候，把行叫做观测，而把列叫做变量。所以我们在提取行的时候，就是在提取我们感兴趣的观测，而在提取列的时候，就是在提取我们感兴趣的变量。为什么我要翻来覆去地说这个呢，是因为我觉得以行作为观测，列作为变量是一个比较好的呈现数据的方式，也是后面很多我们生统要用到的包需要的格式。也是后面我们在说长宽数据转换时候要再次提到的一点。我们再来看一下糖尿病人的例子。 1234567891011&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor 这里是 4 行，就是 4 个观测值，4 个病人。这里有 4 列，4 个变量，就是用 4 个不同地指标去衡量了这些病人。当然，我们也会遇见不是这样格式的数据，比如我们在第五次生统作业上遇见的那个用药的数据集 123456789&gt; test2 &lt;- read.table("rawdata/test2.txt",header = T)&gt; head(test2) control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.23 这个数据集是 15 行，4 列。但我们并不能说我们做了 15 个观测，应用了 4 个变量。实际上，我们根据题目可知，总共是 60 只小鼠，只用了 1 个变量，即用药的浓度。你会发现这个数据集的每一行都不是同一只老鼠，但前面的糖尿病人数据集，每一行都是同一个病人，所以我们可以说每一行都是一个观测。 初次学 R 的人，对于这种数据的结构可能会感到困惑。不过不要紧，数据处理多了，就会慢慢清晰起来。 顺便提一下，现在生物学的数据跟传统社会学的数据有一个很大的不同就是，社会学的数据往往是低维度，高观测，而生物学的数据则恰好相反，是高维度，低观测的。这里的维度指的就是变量。举个例子，比如你要分发问卷给别人来统计大家对你的产品感不感兴趣，你可能在问卷上只有 2 个问题（2个变量，2个维度），但你却分发给了 1w 个人（1w 个 观测）。生物学的例子就好比，你对 100 个植株进行了 50w 个SNP位点的分析，这里就是 100 个观测，50w 的维度。数据结构的不同，就会导致分析方法的不同。 由于生统的数据列数最多也就 4,5 列，加上整列的提取并不需要逻辑运算符，所以后面的提取不涉及到列的提取。同时，为了让大家加深印象，我会交叉地用行以及观测这两个名词。 利用 [] 来提取感兴趣的观测 我们之前在向量里面提到过，如何提取符合条件的数据，这里运用的方法也是一样的，也是利用 which 或者 TRUE 来提取。不过在提取数据框数据的时候，我有一个小建议，就是分步完成你的提取任务。我们还是拿糖尿病人的数据集为例子。比如我们希望提取出年龄大于 30 岁的糖尿病人的数据。 12345678910111213141516171819202122# 先得到索引&gt; patientdata$age &gt; 30[1] FALSE TRUE FALSE TRUE&gt; which(patientdata$age &gt; 30)[1] 2 4# 把索引输入 [] 里面&gt; patientdata[patientdata$age &gt; 30,] patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor&gt; patientdata[which(patientdata$age &gt; 30),] patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor# 有时候嫌得到索引那步比较长，就可以把索引结果存为一个变量&gt; result &lt;- patientdata$age &gt; 30&gt; patientdata[result,] patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor patientdata$age &gt; 30 提取出来的索引值顺利能够放入数据框 [] 的逗号前面是因为我们之前提到过，数据框每列是等长的。想象下，我们有 4 个观测，我们我们用 patientdata$age 提取出来的，实际上是一串有 4 个值的向量，我们对向量进行了逻辑运算符，然后得到了 4 个 TRUE 或者 FALSE值，然后我们就可以把这些 TRUE 或者 FALSE 值和我们的观测一一对应。从而提出我们想要的观测。 事实上，在利用索引提取的时候，我还犯了个小错误，就是把索引输入到了错误的数据框里面，但并没有报错。 123456789&gt; test2[result,] control low middle high2 22.91 24.74 28.67 37.944 19.34 19.66 30.28 27.946 23.79 29.10 23.47 34.238 18.53 18.64 29.62 29.1310 20.14 25.49 34.64 36.1512 19.36 22.69 29.22 24.0714 24.13 20.36 35.12 35.24 这个故事告诉我们的是，索引得到的只是一串数字，他并不跟你产生这个索引结果的数据集有一毛钱的关系。 不要认为 R 的命令是黑箱，一步步地去拆解命令，你就可以很清晰地理解。 如果我们想要两个条件呢，即年龄大于30岁，且犯的是 Type I 型糖尿病呢。年龄大于 30 用的是 &gt; ，I 型糖尿病用的是等于 == ,那且是什么呢。就是我们之前提到的与或非了。 运算符 描述 x | y x或y x &amp; y x和（且）y 非的话是 !，不等于是 != 。不过我们估计是用不到的，所以我这里也就不讲了。 再次来提取我们想要的观测 123456789101112# 先得到索引&gt; patientdata$age &gt; 30[1] FALSE TRUE FALSE TRUE&gt; patientdata$diabetes == "Type1"[1] TRUE FALSE TRUE TRUE&gt; patientdata$age &gt; 30 &amp; patientdata$diabetes == "Type1"[1] FALSE FALSE FALSE TRUE# 提取&gt; patientdata[patientdata$age &gt; 30 &amp; patientdata$diabetes == "Type1",] patientID age diabetes status4 4 52 Type1 Poor 我们还可以在提取我们想要的观测的同时，提取一部分变量（列）出来 1234&gt; patientdata[patientdata$age &gt; 30,c("age","status")] age status2 34 Improved4 52 Poor 利用subset来提取 前面的那番操作大家可能会感觉写的有点长，那有没有一些简写呢，事实上是有的。你可以利用 R 基本包的 subset 函数来进行跟上面一模一样的操作。 别忘了用 ？ 来看看这个函数 有些人可能会提到用 attach 这个函数把数据框添加到 R 的搜索路径中，但实际上我不太推荐这样，因为一旦你要完成有许多个数据框的作业，而你又忘了detach，那么很有可能造成你不同数据框的不同变量之间的混淆。 subset 第一个要输入的参数是你的数据框，第二个要输入的参数是你对于观测（行）的筛选，可以用逻辑运算符串联，第三个可选择输入的是你要选择的列（变量）。跟之前一样的筛选条件，不过这次用的是 subset 函数。 123456789101112131415# 年龄大于30岁&gt; subset(patientdata, age &gt; 30) patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor# 年龄大于30，且 I 型糖尿病&gt; subset(patientdata, age &gt; 30 &amp; diabetes == "Type1") patientID age diabetes status4 4 52 Type1 Poor# 年龄大于30，且 I 型糖尿病的病人的年龄和病情&gt; subset(patientdata, age &gt; 30 &amp; diabetes == "Type1",c("age","status")) age status4 52 Poor]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part4]]></title>
    <url>%2F2019%2F05%2F10%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part4%2F</url>
    <content type="text"><![CDATA[上一节我们讲到了一些向量提取的操作。这一部分我们会讲一些数据框提取的操作。在R里面，数据框提取的基础操作跟向量很相似，还是以 [] 符号为基础。不过由于数据框是一个二维的数据结构，即有行与列，所以我们要以 dataframe[行索引, 列索引] 这种操作来提取数据框中的元素。 由于生统的数据一般只有两列，且比较长，可能不太适合演示，所以我这会用的还是之前糖尿病人的那个数据框。 1234567891011&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor 这次的内容参考了《R语言实战》的 4.10 部分，推荐大家可以去看一看。 数据框列的提取 根据坐标提取 跟我们之前介绍的向量读取差不多，也是可以利用坐标来读取。我们用坐标来提取第二列，即病人的年龄。 12&gt; patientdata[,2][1] 25 34 28 52 我们前面提到，逗号前面部分代表的是行索引（坐标可能更直白一点，但索引可能更正式一点，我还是用索引吧），逗号后面代表你要提取的列的索引。如果行的索引信息是缺失的话，那么就 R 就会默认你选择所有行。所以这里提取出了所有的 4 个病人的年龄。 根据列名提取 但这种坐标信息的提取是比较麻烦的，尤其是对于很多列的数据。你可能数不清你想要的那一列在第几列上。事实上，由于数据框的特殊数据格式，对于列的提取，也有了特殊的提取方式。因为数据框是一个二维数据结构，有行有列。意味着我们有行名和列名。所以在提取列的时候，我们也可以把列名放入原本数字索引在的地方，也可以达到跟数字索引提取同样的效果。 12&gt; patientdata[,"age"][1] 25 34 28 52 这里我们知道了我们的列名叫 age ，所以我们就把 age 加引号放入原本坐标在的那个地方，也顺利地提取出来了第二列，即病人的年龄。 $符号提取 数据框的提取还有一个简单的方式，就是在数据框后面加 $ 符号，然后就会自动跳出你的列名，你只要选择你想要的列名也可以顺利的提取出来。 12&gt; patientdata$age[1] 25 34 28 52 提取多列 多于多列数据的提取，还是用到我们上面提到的提取方法。 1234567891011121314151617# 数字索引提取&gt; patientdata[,c(2,4)] age status1 25 Poor2 34 Improved3 28 Excellent4 52 Poor# 列名提取&gt; patientdata[,c("age","status")] age status1 25 Poor2 34 Improved3 28 Excellent4 52 Poor# $符号可能不行 事实上，在提取列的时候，还有一个小问题，可能大家并没有发现。就是我们在提取单列的时候，得到的是一个向量型的结果，但我们在提取多列的时候，得到的是一个数据框型的结果。 12345&gt; class(patientdata[,"age"])[1] "numeric"&gt; class(patientdata[,c("age","status")])[1] "data.frame" 这是因为 R 会在你提取单列的时候，自动将得到的单列进行降维，将数据框变成向量。如果你并不想让数据进行降维，可以设置 drop = F。 123456789&gt; patientdata[,"age",drop = F] age1 252 343 284 52&gt; class(patientdata[,"age",drop = F])[1] "data.frame" 另一种在提取单列的时候，不降维的方法，就是你不遵循 [行索引，列索引] 这种格式，而是直接输入列索引，就像下面那样 12345678910111213141516&gt; patientdata[2] age1 252 343 284 52&gt; patientdata["age"] age1 252 343 284 52&gt; class(patientdata["age"])[1] "data.frame" 我个人不太推荐在生统做的时候用这个方式，还是建议老老实实按逗号的方式来提取，因为这样可能比较符合你的编程习惯。 但这种提出单列数据，自动降维的情况，对于我们生统是比较好的。因为像你后面做正态性检验等等，本质上你输入的应该是一串数值型的向量，而非是一个数据框。 数据框行的提取 行的提取跟列的提取很像，无非就是索引放在逗号前面。 123456789# 根据数字索引&gt; patientdata[1,] patientID age diabetes status1 1 25 Type1 Poor# 根据行名&gt; patientdata["1",] patientID age diabetes status1 1 25 Type1 Poor 大家可能会感到疑惑，1 和 "1" 难道不是一个东西么，事实上并不是。由于我们这里并没有给数据框赋予常见的那种行名，所以 R 会自动以数字即行号作为数据框的行名。所以 "1" 代表的其实是行名。这里为了更加清楚地展示，我们用人名来表示糖尿病人的行名。 1234567891011121314# rownames可以赋予行名rownames(patientdata) &lt;- c("Paul","James","Wade", "Antony")&gt; patientdata patientID age diabetes statusPaul 1 25 Type1 PoorJames 2 34 Type2 ImprovedWade 3 28 Type1 ExcellentAntony 4 52 Type1 Poor# 根据行名&gt; patientdata["Paul",] patientID age diabetes statusPaul 1 25 Type1 Poor 其实生统的作业不太会让你对某一行进行提取，更多的是提取出符合某一条件的几行来。这个就要涉及到我们之前讲到过的逻辑运算符了，这部分我们下一节再讲。 行列的提取 学会了提取列，也学会了提取行，行列就是你在 [] 里面，逗号前后都加上索引。但这个可能在生统中用处不大。 1234&gt; patientdata[1:2,2:3] age diabetes1 25 Type12 34 Type2]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part3]]></title>
    <url>%2F2019%2F05%2F09%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part3%2F</url>
    <content type="text"><![CDATA[因为生统中经常需要用到一些数据的提取，比如提取某一处理来做正态性检验等等。这些数据的提取本质上就是对某一行或者某一列的提取。所以这一部分我们来讲讲常见的数据提取。 R 里面的逻辑运算符 在讲数据提取之前，我们可能需要先了解一些逻辑运算符的基本知识。只有掌握了这些基本知识，才可以在后面灵活地提取出你想要的数据。 这一部分的内容参考了《R语言实战》的 4.3 部分，推荐大家去看看看 我们生统用到的逻辑运算符通常是大于，小于以及等于。符号分别是 运算符 描述 &lt; 小于 &lt;= 小于等于 &gt; 大于 &gt;= 大于等于 == 等于（注意等于并不是 = ，而是 == 。因为一个等号表达的是赋值或者传入参数） 当你利用逻辑运算符讲一个向量与数字进行比较的时候，R 就会返回给你 TRUE 或者 FALSE。 123&gt; vector_0 &lt;- c(1,2,3,4)&gt; vector_0 &gt; 2[1] FALSE FALSE TRUE TRUE 可以看到，凡是大于 2 的，都标明了 TRUE 。值得一提的是，等于不仅仅可以跟数字进行比较，还可以跟字符串进行比较。这在后面对数据框进行数据提取的时候，很有帮助。 12345&gt; vector_1 &lt;- c(rep("A",2),rep("B",5))&gt; vector_1[1] "A" "A" "B" "B" "B" "B" "B"&gt; vector_1 == "A"[1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE 实际上，R 里面还会有与、或、非等逻辑运算符。这对于数据框的提取也是很有帮助的，这个留待我们后面再讲。 向量的数据提取 讲完了逻辑运算符，我们就可以来提取数据了。我们之前介绍了两种生统常见的数据格式，一种是向量，另一种是数据框。我们这次先讲讲如何对向量来进行数据提取。 直接利用坐标提取 在 R 中最基本的数据提取手段就是利用 [] 这个符号。而在利用 [] 这个符号的时候，最简单的提取方式就是根据坐标进行提取了。我们先来尝试一下。 123456789101112131415161718# 创建一个向量&gt; vector_2 &lt;- c(1:10)&gt; vector_2 [1] 1 2 3 4 5 6 7 8 9 10 # 让我们提取第1个数据，注意 R 是以 1 开头的，而不是以 0 开头的。&gt; vector_2[1][1] 1# 提取第2,3,4个数据&gt; vector_2[2:4][1] 2 3 4# 提取第2,5个数据&gt; vector_2[2,5]Error in vector_2[2, 5] : incorrect number of dimensions&gt; vector_2[c(2,5)][1] 2 5 可以看到，我们在一开始提取 2,5 的时候，R 给了我们报错。是因为向量是一个一维的数据结构，而 [2,5] 这种提取适合的是数据框这种二维的数据结构，这一点我们在后面提取数据框数据的时候会提到。 简单来说，对于向量这种一维数据结构的提取，你并不能在 [] 里面使用逗号。所以，你如果想要提取不连续的坐标，就可以把不连续的坐标变成向量的形式放入 [] 里面。 利用which命令来提取 利用坐标的方式来提取有时候局限性会很大，因为有时候数据会很乱，利用坐标提取并没有什么用。比如下面的数据 1234# sample等命令我们会在后面生统常见的命令那边提到&gt; vector_3 &lt;- sample(1:100,10)&gt; vector_3 [1] 31 24 61 36 65 44 60 3 74 8 如果我们想要提取这里面大于60的数字，我们用肉眼观察，然后得到坐标的方式就比较麻烦。这时候我们就可以让 R 来代替我们找到那些大于 60 的数字的坐标。 这里我们用到的是 which 命令。 12&gt; which(vector_3 &gt; 60)[1] 3 5 9 这样我们就得到了大于 60 的数字的坐标了。然后再传入 [] 里面，这样就可以跟之前利用坐标一样来提取数据了。 12&gt; vector_3=3[which(vector_3 &gt; 60)][1] 61 65 74 利用TRUE和FALSE来进行提取 除了用 which命令来提取，我们还可以利用 TRUE 和 FALSE 来进行提取。 1234&gt; vector_3 &gt; 60 [1] FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE FALSE&gt; vector_3[vector_3 &gt; 60][1] 61 65 74 因为 TRUE 在 R 中和 T 是等价的，后面加参数的时候也是同理的。所以我在后面就会用 T 代表 TRUE了，FALSE 同理。 对于 TRUE 和 FALSE 这个类型的结果来说，有一个小彩蛋。就是我们可以把 T 和 F 传入 mean 和 sum 里面。 1234567# 统计有多少是大于 60 的。&gt; sum(vector_3 &gt; 60)[1] 3# 统计有百分之多少是大于 60 的。&gt; mean(vector_3 &gt; 60)[1] 0.3 可以看到，有 3 个数据是大于60，有 30% 的数据是大于60的。这对于大量数据的整体描述是一个非常好的小技巧。 参考文章： 《R语言实战》4.3 下一节我会讲讲如何对数据框进行提取操作。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part2]]></title>
    <url>%2F2019%2F05%2F08%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part2%2F</url>
    <content type="text"><![CDATA[R里面有许多数据类型，跟生统课相关的就是数值型，字符型，逻辑型了。而R也有很多数据结构，包括标量、向量、矩阵、数组、数据框和列表等。跟生统课上相关的就是向量、数据框这两种了。 后面的一些内容会借鉴《R语言实战》中的内容，推荐大家可以去看看这本书的2.1 ，2.2部分。 向量 向量是用于存储数值型、字符型或逻辑型数据的一维数组。 同一向量中无法混杂不同模式的数据。 即不能把数值型、字符串型、逻辑型的混起来放入同一向量中。 让我们来创建一个向量 123456789# 创建向量a &lt;- c(1, 2, 5, 3, 6, -2, 4)b &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)c &lt;- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE)# 查看向量的数据类型class(a)class(b)class(c) 对于数值型的向量创建，使用 : 可以帮助我们直接创建多个数字。这一点对于我们后面在数据框里面提取数值很有帮助。 12&gt; c(1:10) [1] 1 2 3 4 5 6 7 8 9 10 如果想要重复地创建某些值，就可以考虑 rep 函数。这一点对于后面我们给数据框添加列，或者添加列名行名可能会有帮助。 12345# 可以用?rep来查询其具体用法&gt; rep("A",3)[1] "A" "A" "A"&gt; rep(c("A","B"),3)[1] "A" "B" "A" "B" "A" "B" 如果想要间隔地创建数字，可以考虑用seq 1234# 隔3个数创建数字# 起始数字为1，终止数字为13，间隔为3个数字&gt; seq(1,13,3)[1] 1 4 7 10 13 数据框 数据框是一个二维数据结构，有行和列。一般来说，我们会将行表示观测，列表示变量 数据框可以放入不同类型的文件 一般来说，生统中的数据框是不需要自己创建的，只需要读入就行。用 read.table 读进来就已经是个数据框了。 123&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T)&gt; class(test1)[1] "data.frame" 12345678&gt; head(test1) yield seed1 383 12 406 13 351 14 400 15 390 16 361 1 我们可以对这个数据框进行一些探索。 首先看下这个数据框是几行几列的 123456&gt; dim(test1)[1] 29 2&gt; nrow(test1)[1] 29&gt; ncol(test1)[1] 2 发现是一个 29 X 2 的数据框。然后我们可以看下我们数据框的行名和列名是什么。 12345678# 提取行名&gt; rownames(test1) [1] "1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19"[20] "20" "21" "22" "23" "24" "25" "26" "27" "28" "29"# 提取列名&gt; colnames(test1)[1] "yield" "seed" 我们也可以对行名和列名进行更改 123&gt; colnames(test1) &lt;- c("A","B")&gt; colnames(test1)[1] "A" "B" 当然我们也可以自己来创建一个数据框。用到的是 data.frame 函数。 12345678910111213141516171819202122# R语言实战的例子&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor# 自己来一个例子&gt; data.frame(A = c(rep(1,2),rep(2,2),rep(3,2)), B = rep("test", 6)) A B1 1 test2 1 test3 2 test4 2 test5 3 test6 3 test 需要注意的是，数据框跟列表不一样，数据框里面的每一列都必须是等长的。像我这里就是 2 个 A ，2 个 B ， 2 个 C ，再加上 6 个 test 。 如果不等长就有可能会报错。也有可能不报错，用 NA 或者其他的值填充了。这个后面可能会提到。 因子 在我看来，因子的作用是为了对变量进行分类。就比如我们在做AONVA分析的时候，我们会做多种处理，那么我们就可以认为这些处理每个都是一类。拿上面的例子举例。 123456&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor 这里的糖尿病类型 Diabetes，有两种类型，分别是 Type1 和 Type2 。病情Status 有三种类型，分别是 poor、 improved、 excellent。所以这两列所含有的数据就是因子型的数据。 值得注意的是，R在构建数据框的时候，会自动将所有字符串类型的值转换成因子。我们可以看下 12345678&gt; patientdata$diabetes[1] Type1 Type2 Type1 Type1Levels: Type1 Type2&gt; patientdata$status[1] Poor Improved Excellent Poor Levels: Excellent Improved Poor&gt; patientdata$patientID[1] 1 2 3 4 如果这里有 Levels ，就代表这里的数据是因子。可以看到，patientID由于是数值型的变量，所以并没有自动地转换成因子。 我们同样也可以用 class 来看下类别。 12345&gt; class(patientdata$diabetes)[1] "factor"&gt; class(patientdata$patientID)[1] "numeric" 但有时候，字符串变量自动转换成因子也不是所有都对的，比如一开始我们有name这一列。 1234567891011121314151617181920&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; names &lt;- c("Paul","James","Wade", "Antony")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status,names)&gt; patientdata patientID age diabetes status names1 1 25 Type1 Poor Paul2 2 34 Type2 Improved James3 3 28 Type1 Excellent Wade4 4 52 Type1 Poor Antony# 看下 names 的类别&gt; class(patientdata$names)[1] "factor"&gt; patientdata$names[1] Paul James Wade AntonyLevels: Antony James Paul Wade 我们会发现 R 自动地将 names 这一列也变成了因子。但实际上，名字是独一无二的，并不是一个分类变量。所以，我们不应当将其变成一个 factor 。不过，你会发现，如果是这一列是后添加上去的，就不会自动转成因子。 123456789101112# 这个操作可以自动加上一列名为name_new的列patientdata$names_new &lt;- c("Paul","James","Wade", "Antony")&gt; patientdata patientID age diabetes status names names_new1 1 25 Type1 Poor Paul Paul2 2 34 Type2 Improved James James3 3 28 Type1 Excellent Wade Wade4 4 52 Type1 Poor Antony Antony&gt; patientdata$names_new[1] "Paul" "James" "Wade" "Antony" 让我们再来看下我们在第五次生统作业的第一题的数据。 1234&gt; class(test1$yield)[1] "integer"&gt; class(test1$seed)[1] "integer" 明明我们的seed代表的是处理类别，为什么却不是一个因子呢。因为 seed 那一列是数值型的变量，所以 R 并不会自动地将其转换成因子。但如果不转换成因子的话，就可能会在后续的分析中出现一些问题。所以我们可以用 factor 函数，来将其转换成因子。 123&gt; test1$seed &lt;- factor(test1$seed)&gt; class(test1$seed)[1] "factor" 如果想要R不自动地将字符串转换成因子，可以 123456&gt; # 读数据的时候，设置&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T,stringsAsFactors = F)&gt; &gt; # 自己构建数据框的时候，设置&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status,names,stringsAsFactors = F)&gt; 参考文章： 《R语言实战》第二章 如何理解R中因子(factor)的概念 中猴子的回答]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part1]]></title>
    <url>%2F2019%2F05%2F08%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part1%2F</url>
    <content type="text"><![CDATA[因为深感生统的节奏比较快，可能女票跟不上节奏，所以写了一个简略的manual，只针对生统的一些相关操作，不涉及高深的R操作。如果大家觉得还要加什么东西，可以在下面留言。 前期准备 语言问题: 对于 R 或者 R studio来说，我非常建议把语言更改成英文。这样，在你报错的时候，比较方便去搜索 工作路径： 对于有R studio的来说，频繁地切换 setwd 和 getwd 可能不是一个很好的选择。所以我比较推荐新建一个Project，这样你每次你的任务都会是独立的。新建完Project之后你就可以把作业相关的数据放在你的Project里面。在后面读取的时候，就不用切换 setwd 或者打一大串目录了。 保存问题： 我比较推荐的是在Tools-Global Options—General那里，将Save Worksapce to .RData on exit那里设置为Never。这可能会导致你每次打开你的Project，变量都还得重新打一遍。但这可以保证你的代码的可重复性。 镜像及安装问题： 生统课上的如果包都很小，所以镜像设置其实是无所谓的。 如果想安装某个包的话，使用如下代码 12## 以安装pwr包为例，注意加引号#install.packages(&quot;pwr&quot;) 数据读取 生统课上用到的文件一般给的都是 txt 或者 csv 文件，这意味一般着只需要使用 read.table 这个命令来读取文件就可以了。 让我们先来看一下read.table这个函数怎么用。 1?read.table 不懂的时候寻求谷歌或者?+命令，是一个很好的习惯 你会发现read.table()里面跟了一大堆东西，其中跟我们可能相关的是 file：代表你要读的文件路径 header：表达你是否要添加表头，默认值是FALSE，我们一般要设置为TRUE sep：sep代表是你用什么样的形式来分割你读取的文件，一般生统的文件可能会以空格，制表符，逗号来分割。分别对应sep = " "，sep = ","，sep = "\t" 我们来尝试读一个文件 12test1 &lt;- read.table(&quot;rawdata/test1.txt&quot;,header = T)head(test1) 1234567## yield seed## 1 383 1## 2 406 1## 3 351 1## 4 400 1## 5 390 1## 6 361 1 这里我们用了 header = T ，这样我们的数据就会有表头，或者说列名了。即 yield 和 seed。 TRUE和T是等价的，同理FALSE和F也是等价的。 csv是本质上是用逗号分割的文件，所以我们在读的时候加上 sep = "," 即可。 head代表的是你只输出你数据的前几行。同理，tail输出后几行。 再次提醒一遍，感觉不懂命令是什么时候用?或者谷歌。 你还可以用row.names=1，来将第一列当作行名。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MACS2的原理介绍]]></title>
    <url>%2F2019%2F02%2F17%2FMACS2%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[泊松分布 泊松分布是统计与概率中重要的离散分布之一，泊松分布表示在一定的时间或空间内出现的事件个数，比如某一服务设施在一定时间内受到的服务请求的次数、DNA序列的变异数、汽车站台的等候人数。根据MACS的论文中所描述的，Chip-Seq实验中全基因组的reads分布恰好是符合泊松分布的。 泊松分布的概率分布为 \[ P(X = k)=\frac{e^{-\lambda}\lambda^{k}}{k!} \] 其中e代表的是自然常数，而\(\lambda\)是单位时间（或单位面积）内随机事件的平均发生率，比如在一定时间内某一服务设施受到的请求次数是5次。 另外，泊松分布实际上只有一个参数，即\(\lambda\)，其方差和期望也是\(\lambda\)。同时，随着\(\lambda\)的增加，图像分布会趋于对称。 参考资料 二项分布、泊松分布、正态分布的关系 泊松分布和指数分布：10分钟教程 wiki_泊松分布 MACS的算法概览 Adjusting read position based on fragment size distribution Chip-Seq的主要过程为：交联——超声破碎——特异性识别——测序。所以我们测序得到的片段就是我们转录因子结合位点周围的片段。需要注意的一点是，MACS软件出现的年代是2008年，那时候的测序读长都很短，大约50bp左右，且以单端测序为主，并没有真实反应DNA-蛋白结合片段的长度。所以说，我们如果拿测得的50bp去做reads数目的堆积，势必会与真实的结合位置有一定的偏移。事实上，测序的短reads会在真实的结合位置两侧形成双峰，如下图所A示。这也是MACS双峰模型构建的理论基础。 值得一提的是，像转录因子一类的蛋白与DNA，其结合位点比较narrow，所以双峰模型的构建是比较合理的。但像图B所示的，一些蛋白与DNA会产生较宽的结合区域（诸如一些组蛋白修饰），这时候双峰就不那么显著了。 更为麻烦的是，有时候会有一些混合的结合位点模式，比如Polll蛋白，其会在启动子区域结合，也会覆盖整个基因区域。 为了衡量真实的测序片段大小，d，MACS会粗略地以2倍的超声破碎片段长度作为window来鉴定初步的富集区域。为了避免重复区域或者PCR导致极端富集区域的影响，MACS会随机挑选1000个区域作为模型peak构建区域。这些区域的reads富集程度是基因组背景的10-30倍。对于每个区域的模型peak，MACS都会分离出对比到正链和负链上的reads，然后分别计算出这些reads的位置。从而分别构建出这个区域内的正负链上的模型peak，正负链上模型peak顶点之间距离就记为d。在d确定之后，所有的reads都会朝着3'的方向横移（shift）d/2的距离，从而更好地模拟出蛋白-DNA结合位点。 在2012年的Identifying ChIP-seq enrichment using MACS这篇文章中，作者也提到对于一些过度破碎或者有着很宽的结合位点情况，可能会造成算出来的d很小。对于这种情况，我们一般建议用一个特定的片段长度，而非是预测出来的d。 注意shift和extend的区别，在2008年原始的MACS文章中，作者用的是shift，而到了12年的文章，作者写错，写成了extend。当然，在MACS2中，这两种情况都存在了。 Calculate peak enrichment using local background normalization 基于先前已经调整位置的reads，MACS会在全基因组范围内以2d长度的window来寻找那些有显著富集的区域。有重叠的window会融合成一个候选区域。因为会有许多因素影响不同范围内的reads富集程度，所以MACS用了动态的\(\lambda_{local}\)参数来对于reads数目的富集进行泊松分布的建模。即MACS并不会用一个常数\(\lambda\)，而是用一个会在不同区域有变化的\(\lambda_{local}\)。动态参数值定义为 \[ \lambda_{local}=max(\lambda_{BG},[\lambda_{region},\lambda_{1k}],\lambda_{5k},\lambda_{10k}) \] \(\lambda_{BG}\)来自于全基因组的计算，\(\lambda_{region}\)则来自在control中的对应区域，剩下的\(\lambda_x\)则来自control中，以得到的候选区域为中心，1k，5k，10k范围内的区域计算。见下图 lambda 如果control不在，则local值只是在Chip的样本中计算，而region和1k值也会被舍弃。同时如果Chip-Seq和control的样本测序深度不同，MACS会默认地把测序深度更深的样本缩放。 关于\(\lambda\)以及p值这一步的计算可能需要看源代码才可以了解了。 但根据MACS2的wiki来说，似乎p值和\(\lambda\)的计算都是以单个碱基为单位考虑的。 基于泊松分布的模型，我们就可以以单尾检验，计算出p值了。MACS默认以p=1 x 10-5为阈值。 Estimating the empirical false discovery rate by exchanging ChIP-seq and control samples 这里MACS用的Chip和control的置换，从而检验出FDR值我并没有看懂。不过MACS2用的已经是Benjamini-Hochberg方法了，还是比较好懂的。 参考资料： Evaluation of Algorithm Performance in ChIP-Seq Peak Detection Model-based Analysis of ChIP-Seq (MACS) Identifying ChIP-seq enrichment using MACS In-depth-NGS-Data-Analysis-Course MACS2中的一些参数介绍 -f/--format FORMAT 可以接受多种格式参数，默认使用AUTO来检测格式。但并不能检测“BAMPE”或者“BEDPE”格式，即双端测序格式。所以，当你的数据是双端测序数据时，你应该用BAMPE或者BEDPE参数。当你设置成双端参数的时候，MACS2就会跳过建模计算d的那一步，而是直接用片段的insert size来建立堆积。 --extsize 如果使用这个参数，那么MACS就会使用你设置的数值，来把reads从5‘—3’补齐到你指定的数值。这个参数只有当--nomodel参数设置了，或者MACS建模失败，--fix-bimodal开启的时候才可以用。 --shift shift参数会先于extsize参数执行。如果你设置的数值为正，reads会从5‘—3’偏移，而数值为负，reads会从3'—5‘偏移。当格式为BAMPE或者BEDPE的时候，不能设置参数。 --broad 会放宽cutoff的阈值，然后把临近的区域结合起来，形成较宽的peak区域。与broad-cutoff参数是一起的，broad-cutoff参数默认为q-value的参数，为0.1。 有趣的是，shift后面数值如果为正，则正负链的reads会朝着中心偏移，如果后面数值为负，则正负链的reads会各自远离中心，即正链reads向左，负链reads向右。 给个例子： 1234567891011Original Reads:chr1 500 550 read1 . (+)chr1 700 750 read2 . (-)--shift -100chr1 400 450 read1 . (+)chr1 800 850 read2 . (-)--extsize 200chr1 400 600 read1 . (+)chr1 650 850 read2 . (-) 参考资料： MACS_github google_group 如何使用MACS进行peak calling]]></content>
      <categories>
        <category>算法原理</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我刷过的统计学资料]]></title>
    <url>%2F2019%2F02%2F13%2F%E6%88%91%E5%88%B7%E8%BF%87%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[感觉大家的统计学习兴趣都很浓，我这里也把我之前刷的一些统计资料说下。 说人话的统计学 在协和八微信公众号上连载。我认为这套教程是在学完基本的统计学知识之后，非常好的进阶教程。在看完这套教程后，很多之前学过的模糊知识点都会逐渐明晰。值得一提的是，两位作者都是有丰富数据分析经验的学者。可能也正是这样，这套教程并不局限于书本内容，更多地从实际出发。 女士品茶 在没有基本的统计学知识之前，不建议读这本书。可能你读了半天，会发现这整本书就讲了XX干了XX事。但等你的统计学到一定程度，再看这本书，就会有种越看越开心的感觉吧。不过这本书对于统计学本身功底的提升可能极为有限。 可汗学院公开课：统计学 总共85集，每集大约10分钟不到，老师讲的非常有趣，非常适合统计学入门。里面几乎不涉及公式推导，就讲了统计学中最常见的 随机变量、均值方差标准差、统计图表、概率密度、二项分布、泊松分布、正态分布、大数定律、中心极限定理、样本和抽样分布、参数估计、置信区间、伯努利分布、假设检验和p值、方差分析、回归分析等内容。 基本上刷完你就知道了大致的概念了。不过由于每集比较短，里面的解释可能比较含糊，但非常适合初步地过一遍。 深入浅出统计学 可以说是图文并茂吧，但有点太过于概括了。感觉适合快速地扫一遍。就我个人而言，这种过多地图片可能影响我思考问题了。。。 欧姆社的漫画统计学 额，怎么说呢，感觉适合没接触过统计学的，可能不太适合研究生了。毕竟漫画太多了。 概率论与数理统计（茆诗松版） 值得强推！讲的非常非常细，公式推导非常地详细，可能刷过这本才是真正意义上地去学统计了。不过感觉有点难刷，我看到最后很多公式都是跳着看的。 看完这本书你才发现Fisher、Pearson有多厉害。 概率论与数理统计（陈希孺版） 也非常推荐！但不建议当入门统计的去看这本书，最好地状态应该是先去看茆诗松版的，然后感觉有些概念模糊，觉得讲的不清楚去对照着看陈希孺院士这本书，两相映证，可能效果更好。 统计学习导论-基于R应用 这本书虽然叫统计学习导论，但实际上跟我们常见的统计关系不大。更多地是好像是机器学习的一些基础内容，但胜在公式推导不多，所以还是能看进去一点的。推荐和李程的基因组学课程一起看，你就会知道这里面的知识对于生命科学的研究的重要性了。 李程的基因组学生信技能树也推过的。 这本书就像是常见的统计学教科学的后续延伸，因为统计教科学一般都是以线性回归结束的，而这本书恰好是以线性回归开头的。 刷完这些课程的一些感悟 真正有帮助地可能还是教科书而不是比较入门的书籍，因为教科学会有严谨的推导，而一些入门书籍可能为了趣味性会放弃一些严谨性，而只告诉你一些描述性质的东西。 对于一些优秀的偏概括性质或者历史传记性质的书籍，比如《女士品茶》，《统计学七支柱》，《赤裸裸的统计学》，《数理统计学简史》等等，可能并不应该作为入门读物，而是应该作为刷完教科学之后再去翻阅的书。不然你看完这些书，可能最终留下的结论还是某年某月，某某人干了个什么事。又或者你在津津有味地看书的时候，看到作者提出的公式，会立刻跳过，而不是细细琢磨这个公式在这个背景下的作用。 有道是纸上得来终觉浅，可能下一步我的计划就是去努力地将自己学到的统计学知识跟自己平常遇到的生命科学结合起来，来更好地去学以致用。]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mfuzz的使用]]></title>
    <url>%2F2019%2F01%2F06%2FMfuzz%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[最近在进行ATAC和RNA-Seq的联合分析，由于处理的材料是时间相关的，所以time-course也是一个可以分析的点。在一位刘潜老哥的帮助下，我找到了一篇靠谱熊 转录组时间序列数据处理 的文章，里面提到了mfuzz这个包。里面的软聚类的思想非常符合我的预期，然后就决定拿这个包进行我time-course的分析。为了更好的分析，我决定先翻译下这个包，了解下这个包的大致思想。 1 Overview 这部分是我偷懒的随便写的。。。。。。 感觉time-course的方法一般就是聚类。常见的聚类分为三种，分别是层次聚类（Hierarchical Clustering）、硬聚类（hard clustering）、软聚类（soft clustering）。层次聚类好像一般就是像热图那种。看了pheatmap的文档，感觉pheatmap就是层次聚类，当然你可以设置k_means，变成硬聚类。硬聚类常见的就是k-menas。软聚类就是我们这会要用到的这个包的核心思路。 2 Installation requirements 见Bioconductor的安装方法。 3 Data pre-processing 数据集是来源于酵母细胞循环表达数据。6178个基因，横跨160分钟的17个时间点。用的是芯片数据。 12345678&gt; head(yeast@assayData$exprs) cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 cdc28_50 cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 cdc28_120 cdc28_130 cdc28_140 cdc28_150 cdc28_160YDR132C 0.19 0.30 -0.29 0.29 -0.31 0.23 0.20 -0.08 0.19 -0.51 0.00 -0.31 0.11 -0.02 0.20 0.36 -0.54YMR012W -0.15 -0.15 -0.04 -0.28 -0.39 0.03 0.22 0.04 -0.15 0.37 0.47 -0.10 -0.09 NA -0.04 0.07 0.19YLR214W 0.38 0.30 -0.68 -0.52 -0.43 -0.13 -0.17 0.26 -0.03 -0.34 -0.01 -0.20 0.10 NA 0.45 0.40 0.63YLR116W 0.17 0.06 -0.21 0.19 0.33 0.44 0.46 0.38 -0.15 -0.03 0.04 -0.42 -0.15 0.02 NA -0.51 -0.61YDR203W 0.85 -0.10 -0.56 -0.31 -0.43 0.00 -0.34 0.17 0.40 -0.37 0.15 0.24 0.24 0.17 -0.12 -0.02 0.02YEL059C-A 0.45 0.20 0.06 0.10 -0.21 -0.08 -0.27 -0.01 -0.29 0.41 -0.08 -0.22 -0.27 NA -0.30 0.25 0.26 3.1 Missing value 第一步，去除那些有超过25%数据缺失的基因。注意这些数据缺失值应该是设为NA。 12yeast.r &lt;- filter.NA(yeast, thres=0.25)49 genes excluded. 这里就如上面的数据一样，一行即一个基因有16个时间点的数据，如果16个时间点里面有25%，即4个时间点都是NA，则剔除这个基因。 123456&gt; nrow(yeast)Features 3000 &gt; nrow(yeast.r)Features 2951 Fuzzy c-means就像其他聚类算法一样，其并不允许有缺失值的存在。所以我们会对剩下那些缺失值（16个数据点里面就缺了1个2个那种）进行填充。用的是对应基因的平均表达值。 对于RNA-Seq来说，你可以加上一些pseudocount，比如0.01。 1yeast.f &lt;- fill.NA(yeast.r,mode="mean") 当然，你也可以用（weighted） k-nearest neighbour method。（mode='knn'/'wknn'）。这些方法相比较而言比上面这种简单的方法要好，但需要耗费更多的算力。 3.2 Filtering 许多已经出版的聚类分析包含过滤的步骤，从而来去除那些表达相对比较低的，或者表达不怎么变化的。通常来说，比较受欢迎的就是样本的标准差作为阈值。 1tmp &lt;- filter.std(yeast.f,min.std=0) 然而在基因低表达到高表达的过程中，变化是非常平缓的。所以给定阈值筛选并不一定是可靠的，可能是非常武断。因为现在并没有很多有说服力的筛选手段，所以我们还是避免对基因数据做提前的筛选。这可以避免损失一些有生物学重大意义的基因。 比如1,2,4,10,12,13,15。看起来变化很大，但方差可能并不如你想象中的那么大。 Standardisation 由于聚类是在欧几里德空间中进行的，因此基因的表达值被标准化为平均值为零，标准差为1。该步骤确保了在欧几里得空间中具有相似表达模式的基因是相互接近的。 1yeast.s &lt;- standardise(yeast.f) 重要的是，Mfuzz认为输入的表达数据是完全经过前期数据标准化的。standardise 并不能代替标准化步骤。注意差异：标准化是为了让不同的样品间可以比较，而Mufzz中standardisation则是让转录本或者基因间可以比较。 4 Soft clustering of gene expression data 聚类可以用来解释基因表达的调控机制。众所周知的，基因的表达并不是开和关的，而是一个逐渐变化的过程。一个聚类算法应该展现出一个基因有多么的符合dominant cluster pattern。软聚类应该是一个非常好的方法，因为其可以利用membership \(μ_{ij}\)衡量一个基因 i跟cluster j的关系。 其实就是说基因A跟每个cluster都有关系，无非是membership score的值不一样而已。 软聚类的mfuzz函数基于的是e1071包的fuzzy c-means算法。对于软过滤而言，聚类中心点\(c_j\)来源于所有聚类成员的权重值。在图中的membership值可以用mfuzz.plot来展现。你也可以用mfuzz.plot2来看，其会有更多的选项。 值得注意的是，clustering只会基于表达矩阵，不会使用phenoData的任何信息。还有，在mfuzz中重复会被当作是独立的信息，所以他们应该提前被算好平均值，或者放进不同的ExpressionSet对象里面。 12&gt; cl &lt;- mfuzz(yeast.s,c=16,m=1.25)&gt; mfuzz.plot(yeast.s,cl=cl,mfrow=c(4,4),time.labels=seq(0,160,10)) 123456789101112131415161718192021222324# center代表的应该是你选择的16个中心点的表达模式## 感觉可以用来画图&gt; head(cl$centers,2) cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 cdc28_50 cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 cdc28_120 cdc28_130 cdc28_140 cdc28_150 cdc28_1601 0.1971169 -1.0925729 -1.6203551 -0.7961482 -0.33954720 -0.1567524 -0.05036767 0.08380756 0.5122518 0.3843354 0.4905732 0.437668149 0.4526805 0.3170533 0.2866267 0.2890568 0.60457312 -0.7393245 -0.5872038 0.2438611 -0.1883262 0.03321276 -1.0122666 -0.38203192 -0.47328266 -0.6289479 2.1494891 0.5371715 -0.001270464 -0.5672875 0.2288121 0.2331435 0.5896372 0.5646142# size代表的是各个聚类的基因数目 &gt; head(cl$size,2)[1] 175 244# cluster代表的是基因所属的membership score最高的那个簇&gt; head(cl$cluster,5)YDR132C YMR012W YLR214W YLR116W YDR203W 4 11 16 13 16 # membership代表的是每个基因对应16个簇的membership值&gt; head(cl$membership,5) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16YDR132C 0.014386854 0.0023657075 0.0478663840 0.47749014 0.007117060 0.070156647 0.010004348 0.051767933 0.01711759 0.028929516 0.0064926466 0.011933880 0.110039692 0.025007449 0.033967103 0.08535705YMR012W 0.082025023 0.1807366237 0.0088059459 0.00371571 0.012467968 0.002590922 0.127711735 0.007948114 0.06412901 0.004626184 0.3408675360 0.105343965 0.008278282 0.011112051 0.012020122 0.02762081YLR214W 0.130082565 0.0047176611 0.0041702207 0.06382311 0.001340277 0.004636745 0.013298969 0.043267912 0.21204674 0.016651050 0.0850329333 0.023956515 0.010243391 0.003459240 0.022271137 0.36100153YLR116W 0.002047923 0.0008467741 0.0412749579 0.01409627 0.002758020 0.034171711 0.001234725 0.002968499 0.00083482 0.003580831 0.0006540104 0.003016454 0.846273635 0.023517377 0.012508494 0.01021550YDR203W 0.083941355 0.0008482787 0.0008575124 0.02562416 0.001318053 0.004043468 0.001274160 0.032185727 0.12237271 0.002649867 0.0125075149 0.003545481 0.005389429 0.001504605 0.007198611 0.69473907 123mfuzz.plot(eset,cl,mfrow=c(1,1),colo,min.mem=0,time.labels,new.window=TRUE)colo可以设置颜色，min.mem可以设置membership的阈值 4.1 Setting of parameters for FCM clustering 对于fuzzy c-means来说，模糊值m和聚类数c必须提前设置好。对于m，我们应该选择一个可以防止随机数据聚类的值。值得注意的是，fuzzy 聚类可以遵守这样的准则，随机数据并不能被聚类。这相比于硬聚类（例如k-means）来说，是一个明显的优点。因为其即使在随机数据中，也可以检测到cluster。为了达到这一点，你可以使用下列选项： partcoef函数，来检测是否在某一特定的m设置下，随机数也会被聚类 或者直接计算 12&gt; m1 &lt;- mestimate(yeast.s)&gt; m1 # 1.15 设置一个合理的聚类值c是很有挑战性的，尤其是那些short time series，很有可能就会有overlapping clusters。我们可以设置一个最大的c值，大到最后出现了一个空的empty clusters（看 cselection函数） 12345678# 不太懂repeat值代表了什么&gt; cselection(yeast.s,m=1.25,crange=seq(4,32,4),repeats=5,visu=TRUE) c:4 c:8 c:12 c:16 c:20 c:24 c:28 c:32repeats:1 4 8 12 16 19 24 27 31repeats:2 4 8 12 16 20 23 28 30repeats:3 4 8 12 16 20 23 28 32repeats:4 4 8 12 16 20 24 28 31repeats:5 4 8 12 16 20 23 27 32 在cluster centroid之间最小距离\(D_{min}\) 也可以作为簇有效指数。在这里，我们可以检测不同的c值之间的\(D_{min}\)。我们可以预期D.min在达到最合适值之后，下降幅度会变低。你也可以选择 4.2 Cluster score Membership值也可以暗示两个向量之间的相关性。如果两个基因对于一个特定的cluster都有高的membership score，那么他们通常来说表达模式是相似的。我们对于高于阈值α的基因，叫做这个cluster的α-core。 membersip score的设置通常可以作为基因的后验筛选。我们可以用acore函数。 12tmp &lt;- acore(yeast.s,cl,min.acore = 0.5)# 生成的似乎是个列表，里面有16个。就可以知道每个簇里面含有的基因ID了。 5 Cluster stability FCM参数的变化也可以体现出cluster的稳健性。我们认为那些稳健的clusters具有某个特征，即在m的变化下，也只会展现出很小的变化。 12cl2 &lt;- mfuzz(yeast.s,c=16,m=1.35)mfuzz.plot(yeast.s,cl=cl2,mfrow=c(4,4),time.labels=seq(0,160,10)) 6 Global clustering structures 软聚类有趣的一点就是clusters之间的overlap或者coupling。在cluster k和l之间的coupling coefficient \(V_{kl}\) 可以定义为： \[ V_{kl}=\frac{1}{N}\sum^{N}_{i=1}{\mu_{ik}}{\mu_{il}} \] N是整个基因表达矩阵的数目。如果coupling值越低，说明两者的表达模式距离越远。如果越高，说明表达模式越相近。 12O &lt;- overlap(cl)Ptmp &lt;- overlap.plot(cl,over=O,thres=0.05) 7 Mfuzzgui - the graphical user interface for the Mfuzz pack-age mfuzz有图形化界面，不过我没去用。 小结 最近期末考试复习太忙了。。。。有空再加上点注意事项。]]></content>
      <categories>
        <category>软件的使用</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
      </tags>
  </entry>
</search>
