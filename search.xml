<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part19]]></title>
    <url>%2F2019%2F06%2F19%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part19%2F</url>
    <content type="text"><![CDATA[聚类 聚类(clustering),指将样本分到不同的组中，使得同一组中的样本差异尽可能的小，而不同组中的样本差异尽可能的大（这个定义很虚哈╮（╯＿╰）╭）。我也不知道考试的话，聚类能考个啥 聚类的话，课件上和作业里提到的似乎是层次聚类（Hierarchical cluster ），可以用R里面的hclust函数。然后稍微注意几点的是，hclust函数有不同的method，到时候如果要的话，根据题目来就行了。 12method the agglomeration method to be used. This should be (an unambiguous abbreviation of) one of "ward.D", "ward.D2", "single", "complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC). 然后hclust函数输入的数据是各个样本之间的距离，用dist函数就可以了，dist函数里面可以设置不同的度量距离的方法，比如欧氏距离，曼哈顿距离等等 12method the distance measure to be used. This must be one of "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski". Any unambiguous substring can be given. 然后画图的话，就是plot。 举个作业上的例子：请利用广泛使用的iris数据的花瓣属性值进行简单层次聚类。 12345678910111213141516# 整理数据，因为鸢尾花数据第5列是花的品种，所以不选&gt; dat &lt;- iris[,1:4]&gt; head(dat) Sepal.Length Sepal.Width Petal.Length Petal.Width1 5.1 3.5 1.4 0.22 4.9 3.0 1.4 0.23 4.7 3.2 1.3 0.24 4.6 3.1 1.5 0.25 5.0 3.6 1.4 0.26 5.4 3.9 1.7 0.4# 然后利用dist计算距离&gt; dat_dist &lt;- dist(dat)# 把距离输入hclust，然后画图plot(hclust(dat_dist)) 聚类的话，在《R语言实战》第二版的第16章。 在聚类那一章里面提到了缩放数据，这个可以稍微提下，好像看到了往年有道题目考了这个。代码来源于P343 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 因为我们是变量内部进行缩放，变量之间是没有关系的，而我们每列都是一个变量，所以apply的margin是2data &lt;- data.frame(x1=c(100:105),x2=(0:5))&gt; data x1 x21 100 02 101 13 102 24 103 35 104 46 105 5# 每个变量标准化为均值为0和标准差为1的变量。&gt; apply(data, 2, function(x)&#123;(x-mean(x))/sd(x)&#125;) x1 x2[1,] -1.3363062 -1.3363062[2,] -0.8017837 -0.8017837[3,] -0.2672612 -0.2672612[4,] 0.2672612 0.2672612[5,] 0.8017837 0.8017837[6,] 1.3363062 1.3363062# 每个变量被其最大值相除&gt; apply(data, 2, function(x)&#123;x/max(x)&#125;) x1 x2[1,] 0.9523810 0.0[2,] 0.9619048 0.2[3,] 0.9714286 0.4[4,] 0.9809524 0.6[5,] 0.9904762 0.8[6,] 1.0000000 1.0# 该变量减去它的平均值并除以变量的平均绝对偏差（Mean Absolute Deviation，查下百度吧）&gt; apply(data, 2, function(x)&#123;(x - mean(x)) / mad(x)&#125;) x1 x2[1,] -1.1241513 -1.1241513[2,] -0.6744908 -0.6744908[3,] -0.2248303 -0.2248303[4,] 0.2248303 0.2248303[5,] 0.6744908 0.6744908[6,] 1.1241513 1.1241513# 第一种方法可以用scale解决&gt; scale(data) x1 x2[1,] -1.3363062 -1.3363062[2,] -0.8017837 -0.8017837[3,] -0.2672612 -0.2672612[4,] 0.2672612 0.2672612[5,] 0.8017837 0.8017837[6,] 1.3363062 1.3363062attr(,"scaled:center") x1 x2 102.5 2.5 attr(,"scaled:scale") x1 x2 1.870829 1.870829 主成分分析 主成分分析的话，我用一个例子来说明我们可能会问到的问题（我PCA其实搞的不清楚，所以还是按照作业答案来。） 对鸢尾花数据进行PCA分析 进行主成分分析 1234# 因为鸢尾花第5列是物种名，所以做PCA的时候去掉第五列# 记得要cor = T，这样应该是可以保证对你的数据是标准化# 但具体原因还是不太清楚iris_pca &lt;- princomp(iris[,1:4], cor = T) 各个主成分能解释多少方差 12345678910# 主成分概述# 这里看Proportion of Variance那一列，代表主成分能解释多少变异# 看Cumulative Proportion就可以知道，前面的几个主成分能累积解释多少变异&gt; summary(iris_pca)Importance of components: Comp.1 Comp.2 Comp.3 Comp.4Standard deviation 1.7083611 0.9560494 0.38308860 0.143926497Proportion of Variance 0.7296245 0.2285076 0.03668922 0.005178709Cumulative Proportion 0.7296245 0.9581321 0.99482129 1.000000000 哪些变量能被PC1所解释 12345678910111213141516# 在loadings那边看，所有变量应该都能被PC1所解释# PC2那边Petal.Length，Petal.Width，loading就很小，没有显示（其实是有的，不过很小），应该就无法被解释&gt; iris_pca$loadingsLoadings: Comp.1 Comp.2 Comp.3 Comp.4Sepal.Length 0.521 0.377 0.720 0.261Sepal.Width -0.269 0.923 -0.244 -0.124Petal.Length 0.580 -0.142 -0.801Petal.Width 0.565 -0.634 0.524 Comp.1 Comp.2 Comp.3 Comp.4SS loadings 1.00 1.00 1.00 1.00Proportion Var 0.25 0.25 0.25 0.25Cumulative Var 0.25 0.50 0.75 1.00 降维后的数据 123456789101112131415161718&gt; head(iris_pca$scores) Comp.1 Comp.2 Comp.3 Comp.4[1,] -2.264703 0.4800266 0.12770602 0.02416820[2,] -2.080961 -0.6741336 0.23460885 0.10300677[3,] -2.364229 -0.3419080 -0.04420148 0.02837705[4,] -2.299384 -0.5973945 -0.09129011 -0.06595556[5,] -2.389842 0.6468354 -0.01573820 -0.03592281[6,] -2.075631 1.4891775 -0.02696829 0.00660818# 只取投射到PC1和PC2上的数据&gt; head(iris_pca$scores[,1:2]) Comp.1 Comp.2[1,] -2.264703 0.4800266[2,] -2.080961 -0.6741336[3,] -2.364229 -0.3419080[4,] -2.299384 -0.5973945[5,] -2.389842 0.6468354[6,] -2.075631 1.4891775 写下PC1（以向量的形式） 12345678910111213141516# 还是在loading那边看&gt; iris_pca$loadingsLoadings: Comp.1 Comp.2 Comp.3 Comp.4Sepal.Length 0.521 0.377 0.720 0.261Sepal.Width -0.269 0.923 -0.244 -0.124Petal.Length 0.580 -0.142 -0.801Petal.Width 0.565 -0.634 0.524 Comp.1 Comp.2 Comp.3 Comp.4SS loadings 1.00 1.00 1.00 1.00Proportion Var 0.25 0.25 0.25 0.25Cumulative Var 0.25 0.50 0.75 1.00PC1：(0.521,-0.269,0.580,0.565)]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part18]]></title>
    <url>%2F2019%2F06%2F18%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part18%2F</url>
    <content type="text"><![CDATA[apply和function 差异基因的检验估计会用到function和apply。不过差异基因表达的function和apply估计也不会有太大的难度，所以我这里就直接放上《R语言实战》的例子。详细地题目应对我还是放到后面具体题目（当然也是因为我R语言太菜了……） function function可以去看下《R语言实战》第二版的P436，我这里截个图 function的基本语法为 1234functionname &lt;- function(parameters)&#123; statements return(value)&#125; functionname就是你的函数名字，你也可以写成wodehanshu这种名字。 parameters里面就是设定你要用到的参数 statement就是你执行的一系列操作，一般来说你这一系列操作会产生一个值。然后用return返回 例子的话，上面已经举了。 apply apply的使用我还是放一个《R语言实战》的例子，在P95和P96 差异基因表达 差异基因估计是用t-test做检验，然后用p.adjust做矫正（虽然照理说不能用t-test做）。我这里就不详细地讲了。举几个例子吧 题目1 我们第4次作业的第四题 Type 1 diabetes is a multigenic disease caused by T-cell mediated destruction of the insulin producing β-cells. Although conventional (targeted) approaches of identifying causative genes have advanced our knowledge of this disease, many questions remain unanswered. Here we have a gene data from NOD mouse after(case) and before(control) treatment. The data can be found in “Data.txt”.Use the information mentioned above to answer the following questions: 1.use paired t-test to find genes which have significant expression (p&lt;0.05) between case and control sample. Give the number of differential expressed genes and give the names of top 10 significantly differential expression genes. hint: “apply(data,1,function(x){…})” can apply function to every row in data more quickly than “for{}”, “names()” or “rownames()” can be used to extract names of differentially expressed genes 这里是输入了基因表达的数据。分别是10个control和10个case，然后做配对的t-test。我们可以手动地对每个基因（即每一列）对t-test。但这样太麻烦，所以我们就可以用apply。apply可以对每一列做批量化的操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 读取和整理数据# paste0这个操作就是把“gene”这个字符串和对应的列名粘起来，这样使得列名比较直观一点test4 &lt;- read.table("rawdata/test4.txt",header = T)rownames(test4) &lt;- paste0("gene",rownames(test4))# 看下数据（我后来发现没有gene1了。。。。。但我最后的答案跟作业答案一样，应该是原始文件的锅）&gt; head(test4) control1 control2 control3 control4 control5 control6 control7 control8 control9 control10 case1 case2gene2 6.917468 6.308350 5.318841 5.886811 5.082975 5.629453 4.919035 3.134226 4.242564 5.783208 3.574525 5.371273gene3 7.862730 7.065809 6.783732 6.275773 3.063104 5.131017 3.708938 2.766133 6.755942 3.954392 7.163509 5.600665gene4 7.425996 7.707939 7.550885 5.708736 5.900326 6.888329 5.987753 5.948979 7.281995 6.758037 5.367602 7.898202gene5 6.544029 8.364054 7.239746 9.037322 8.783692 8.863040 10.602350 9.046190 9.007177 7.866627 8.079780 8.128662gene6 4.986893 7.248867 7.098780 6.787237 6.252609 6.428099 7.758556 6.726613 7.123883 6.584567 6.642005 7.376994gene7 6.995863 6.448682 6.136518 7.098878 4.984325 5.896835 6.488555 5.299349 5.524948 6.763232 7.004367 6.478264 case3 case4 case5 case6 case7 case8 case9 case10gene2 7.217279 6.786191 5.523913 5.355584 6.693710 4.364467 6.270432 6.993901gene3 6.079167 7.125393 5.169236 4.524597 4.715383 2.946439 4.200206 5.798403gene4 6.321147 7.810430 6.231793 6.367243 7.579535 7.207376 7.548390 6.522594gene5 8.716447 9.940214 9.224614 9.139514 9.429897 9.373779 9.287744 9.275545gene6 7.080039 7.100746 6.453575 6.618557 7.614747 7.141351 6.076726 7.369737gene7 6.273696 6.413472 4.983931 5.629765 5.213316 5.931045 5.776625 6.522169# 构建函数（我个人可能比较推荐先构建函数，然后把函数放到apply中，这样可能比较直观一点）# 我输入数据为x，然后我提取1:10个数为data1,11:20个数为data2。然后做t.testmyFun &lt;- function(x)&#123; data1 &lt;- x[1:10] data2 &lt;- x[11:20] t_result &lt;- t.test(data1,data2,paired = T) p_value &lt;- t_result$p.value return(p_value)&#125;# 利用apply，一行行地把数据放入myFun里面，一行行地做t.test# 输出应该是个向量test4_result &lt;- apply(test4_paired_result, 1, result)# 也可以直接一行代码apply(test4, 1, function(x) &#123;t.test(x[1:10],x[11:20],paired = T)$p.value&#125;)# 数下差异基因的数量&gt; sum(test4_result &lt; 0.05)[1] 2296# 输出前10个## 先对得到的p-value进行排序test4_result[order(test4_result)]## 然后得到前10&gt; test4_result[order(test4_result)][1:10] gene28801 gene27868 gene27438 gene21642 gene24019 gene12323 gene12962 gene28939 gene2387 4.277344e-06 6.093302e-05 8.229606e-05 9.889894e-05 1.124717e-04 1.261658e-04 1.298200e-04 1.462493e-04 1.522920e-04 gene18712 1.601297e-04 2.Adjust the p-values in question a) with bonferroni and FDR method to find differentially expressed genes in stringent way( list the differentially expressed gene names and the adjusted p-value) 矫正的话，直接用p.adjust就可以了。这里再用sum或者mean数一数差异基因的个数 12345test4_adjust_bonf &lt;- p.adjust(test4_result, method = "bonferroni")mean(test4_adjust_bonf &lt; 0.05)test4_adjust_fdr &lt;- p.adjust(test4_result, method = "fdr")mean(test4_adjust_fdr &lt; 0.05) 题目2 把题目中的paired数据变成非配对的数据。那么我们就要多一步方差齐性检验了。 1234567891011121314151617# 在函数里面做判断myFun &lt;- function(x)&#123; data1 &lt;- x[1:10] data2 &lt;- x[11:20] if (var.test(data1,data2)$p.value &gt; 0.05)&#123; t_result &lt;- t.test(data1,data2,var.equal = T) p_value &lt;- t_result$p.value return(p_value) &#125; else &#123; t_result &lt;- t.test(data1,data2,var.equal = F) p_value &lt;- t_result$p.value return(p_value) &#125;&#125;test4_result &lt;- apply(test4, 1, myFun) 题目3 来自17年考试的第5题 Nonalcoholic steatohepatitis or NASH is a common liver disease, and was found to be linked to obesity and diabetes, suggesting an important role of adipose tissue in the pathogenesis of NASH. Therefore, the mouse model was used to investigate the interaction between adipose tissue and liver. Wildtype male C57Bl/6 mice were fed low fat food (LFD) or high fat food (HFD) for 21 weeks. The detailed data can be found in GDS4013.txt. Hint:provide the R code and result calculated with R. （本题共20分） 1.Read the data. Check whether there are NAs in the data? If NAs exist, you should deaf with them before next steps. Hint: delete the rows with NAs in data.** (5分）** 123456789101112131415161718192021222324252627282930# 先读取数据&gt; test &lt;- read.table("rawdata/GDS4013.txt",header = T)&gt; head(test) LFD LFD.1 LFD.2 LFD.3 LFD.4 LFD.5 LFD.6 LFD.7 LFD.8 LFD.9 HFD HFD.1 HFD.2COPG1 7.756187 7.513726 7.927167 7.742323 7.974731 7.632936 7.419638 8.022296 7.649602 7.694316 7.717317 7.721638 7.873688ATP6V0D1 8.968808 9.004967 9.025234 9.139161 9.142125 8.998360 9.097204 9.181852 9.127648 9.149562 9.029930 8.948781 9.095943GOLGA7 9.698317 9.798994 9.765514 9.581075 9.776361 9.829636 9.670068 9.675262 9.812124 9.637824 9.625283 9.822128 9.700576PSPH 6.897664 NA 6.744140 7.225862 7.343744 6.877918 6.422594 7.188237 6.819006 6.524627 6.514469 6.949181 7.044664TRAPPC4 5.406903 5.473714 5.749002 5.416623 5.351018 5.431658 5.606044 5.401849 5.374199 5.331412 5.439976 5.487543 5.290085DPM2 6.532734 6.215751 6.520986 6.143827 6.597586 6.731978 6.553594 6.659309 6.288114 6.561733 6.819006 6.616581 6.349064 HFD.3 HFD.4 HFD.5 HFD.6 HFD.7COPG1 7.637193 7.570261 7.549871 7.687248 7.603990ATP6V0D1 9.092459 8.981442 9.014060 9.122049 9.000419GOLGA7 9.739647 9.679455 9.790654 9.817816 9.761968PSPH 6.708345 6.413345 6.617198 6.843793 6.467956TRAPPC4 5.485993 5.376865 5.522681 5.430362 5.490806DPM2 6.554808 6.416926 6.562119 6.524627 6.771978# 利用table看看缺失值的多少&gt; table(is.na(test)) FALSE TRUE 481694 4 # 利用na.omittest_new &lt;- na.omit(test)&gt; table(is.na(test_new)) FALSE 481626 2.Find differentially expressed genes (DEGs) (up-regulated, HF&gt;LF) between LFD and HFD samples, list the DEGs with adjusted p-values less than 0.05. For expressions of each gene, check the homogeneity of the variances in two groups at first. Hint: FDR should be used to adjust p-values. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 因为不是配对检验，所以我们首先要做的是检验方差齐性## 所以我们需要先构建一个函数来检验方差的齐性myFun_var &lt;- function(data)&#123; LFD &lt;- data[1:10] HFD &lt;- data[11:18] var_result &lt;- var.test(LFD,HFD) p_value &lt;- var_result$p.value&#125;## 应用apply来得到p-valuevar_pvalue &lt;- apply(test_new, 1, myFun_var)## 然后往表达矩阵上多加一列p-value，得到有附加信息的表达矩阵&gt; test_new_var &lt;- cbind(test_new,var_pvalue)&gt; head(test_new_var) LFD LFD.1 LFD.2 LFD.3 LFD.4 LFD.5 LFD.6 LFD.7 LFD.8 LFD.9 HFD HFD.1COPG1 7.756187 7.513726 7.927167 7.742323 7.974731 7.632936 7.419638 8.022296 7.649602 7.694316 7.717317 7.721638ATP6V0D1 8.968808 9.004967 9.025234 9.139161 9.142125 8.998360 9.097204 9.181852 9.127648 9.149562 9.029930 8.948781GOLGA7 9.698317 9.798994 9.765514 9.581075 9.776361 9.829636 9.670068 9.675262 9.812124 9.637824 9.625283 9.822128TRAPPC4 5.406903 5.473714 5.749002 5.416623 5.351018 5.431658 5.606044 5.401849 5.374199 5.331412 5.439976 5.487543DPM2 6.532734 6.215751 6.520986 6.143827 6.597586 6.731978 6.553594 6.659309 6.288114 6.561733 6.819006 6.616581PSMB5 10.217143 9.992716 9.974211 10.201814 10.244172 10.023191 10.072604 10.147738 10.267069 10.024750 10.142353 10.119065 HFD.2 HFD.3 HFD.4 HFD.5 HFD.6 HFD.7 var_pvalueCOPG1 7.873688 7.637193 7.570261 7.549871 7.687248 7.603990 0.1119611ATP6V0D1 9.095943 9.092459 8.981442 9.014060 9.122049 9.000419 0.5807521GOLGA7 9.700576 9.739647 9.679455 9.790654 9.817816 9.761968 0.6524620TRAPPC4 5.290085 5.485993 5.376865 5.522681 5.430362 5.490806 0.1770907DPM2 6.349064 6.554808 6.416926 6.562119 6.524627 6.771978 0.6063077PSMB5 10.086752 10.240642 10.086752 10.246507 10.291952 10.129569 0.3887486# 得到了方差检验的p-value，我们就可以来把这个条件加入到t.test里面，然后做t-test了## 首先也要构建t-test的函数myFun_t_test &lt;- function(data)&#123; LFD &lt;- data[1:10] # 1:10列为LFD HFD &lt;- data[11:18] # 11:18列为HFD var_result &lt;- data[19] # 19列为方差齐性的p-value t_result &lt;- t.test(LFD,HFD,alternative = "less",var.equal = var_result &gt; 0.05) ## 这里var_result是p-value，p-value如果大于0.05了（方差齐性），就会输出TRUE。 p_value &lt;- t_result$p.value&#125;## 利用apply，一行行地输入函数里面，做t-testt_test_pvalue &lt;- apply(test_new_var, 1, myFun_t_test)# 得到了p-value之后，就用fdr方法做矫正&gt; t_test_padjust &lt;- p.adjust(t_test_pvalue,method = "fdr")&gt; t_test_padjust[t_test_padjust &lt; 0.05] SEMA5B BC048403 0.01698630 0.04004457 大家可能会奇怪这里为什么是less了，一般我们不都是two.side么 选择单双边实际上是跟你的假设有关系的，我们这里是假设up-regulated, HF&gt;LF。只是单边而已。那为啥又是less，而不是greater呢。因为t.test(a,b,alternative)里面的alternative是针对前一个比后一个，即如果是greater的话，就是a比b大。我们这里是LFD放在了前面，假设是HF&gt;LF的话，那么我们写的时候应该是LFD less than HFD。 举个例子的话 12345678910111213141516171819202122232425262728293031# 模拟两个值&gt; a &lt;- rnorm(20)&gt; b &lt;- rnorm(20,mean = 1)# 可以看到下面的统计检验，如果是greater的话，即a&gt;b，是不显著的。而less是显著的&gt; t.test(a,b,alternative = "greater") Welch Two Sample t-testdata: a and bt = -1.5134, df = 30.883, p-value = 0.9298alternative hypothesis: true difference in means is greater than 095 percent confidence interval: -0.7961539 Infsample estimates:mean of x mean of y 0.4103696 0.7858315 &gt; t.test(a,b,alternative = "less") Welch Two Sample t-testdata: a and bt = -1.5134, df = 30.883, p-value = 0.07017alternative hypothesis: true difference in means is less than 095 percent confidence interval: -Inf 0.04523018sample estimates:mean of x mean of y 0.4103696 0.7858315 然后先方差齐性检验，后做t-tes的代码，除了上面的，也可以用题目2的代码 1234567891011121314151617181920212223# 用了if elsemyFun &lt;- function(x)&#123; data1 &lt;- x[1:10] data2 &lt;- x[11:18] if (var.test(data1,data2)$p.value &gt; 0.05)&#123; t_result &lt;- t.test(data1,data2,var.equal = T,alternative = "less") p_value &lt;- t_result$p.value return(p_value) &#125; else &#123; t_result &lt;- t.test(data1,data2,var.equal = F,alternative = "less") p_value &lt;- t_result$p.value return(p_value) &#125; &#125;t_test_pvalue &lt;- apply(test_new_var, 1, myFun)t_test_padjust &lt;- p.adjust(t_test_pvalue,method = "fdr")t_test_padjust[t_test_padjust &lt; 0.05] 也可以用答案里面简略的 12345678910111213141516# 方差齐性检验p.vartest &lt;- apply(GDS4013_new, 1, function(x)var.test(x[1:10],x[11:18])$p.value)# 利用cbind合并p-value和表达矩阵GDS4013_new_data &lt;- cbind(GDS4013_new, p.vartest)# t.test检验p.value &lt;- apply(GDS4013_new_data, 1, function(x)t.test(x[1:10],x[11:18],alternative = 'less', var.equal = (x[19] &gt;= 0.05))$p.value)p.fdr &lt;- p.adjust(p.value, "fdr")&gt; sum(p.fdr&lt;0.05)[1] 2&gt; sig.p.fdr&lt;-p.fdr[p.fdr&lt;0.05]&gt; names(sort(sig.p.fdr))[1] "SEMA5B" "BC048403" 富集分析 富集分析的原理我应该会在考完试有空的时候再重新理一下，我这里就直接放上用fihser exact test（fisher 精确检验）做富集分析的代码。 稍微提下富集的原理。 一般差异表达的基因会有上千个，一个个手动看肯定不现实。所以我们就需要从宏观上来看这些基因，用到的一个方法就是富集分析。一般用的比较多的是GO富集分析，就是比如我们得到1000个差异基因，然后我们感兴趣的GO通路在差异基因中有50个基因。总的基因有2w个，在这2w个基因中，我们感兴趣的GO有200个。我们想知道感兴趣GO相比于总体来说，在差异基因集中是不是富集的。 fisher exact test输入的格式是列联表的格式，就像下面 差异基因 无差异基因 在通路中 50 200-50=150 不在通路中 1000-50=950 19000-150=18850 变成代码就是 1234567891011121314151617&gt; test &lt;- matrix(c(50,950,150,18850),2)&gt; test [,1] [,2][1,] 50 150[2,] 950 18850&gt; fisher.test(test) Fisher's Exact Test for Count Datadata: testp-value &lt; 2.2e-16alternative hypothesis: true odds ratio is not equal to 195 percent confidence interval: 4.670869 9.230328sample estimates:odds ratio 6.61189 看下我们某年的一个GO富集题目 Usually you are interested in the function indicated by differentially expressed genes, for which GO enrichment is a widely used method. In order to find whether the differentially expression genes (downregulated, p&lt;=0.05) are enriched in “leukocyte activation during immune response” (GO term), please show a conclusive result using fisher exact test. Known genes annotated with this GO are listed in “GO_2_2.txt” 全部基因总共是14082个基因，前面差异表达做出来的差异基因是617个。然后GO_2_2.txt里面给出了leukocyte activation during immune response这个GO的基因，基因数目为193。我们就需要找差异基因中有多少这个GO通路的基因，答案里用到的intersect，其实就是取交集。我举个例子 123456789&gt; total &lt;- c(1:10)&gt; part &lt;- c(1,4,5)&gt; intersect(part,total)[1] 1 4 5# 还可以知道有多少&gt; length(intersect(part,total))[1] 3 然后答案里的intersect出来是10个。这样列联表的数据就全了。 差异基因 无差异基因 在通路中 10 193-10=183 不在通路中 617-10=607 14082-607-183-10=13282 然后就可以做fisher了 1234567891011121314151617&gt; test &lt;- matrix(c(10,607,183,13282),nrow = 2)&gt; test [,1] [,2][1,] 10 183[2,] 607 13282&gt; fisher.test(test) Fisher's Exact Test for Count Datadata: testp-value = 0.5923alternative hypothesis: true odds ratio is not equal to 195 percent confidence interval: 0.5609475 2.2650703sample estimates:odds ratio 1.195686 然后有人提到答案里面其实是不准的，因为GO2_2的表中，有几个基因是不在总的基因里面的。 画出列联表的步骤我觉得可以是这样的 差异基因数目定下了，然后GO表和差异基因取交集，那么就得到了列联表的左上。 然后差异基因减去左上，就得到了左下 然后GO表减去左上，就得到了右上 然后总的基因减去左上、左下、右上，就得到了右下。 我只是按照了答案的思路来，不保证对哈。大家自行斟酌，原理就是这么个原理。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part17]]></title>
    <url>%2F2019%2F06%2F16%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part17%2F</url>
    <content type="text"><![CDATA[逻辑斯蒂回归 我们之前提到的线性回归是利用X来预测Y，Y是连续型的数值变量。但有时候Y并不是连续型的变量，而是一种离散型的变量。或者说，更准确来说，是一种定类数据。举个《统计学习导论》书上的例子，假设现在要通过一个急救室病人的症状来预测其患病情况。我们有三种可能的诊断：中风，服药过量，癫痫发作。我们分别用数字来表示这种诊断 中风：1 服药过量：2 癫痫发作：3 这里的诊断结果就是Y，症状就是X。我们也可以用前面线性回归来做，但这样就是默认其实是一个有序的输出。但实际上，中风，服药过量，癫痫发作这三类数据虽然我们用数字来代表，但其实并不是有顺序之分的，所以用线性回归并不适合。所以，我们就可以考虑用logistic回归来解决这种分类问题。 我之所以说Y是一种离散型的变量，更准确来说是一种定类数据。是因为还有离散型变量里面还有一类数据，叫做定序数据。其虽然是离散的，但数据之间是有程度之分的，比如轻微、严重、特别严重等等。这时候，我们既不能用线性回归，也不能用logistic回归，我们应该用的是ordinal regression（定序回归）。当然，这部分绝对不会考。。。感兴趣的可以先去看看我之前推荐过的说人话的统计学 因为通常我们遇到的一般都是二元的响应变量，即预测结果只有两类（下雨不下雨，患病不患病），包括课上讲的也是二元的，所以后面我提到的都是二元的。 在做logistic回归的时候，我们也会将我们最后的二元结果用数字来表示，一个代表1，一个代表0。我们最后预测能得到的是y=1 的概率！ logistic模型 我们来看下logistic的模型 \[ ln(\frac{p}{1-p}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdot\cdot\cdot + \beta_p X_p + \epsilon \] 其中 p 代表 y = 1 的概率，x 代表了不同的自变量，\(\epsilon\)表示了误差项。与线性回归模型对比，等式右边完全相同，实际上逻辑回归模型也是广义上的线性模型。而等式的左边形式更复杂了，引入了一些非线性的变换。大家可以看到，我们这里等式左边变成了一个对数，我们常称为对数发生比（log-odd）或分对数（logit）。对数里面的\(\frac{p}{1-p}\)就是发生比（odd），取值范围可以从0到正无穷。 然后我们估计回归参数的话，就变成 \[ ln(\frac{\hat{p}}{1-\hat{p}}) = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdot\cdot\cdot + \hat{\beta}_p x_p \] 有时候，也会将logistic的模型写成（我不太喜欢这么写，但有时候看助教答案是这么写的） \[ \hat{y}=\hat{p}=\frac{e^{\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdot\cdot\cdot + \hat{\beta}_p x_p}}{1+e^{\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdot\cdot\cdot + \hat{\beta}_p x_p}} \] \[ =\frac{1}{1+e^{-({\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdot\cdot\cdot + \hat{\beta}_p x_p})}} \] 这里的y就直接表示 y=1 的概率了。我感觉不太直观…… 下面是logistic回归的一些tip 发生比（odd）这个概念估计是会考到的。 因为我们通常来说，会把成功写作1，失败写作0。那么发生比就是成功与失败的比例，有时候就写作 odds of success。 谁是0，谁是1的设置其实是很重要的，如果设置反了，就变成预测对立的概率了。这点我在后面例子的时候再详细地讲。 logistic回归的模型显著性和参数显著性我就不讲了，一来我不太懂╮（╯＿╰）╭，二来老师上课也没怎么提。 对于我们前面提到的逻辑斯蒂回归模型而言，系数的意思（比如\(\beta_1\)）代表的是可以解释为在所有其他预测变量保持不变的情况下，\(X_1\)增加一个单位，对数发生比的变化为\(\beta_1\)。 R语言的实现 用两个作业里面的题目举个例子： 题目1 数据文件“Drivers.csv”为对45名司机的调查结果，其中四个变量的含义为： x1：表示视力状况，它是一个分类变量，1表示好，0表示有问题； x2：年龄，数值型； x3：驾车教育，它也是一个分类变量，1表示参加过驾车教育，0表示没有； y：一个分类型输出变量，表示去年是否出过事故，1表示出过事故，0表示没有； 把这部分数据称为test5，已经放入Part0 123# 读取并整理数据test5 &lt;- read.table("rawdata/homework-6.5-Drivers.csv",header = T,sep = ",")colnames(test5) &lt;- c("eyesight","age","eduction","accident") 12345678&gt; head(test5) eyesight age eduction accident1 1 17 1 12 1 44 0 03 1 48 1 04 1 55 0 05 1 75 1 16 0 35 0 1 1.请在R语言中调用logistic回归函数，计算视力状况、年龄、驾车教育与是否发生事故的logistic回归模型，并以“odds=……”的形式写出回归公式。 R语言里面调用logistic回归函数是用glm函数。glm函数其实是拟合广义线性模型的函数，logistic回归只是其中一种。所以你要加上family=binomial，代表逻辑斯蒂回归 1234567891011121314151617181920212223242526&gt; test5_logistic &lt;- glm(accident ~ .,data = test5,family = binomial())&gt; summary(test5_logistic)Call:glm(formula = accident ~ ., family = binomial(), data = test5)Deviance Residuals: Min 1Q Median 3Q Max -1.5636 -0.9131 -0.7892 0.9637 1.6000 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.597610 0.894831 0.668 0.5042 eyesight -1.496084 0.704861 -2.123 0.0338 *age -0.001595 0.016758 -0.095 0.9242 eduction 0.315865 0.701093 0.451 0.6523 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 62.183 on 44 degrees of freedomResidual deviance: 57.026 on 41 degrees of freedomAIC: 65.026Number of Fisher Scoring iterations: 4 然后我们的logistic回归模型就是 \[ ln(\frac{P_{事故}}{1-P_{事故}})=0.5976-1.496X_1-0.0016X_2+0.3159X_3 \] 但题目要求的是odds的形式，那么我们再改写下 \[ \frac{P_{事故}}{1-P_{事故}}=e^{0.5976-1.496X_1-0.0016X_2+0.3159X_3} \] 2.指出（1）得到的模型中哪些因素对是否发生事故有显著性影响。如果存在对是否发生事故没有显著性影响的因素，请去除这些因素后重新计算logistic回归模型，并以“p=……”的形式写出回归公式 我们根据前面结果的p-value，就发现视力是由显著性影响的，其他因素没有显著性影响。 然后我们去掉这些因素，只留下视力这个因素，再次拟合一个logistic回归模型 1234567891011121314151617181920212223242526272829&gt; test5_logistic_reduced &lt;- glm(accident ~ eyesight,data = test5, family = binomial())&gt; summary(test5_logistic_reduced)Call:glm(formula = accident ~ eyesight, family = binomial(), data = test5)Deviance Residuals: Min 1Q Median 3Q Max -1.4490 -0.8782 -0.8782 0.9282 1.5096 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.6190 0.4688 1.320 0.1867 eyesight -1.3728 0.6353 -2.161 0.0307 *---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 62.183 on 44 degrees of freedomResidual deviance: 57.241 on 43 degrees of freedomAIC: 61.241Number of Fisher Scoring iterations: 4# 可以输出下截距，虽然前面的结果也已经输出截距了&gt; coefficients(test5_logistic_reduced)(Intercept) eyesight 0.6190392 -1.3728110 这回是以p的形式 \[ P_{事故}=\frac{e^{0.6190-1.3728X_1}}{e^{0.6190-1.3728X_1}+1} \] 我们还可以比较下，这两个方程在统计学上是不是有差异的，就是跟线性回归一样用anova函数 12345678&gt; anova(test5_logistic,test5_logistic_reduced,test = "Chisq")Analysis of Deviance TableModel 1: accident ~ eyesight + age + eductionModel 2: accident ~ eyesight Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)1 41 57.026 2 43 57.241 -2 -0.21572 0.8978 发现两个模型是没有差异的。 3.A是一名参加过驾车教育，但视力有问题的50岁老司机；B是一名没有参加过驾车教育，但视力良好的20岁新手。现在A、B都想在某保险公司投保，但按公司规定，被保险人必须满足“明年出事故的概率不高于40%”的条件才能予以承保。请预测A、B两者明年出事故的概率，并告诉保险公司谁可以投保。 这里就是用我们构建的模型去预测结果。但有一个问题就是拿哪个模型去预测呢。答案里面是用了精简以后的（去掉了不显著的变量）模型。 我们可以都看下，反正就一行代码的事情。考试的时候，我觉得用精简的模型吧。虽然我也不太清楚啥时候去掉变量。 123456789101112131415161718# 构建测试数据框# 注意列名都必须和原来的数据框（或者说你用来构建模型的变量）列名是一致的&gt; data &lt;- data.frame(eyesight=c(0,1),age=c(50,20),eduction=c(1,0))&gt; rownames(data) &lt;- c("A","B")&gt; data eyesight age eductionA 0 50 1B 1 20 0# 利用predict进行预测# 添加到原来数据框会更加直观一点。# 注意要写type="response"&gt; data$prob &lt;- predict(test5_logistic,data,type="response")&gt; data$prob1 &lt;- predict(test5_logistic_reduced,data,type="response")&gt; data eyesight age eduction prob prob1A 0 50 1 0.6971400 0.65B 1 20 0 0.2828481 0.32 题目2 这题的数据比较多，且比较麻烦。我就不放数据了，写出来只是为了讲下参考变量和训练测试集等几个概念。 这题的大致目的就是用10个自变量去做出诊断，肿瘤是否是良性的（M = malignant（恶性的）, B = benign（良性的））。总的来说，数据集有357个良性肿瘤，212个恶性肿瘤，即共有569个数据点。 123&gt; head(test_mean,n = 1) diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave.points_mean symmetry_mean fractal_dimension_mean1 M 17.99 10.38 122.8 1001 0.1184 0.2776 0.3001 0.1471 0.2419 0.07871 1. Use all mean features to construct a logistic regression model 因为原始数据集是包括了mean，var等等。这里只要求用mean部分的数据。所以我们就先提取了mean那部分数据集，然后还是用glm。 1234567891011121314151617181920212223242526272829303132333435363738394041# 读取，提取数据test &lt;- read.table("rawdata/homework-6.6-data.csv",header = T,sep = ",")test_mean &lt;- test[,2:12]# 构建logistic回归模型，formula那边我用一个.代替了所有的变量，这样写写更方便&gt; test_logistic &lt;- glm(diagnosis ~ . , data = test_mean, family = binomial())Warning message:glm.fit: fitted probabilities numerically 0 or 1 occurred &gt; summary(test_logistic)Call:glm(formula = diagnosis ~ ., family = binomial(), data = test_mean)Deviance Residuals: Min 1Q Median 3Q Max -1.95590 -0.14839 -0.03943 0.00429 2.91690 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -7.35952 12.85259 -0.573 0.5669 radius_mean -2.04930 3.71588 -0.551 0.5813 texture_mean 0.38473 0.06454 5.961 2.5e-09 ***perimeter_mean -0.07151 0.50516 -0.142 0.8874 area_mean 0.03980 0.01674 2.377 0.0174 * smoothness_mean 76.43227 31.95492 2.392 0.0168 * compactness_mean -1.46242 20.34249 -0.072 0.9427 concavity_mean 8.46870 8.12003 1.043 0.2970 concave.points_mean 66.82176 28.52910 2.342 0.0192 * symmetry_mean 16.27824 10.63059 1.531 0.1257 fractal_dimension_mean -68.33703 85.55666 -0.799 0.4244 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 751.44 on 568 degrees of freedomResidual deviance: 146.13 on 558 degrees of freedomAIC: 168.13Number of Fisher Scoring iterations: 9 大家可以在输出结果里面看到这10个变量。 2. Then try to reduce the number of features from your last model, construct another regression model,and you will need to write down the equation of your logistic regression model(Tips: Logit P = α+β1X1+β2X2+..+βpXp) 我们可以把显著性的变量挑出来，再次构建一个新的logistic回归模型 123456789101112131415161718192021222324252627282930&gt; test_logistic_reduced &lt;- glm(diagnosis ~ texture_mean + area_mean + smoothness_mean + concave.points_mean, data = test_mean, family = binomial())Warning message:glm.fit: fitted probabilities numerically 0 or 1 occurred &gt; summary(test_logistic_reduced)Call:glm(formula = diagnosis ~ texture_mean + area_mean + smoothness_mean + concave.points_mean, family = binomial(), data = test_mean)Deviance Residuals: Min 1Q Median 3Q Max -2.31798 -0.15623 -0.04212 0.01662 2.84201 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -23.677816 3.882774 -6.098 1.07e-09 ***texture_mean 0.362687 0.060544 5.990 2.09e-09 ***area_mean 0.010342 0.002002 5.165 2.40e-07 ***smoothness_mean 59.471304 25.965153 2.290 0.022 * concave.points_mean 76.571210 16.427864 4.661 3.15e-06 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 751.44 on 568 degrees of freedomResidual deviance: 156.44 on 564 degrees of freedomAIC: 166.44Number of Fisher Scoring iterations: 8 回归模型为 \[ ln(\frac{P_{M}}{1-P_{M}})=-23.677816+0.362687*X_1+0.010342*X_2+59.471304*X_3+76.571210*X_4 \] 3. Use proper test to test the difference between two models 用anova就可以了 12345678910&gt; anova(test_logistic,test_logistic_reduced,test = "Chisq")Analysis of Deviance TableModel 1: diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_meanModel 2: diagnosis ~ texture_mean + area_mean + smoothness_mean + concave.points_mean Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)1 558 146.13 2 564 156.44 -6 -10.31 0.1122 4. You may split the data properly, use part of them to train your regression model and use another part to make predictions. Lastly, you may try to calculate the accuracy of your model.(Tips: To split the data, you can use the first 398 rows as training data, use the last 171 rows as prediction data.The predict function return a value between 0 and 1, 0.~0.5 belong to the first class, and 0.5~1 belong to second class in binary classification problems) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 构建训练集和测试集数据data_train &lt;- test[1:398,]data_test &lt;- test[399:569,]# 利用训练集构建logistic回归模型&gt; test_train_logistic &lt;- glm(diagnosis ~ texture_mean + area_mean + smoothness_mean + concave.points_mean, data = data_train, family = binomial())&gt; summary(test_train_logistic)Call:glm(formula = diagnosis ~ texture_mean + area_mean + smoothness_mean + concave.points_mean, family = binomial(), data = data_train)Deviance Residuals: Min 1Q Median 3Q Max -2.39278 -0.14454 -0.02447 0.03635 2.60665 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -27.47397 4.74798 -5.786 7.19e-09 ***texture_mean 0.46244 0.08434 5.483 4.19e-08 ***area_mean 0.01082 0.00235 4.606 4.11e-06 ***smoothness_mean 90.11221 30.96961 2.910 0.003618 ** concave.points_mean 59.01212 17.51779 3.369 0.000755 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1) Null deviance: 544.93 on 397 degrees of freedomResidual deviance: 108.30 on 393 degrees of freedomAIC: 118.3Number of Fisher Scoring iterations: 8# 先把测试集里面真实的诊断结果提出来diagnosis_pred &lt;- data_test[,"diagnosis",drop=F]# 用模型去预测测试集的结果，并把预测的概率结果跟真实结果放一起diagnosis_pred$pred &lt;- predict(test_train_logistic,data_test,type="response")# 把&gt;0.5的定义为M，即恶性。把&lt;0.5定义为B，即良性diagnosis_pred$pred_result &lt;- ifelse(data_test$pred &gt; 0.5, "M","B")# 然后就拿预测的结果跟真实的结果进行比较，并计算有多少是TRUE的。就是所谓的accuracy&gt; mean(diagnosis_pred$diagnosis == diagnosis_pred$pred_result)[1] 0.9064327 这样一套下来，大家可能会感觉有些奇怪。在题目1的时候，1代表出事故，0代表没出事故。然后\(ln(\frac{p}{1-p})\) 里面的p代表的是y=1的概率，即出事故的概率。这一切都很顺理成章。但是在题目2的时候，M代表恶性的，B代表良性的。那为啥\(ln(\frac{p}{1-p})\) 里面的p代表的就是M呢。或者说为啥M代表的就是1，B代表的就是0呢。 以下纯属个人观点 事实上，对于二元变量，glm会确定你的响应变量里面谁是reference level。或者说，会确定谁是那个0。那么，glm是怎么确定的呢。 如果你的响应变量本身就是0和1了，那么自然0就是reference level了。 这时候，只要响应变量是0和1，那么其就不需要是因子型的变量。数值型的0和1也是可以的 如果你的响应变量是字符串型的，那么首先，你的响应变量必须是因子型的变量才可以让glm决定谁是reference level，即谁是那个0。我们有两种方法来知道谁是reference level。 利用str或者直接输出响应变量（本来应该是test_mean做示例的，我懒得再改了……） 12345678910# 可以看到 levels 后面，排在最前面的就是reference level，即B是reference level&gt; str(test)'data.frame': 569 obs. of 32 variables: $ id : int 842302 842517 84300903 84348301 84358402 843786 844359 84458202 844981 84501001 ... $ diagnosis : Factor w/ 2 levels "B","M": 2 2 2 2 2 2 2 2 2 2 ... &gt; test$diagnosis……[532] B B M B M M B B B B B B B B B B B B B B B B B B B B B B B B B M M M M M M BLevels: B M 利用contrast 12345# 清除地显示了B是0，M是1&gt; contrasts(test$diagnosis) MB 0M 1 如果你的响应变量既不是0和1，也不是因子型的变量，就会报错 1234# 把B和M变成字符串型的&gt; test_mean$diagnosis &lt;- as.character(test_mean$diagnosis)&gt; test_logistic &lt;- glm(diagnosis ~ . , data = test_mean, family = binomial())Error in eval(family$initialize) : y values must be 0 &lt;= y &lt;= 1 当然，如果你的响应变量是数值型，但还是没有变成因子型，照样还是会报错，可以自己试试。 通过上面的讲述，我们就发现了B是reference level，即是0。而M是对立的那个level，即是1。而我们logistic输出的是y=1的概率，即y=恶性肿瘤的概率。然后也刚好对应的我们前面的 12# 把&gt;0.5的定义为M，即恶性。把&lt;0.5定义为B，即良性diagnosis_pred$pred_result &lt;- ifelse(data_test$pred &gt; 0.5, "M","B") 只有y=M的概率足够大，才定义为M。这里我们设定的足够大是0.5。你也可以认为大于0.9才算出是M，这样结果就会不太一样。 关于ifelse，可以去看下《R语言实战》第二版的p435，这里不再讲述。 我们也可以把我们的响应变量直接变成0和1 1&gt; test_mean$diagnosis &lt;- factor(test_mean$diagnosis,levels = c("B","M"), labels = c(0,1)) 那么，我们该如果更改我们的reference level呢，一种方法我觉得应该可以是像上面那样，响应变量直接变成0和1，非常直观，但个人觉得不太符合逻辑……。另一种就是下面那样 123456789101112test_mean$diagnosis &lt;- relevel(test_mean$diagnosis, ref = "M")&gt; test_mean$diagnosis……[532] B B M B M M B B B B B B B B B B B B B B B B B B B B B B B B B M M M M M M BLevels: M B# 现在M变了0，B变成了1&gt; contrasts(test_mean$diagnosis) BM 0B 1 最后强调下，glm会找谁是reference level，即谁是0。但我们最终得到的概率是对立的level的概率！ 参考文章 https://stats.stackexchange.com/questions/207427/confused-with-the-reference-level-in-logistic-regression-in-r https://stackoverflow.com/questions/17772775/change-reference-group-using-glm-with-binomial-family https://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part16]]></title>
    <url>%2F2019%2F06%2F14%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part16%2F</url>
    <content type="text"><![CDATA[多元线性回归 多元线性回归的方程写为： \[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdot\cdot\cdot + \beta_p X_p + \epsilon \] 其中\(X_j\)代表第\(j\)个预测变量，\(\beta_j\)是对应的模型参数。\(\beta_j\)可以解释为在所有其他预测变量保持不变的情况下，\(X_j\)增加一个单位对\(Y\)产生的平均影响。（这段话其实很重要，我觉得有可能要考的） 和前面的一元线性回归类似，我们需要用\(\hat{\beta}_0,\hat{\beta}_1,……,\hat{\beta}_p\)来估计\(\beta_0,\beta_1,……,\beta_p\)。对于给定的\(\hat{\beta}_0,\hat{\beta}_1,……,\hat{\beta}_p\)，我们的回归模型就变成了 \[ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdot\cdot\cdot + \hat{\beta}_p x_p \] 与一元线性回归类似，多元线性回归的参数也是用最小二乘法进行估计的，也是使得残差平方和（RSS）最小。计算方法的话，似乎是矩阵代数最为方便。一来我也不会╮（╯＿╰）╭，二来老师PPT也没有放，所以这里就不算了。 然后我们还是按照一元线性回归的步骤，首先去检验整个方程的显著性。 模型的显著性检验（模型的意义） 假设检验第一步首先是建立零假设和备则假设（这里跟一元就不一样了）。 \[ H_{0}: \beta_1 = \beta_2 =... = \beta_p = 0 \] \[ H_{1}: 至少一个\beta_j不等于0 \] 统计检验的F统计量为 \[ F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)} \sim F(p,n-p-1) \] 然后计算p-value就可以了。 模型参数的显著性 还记得我们在简单线性回归中提到的，模型参数的准确性么。我们利用了 t 统计量，即可以用来做模型参数的置信区间，也可以用来做模型参数的统计检验。但在一元线性回归中，t 检验跟 F 检验几乎可以说是等价的，因为只有一个参数，你检验了那一个参数的显著性，就是等于检验了整个方程的显著性。但在这里，t 检验就非常有用了。其可以检验每个模型参数的统计显著性，而F检验做的，是整个模型的统计检验性。 假设检验： \[ H0:\beta_j=0,其他模型参数不为0 \] \[ H1:\beta_j \ne0，其他模型参数不为0 \] 然后统计量为： \[ t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})} \sim t(n-p-1) \] 然后计算 p-value 就行了。 模型的比较 我们先前讲过了 F 统计量是检验整体的统计量，t 统计量可以检验单参数的统计量，其实我们还可以检验部分参数的统计量（这个考试的时候，应该是用anova函数来比较含有不同参数的模型）。 在前面 F 统计量的假设检验是假设所有系数都是零。但这里我们想要检验的是规模为q的特定子集为0。 \[ H0:\beta_{p-q+1}=\beta_{p-q+2}……=\beta_{p}=0，其他剩余的p-q个不等于0 \] \[ H1:规模为q的特定子集不都为0，其他剩余的p-q个不等于0 \] 为方便起见，我们用除了q个变量之外的所有变量建立第二个模型。假设该模型的残差平方和为\(RSS_0\)，相应的F统计量为 \[ F=\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)} \sim F(q,n-p-1) \] 这部分我觉得不太需要掌握。唯一需要掌握的是利用anova函数比较去掉变量前后的模型，这个后面会给出例子。 评估模型的准确性 这部分还是用残差标准误和\(R^2\)统计量暂时应该就可以了。残差标准误稍微注意下 \[ RSE = \sqrt{\frac{RSS}{n-p-1}} \] 参考文章 《统计学习导论》 Ｒ语言实现 R 语言实现的函数就是一个 lm 函数，用法见《R语言实战》第二版162页。我这里放个截图 我这里用的是我们生统第六次作业的第二题，我将这个数据集命名为test3（数据集已放入part0部分）。 某农场通过试验取得早稻收获量与春季降雨量和春季温度的数据如下： 123456789101112# 准备数据&gt; test3 &lt;- read.table("rawdata/homework-6.2.txt",header = T)&gt; colnames(test3) &lt;- c("weight","rainfall","tempature")&gt; test3 weight rainfall tempature1 2250 25 62 3450 33 83 4500 45 104 6750 105 135 7200 110 146 7500 115 167 8250 120 17 建立早稻收获量对春季降雨量和春季温度的二元线性回归方程。 用的是lm函数，用summary函数展现结果 123456789101112131415161718192021&gt; test3_lm &lt;- lm(weight ~ ., data = test3)&gt; summary(test3_lm)Call:lm(formula = weight ~ ., data = test3)Residuals: 1 2 3 4 5 6 7 -275.101 90.464 216.483 140.280 150.676 -316.599 -6.203 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.591 505.004 -0.001 0.9991 rainfall 22.387 9.601 2.332 0.0801 .tempature 327.672 98.798 3.317 0.0295 *---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 261.4 on 4 degrees of freedomMultiple R-squared: 0.9913, Adjusted R-squared: 0.987 F-statistic: 228.4 on 2 and 4 DF, p-value: 7.532e-05 写出方程 \[ y= -0.591+22.387x1+327.672x2 \] 检验模型的显著性（模型的意义）** 第一步 提出假设，设置显著性水平： \[ H_{0}: \beta_1 = \beta_2 = 0 \] \[ H1:不都为0 \] 显著性水平\(\alpha\)设置为0.05 第二步 计算检验统计量 F F统计量为 \[ F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)} \sim F(p,n-p-1) \] 第三步 做出决策。按 R 输出结果为 228.4，显著性水平 P=7.532e-05 &lt; 0.05。所以拒绝 \(H_0\)，即收获量y与降雨量和温度之间的线性关系显著。 判断每个自变量对因变量的影响是否都是显著的 需要对各回归系数\(\beta_j\)分别进行 t 检验，假设检验和t统计量见前面的模型参数的显著性。根据前面线性回归输出的结果，可以看到降雨量和温度的回归系数对应的显著性水平分别为0.0801和0.0295。只有温度对应的显著性水平小于0.05通过检验，这表明影响收获量的自变量中，只有温度对收获量的影响显著，而降雨量对收获量的影响不显著。 各回归系数的置信区间（根据前面截图的函数） 12345&gt; confint(test3_lm) 2.5 % 97.5 %(Intercept) -1402.707516 1401.52552rainfall -4.268921 49.04184tempature 53.364699 601.97873 检验两个模型的区别 因为先前我们发现降雨量对收获量的影响不显著，所以我们剔除到降雨量，再重新构建一个模型。然后拿新模型和旧模型比较，看去掉的那个变量是否重要 123456789101112131415161718192021222324252627282930313233# 重新构建模型&gt; test3_lm_new &lt;- lm(weight ~ tempature, data = test3)&gt; summary(test3_lm_new)Call:lm(formula = weight ~ tempature, data = test3)Residuals: 1 2 3 4 5 6 7 -150 -50 -100 500 400 -400 -200 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -900.00 447.82 -2.01 0.101 tempature 550.00 35.56 15.47 2.05e-05 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 359.2 on 5 degrees of freedomMultiple R-squared: 0.9795, Adjusted R-squared: 0.9754 F-statistic: 239.2 on 1 and 5 DF, p-value: 2.052e-05# 利用anova函数比较&gt; anova(test3_lm,test3_lm_new)Analysis of Variance TableModel 1: weight ~ rainfall + tempatureModel 2: weight ~ tempature Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 4 273385 2 5 645000 -1 -371615 5.4372 0.08009 .---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 稍微要注意的一点就是，在后面逻辑斯蒂回归的（广义线性模型）比较中，anova要设置test="Chisq"。 其余一些结果 残差标准误（似乎也有叫剩余标准差的）：261.4 \(R^2\)统计量：0.9913。 这里你会发现还有一个 \(adjust\ R^2\)。因为实际上只要你的变量加的足够多，你的\(R^2\)就肯定会增加，哪怕你的变量跟你的Y值是无关的。所以就会引入adjust，来适当调整\(R^2\)。我觉得考试时候两个值可以都写。 置信区间与预测区间 这部分我觉得大家不用看，我只是写一下，考试肯定也不会考。 先提下可约和不可约误差。 我们利用统计模型，对Y做了预测，得到预测值\(\hat Y\)。预测值的精确度取决于两个方面，一方面是可约误差，另一方面是不可约误差。其中可约误差是来自于我们所构建的模型。如果我们构建的模型不是那么好，可约误差就会大一点。但我们可以通过优化我们的模型，比如增加一些变量或者把线性变成非线性等等，来降低这种误差。然而，尽管我们可以得到一个精确的模型，我们还是不能减少不可约误差。因为不可约误差是跟误差项，即我们在最一开始的时候写过 \[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdot\cdot\cdot + \beta_p X_p + \epsilon \] 里面的\(\epsilon\)有关的。按照定义，\(\epsilon\) 是不能用X去预测的。所以我们是无法去减少这部分误差的。那么其实最后的误差来源就表示为 \[ E(Y-\hat{Y})^2=E[f(X)+\epsilon-\hat f(X)]^2 \] \[ =[f(X)-\hat f(X)]^2+Var(\epsilon) \] 我为啥要提上面这个呢。因为其实在针对线性回归有一个函数叫predict，里面有个参数叫interval。大家可以看下。 123456## S3 method for class 'lm'predict(object, newdata, se.fit = FALSE, scale = NULL, df = Inf, interval = c("none", "confidence", "prediction"), level = 0.95, type = c("response", "terms"), terms = NULL, na.action = na.pass, pred.var = res.var/weights, weights = 1, ...) 书里面一般写的是prediction。prediction其实就是在构建区间的时候考虑了误差项的影响，而confidence则没有考虑误差项的影响。这就会导致预测区间比置信区间要宽很多。 顺便提一下，\(y_0=E(y_0)+\epsilon\)。\(E(y_0)\)就是我们通常用线性回归预测出来的值。而confidence构建的就是\(E(y_0)\)的区间，而prediction构建的就是\(y_0\)的区间。 12345678910111213141516171819# 利用先前的结果，看下区别。可以看到预测区间更宽一点。&gt; predict(test3_lm,test3,interval = "confidence") fit lwr upr1 2525.101 1992.745 3057.4572 3359.536 2928.396 3790.6763 4283.517 3795.736 4771.2984 6609.720 6096.023 7123.4175 7049.324 6620.305 7478.3426 7816.599 7407.001 8226.1987 8256.203 7748.618 8763.789&gt; predict(test3_lm,test3,interval = "prediction") fit lwr upr1 2525.101 1624.957 3425.2452 3359.536 2515.298 4203.7743 4283.517 3408.996 5158.0384 6609.720 5720.483 7498.9565 7049.324 6206.167 7892.4816 7816.599 6983.156 8650.0437 8256.203 7370.483 9141.923 参考文献 《统计学习导论-基于R应用》 Y叔的统计笔记]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part15]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part15%2F</url>
    <content type="text"><![CDATA[相关性 当我们在衡量两个变量的相关关系的时候，我们可以用协方差来进行描述。协方差的公式为 \[ Cov(X,Y) = E[(X-E(X))(Y-E(Y))] \] 协即协同的意思，X的方差是\(X-E(X)\)与\(X-E(X)\)的乘积的期望，如今把一个\(X-E(X)\)换成\(Y-E(Y)\)，其形式接近方差，又有\(X,Y\)二者的参与，由此得出协方差的名称。 虽然协方差能反映两个随机变量的相关程度（协方差大于0的时候表示两者正相关，小于0的时候表示两者负相关），但是协方差值的大小并不能很好地度量两个随机变量的关联程度，例如，现在二维空间中分布着一些数据，我们想知道数据点坐标X轴和Y轴的相关程度，如果X与Y的相关程度较小但是数据分布的比较离散，这样会导致求出的协方差值较大，用这个值来度量相关程度是不合理的。 所以，为了更好地度量两个随机变量的相关程度，引入了Pearson相关系数，其在协方差的基础上除以了两个随机变量的标准差，容易得出，pearson是一个介于-1和1之间的值，当两个变量的线性关系增强时，相关系数趋于1或-1；当一个变量增大，另一个变量也增大时，表明它们之间是正相关的，相关系数大于0；如果一个变量增大，另一个变量却减小，表明它们之间是负相关的，相关系数小于0；如果相关系数等于0，表明它们之间不存在线性相关关系。 皮尔逊相关系数（Pearson Correlation）公式如下： \[ \rho(X,Y)=\frac{Cov(X,Y)}{\sigma{X}\sigma{Y}} \] 上面应该表示的是总体的相关系数\(\rho\)。我们会将样本的相关系数用r表示。我把 r 的公式写的再详细一点。 \[ r = \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}} \] R语言里面的协方差和相关系数的函数分别是cov 和 cor。输入参数可以是向量，也可以是矩阵，如果是矩阵，将对每个column两两计算。比如我们用鸢尾花（iris）的数据集做测试。 1234567# 因为第5列是字符串，所以不算相关性。&gt; cor(iris[,-5]) Sepal.Length Sepal.Width Petal.Length Petal.WidthSepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 我觉得考试的时候，相关系数也会让你算的。 好像去年的题目里有要做相关系数的检验，但老师上课也没讲，作业也没提。反正如果要检验的话，就用 cor.test()吧 相关系数也常称为线性相关系数。这是因为，实际上相关系数并不是刻画了X，Y之间一般关系的程度，而只是线性关系的程度。 除了皮尔逊相关系数，还有Sperman相关系数，其刻画的是秩次相关。如果你?cor，会在method那边看到spearman。不过cor默认的相关系数计算是皮尔逊。 参考文章： https://www.zhihu.com/question/19734616 TimXP的答案 Y叔的统计笔记 一元线性回归 参数模拟 一元线性回归的方程写为 \[ Y=\beta_0+\beta_1X+e \] \(\beta_0和\beta_1\)分别代表线性模型的截距和斜率，被称为模型的系数（coefficient）或者参数（parameter）。e是误差项。误差项满足一个正态分布，\(e \sim N(0,\sigma^2)\)。 但由于模型的参数是未知的，我们就需要用训练数据来估计模型参数，我们可以更根据给定的 (x,y) 来估计出模型参数。这样，我们就可以根据给定的 x 来估计 y 了。 \[ \hat{y} = \hat{\beta_0} + \hat{\beta_1} x \] 关于误差项的问题，暂时不在这里深究了。 但我们实际上可以选择无数的模型参数，我们怎么选择最好的那一组模型呢。我们依据的准则就是希望我们模拟出来的那根直线尽可能地靠近数据点（接近程度，closeness）。衡量接近程度的方法有许多，但对于线性回归而言，最常用的还是残差平方和最小化准则（最小二乘法，least squares）。 对于残差平方和最小化，我们需要一些推导和定义的步骤 首先我们先定义残差（residual）： \[ e=y-\hat{y}=y-(\hat{\beta_0} + \hat{\beta_1} x) \] 残差平方和（ residual sum of squares，RSS，Res SS） \[ RSS = {e_1}^2 + {e_2}^2 + … + {e_n}^2 \] \[ e_i = y_i - \hat{y_i} \] 最后根据一番神奇的推导╮（╯＿╰）╭，我们就可以算出两个参数的值了。 \[ \hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \] \[ \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} \] 稍微指代一下。 \[ L_{xy}=\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \] \[ L_{xx}=\sum_{i=1}^n (x_i - \bar{x})^2 \] \[ L_{yy}=\sum_{i=1}^n (y_i - \bar{y})^2 \] 那么\(\beta_1\)就变成了 \[ \hat{\beta_1}=\frac{L_{xy}}{L_{xx}} \] 模型的显著性检验（模型的意义） 在利用最小二乘法得到了参数之后，我们就可以建立一个模型了。之后需要做的一步就是检验这个模型是否具有显著性，或者说我们所谓的模型是否有意义。即（x，y）之间是否具有线性关系。 当然，在讲下面的假设检验之前，再次引入一些概念。 总平方和（ total sum of square，Total SS，TSS），即我们前面所说的\(L_{yy}\)。指的是在执行回归分析之前响应变量（因变量）中的固有变异性 回归平方和（regression sum of squares，Reg SS 或者 explained sum of squares，ESS），即响应变量执行回归分析之后，被解释的变异性。计算是利用总平方和减去残差平方和。 \[ Reg\ SS=Total\ SS -Res \ SS=TSS-RSS \] 残差平方和：现在我们对于残差平方和有了更深的认识，即其测量的是进行回归之后仍无法解释的变量。 在讲完概念之后，我们就可以做假设检验了。 假设检验第一步首先是建立零假设和备则假设。 \[ H0:\beta_1=0 \\ H1:\beta_1 \ne 0 \] 显著性 \(\alpha = 0.05\) 这里我们用的是 F 统计量来做假设检验。其实原理跟之前的ANOVA比较像，ANOVA比较的是组间差异和组内差异，而线性回归的统计检验比较的是回归平方和 和 残差平方和的大小。同样的，我们还要考虑自由度的问题。所以最后 F 统计量的式子为 \[ F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}=\frac{Reg\ SS/p}{Res\ SS/(n-p-1)}=\frac{Reg\ MS}{Res MS} \] 这里的 p 是预测变量的数目，n是数据点的数目。这里写成这样是方便后面的多元线性回归再写一遍。 因为我们只有一个预测变量，那么 F 统计量就是 \[ F = \frac{TSS-RSS}{RSS/(n-2)} \] 前面忘说了，我们一般将MS称为Mean Squre，即均方。 参数的准确性 我们之前提到用点估计来估计总体参数并不是那么的准确。这里也是同理，我们用模型参数\(\hat{\beta_0} 和 \hat{\beta_1}\) 来估计真实的参数也不是那么地准确。我们之前用样本均值的抽样分布来估计我们的样本均值距离我们的真值有多远，同时构建置信区间。这里我们也可以用同样的方法。 在之前构建样本均值的抽样分布中，最重要的一个概念就是标准误（standard error，se）。这里我们也需要构建两个参数的标准误，利用下面的公式 \[ SE(\hat{\beta_0})^2 = \sigma^2 [\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i-\bar{x})^2}] \] \[ SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2} \] 可以发现，我们这里未知的就是 \(\sigma^2\) 这个值了。这个值的计算是 \(\sigma^2 = Var(\epsilon)\)，但通常是未知的。我们就需要用残差标准误（Residual Standard Error）来估计。 \[ RSE=\sqrt{RSS/(n-p-1)} \] 跟上面一样，方便后面的多元线性回归再写一遍。由于我们就一个变量，所以 \[ RSE=\sqrt{RSS/(n-2)} \] 标准误差已经得到了，我们可以用来构建置信区间，也可以用来做参数检验了。这里不做演示，因为最后 lm 函数会输出一切。我这里只是把 t 统计量写下。 假设检验为： \[ H0:\beta_1=0 \\ H1:\beta_1 \ne 0 \] 然后 t 统计量就是： \[ t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})} \sim t(n-p-1) \] 老样子，这里就一个参数是，所以是 t（n-2）。 这里的t检验其实等价于我们之前的F检验，因为就一个参数，我们F检验也是检验\(\beta_1\)是否等于0。 但到了后面的多元线性回归，我们利用 t 检验做的是一个个做参数的统计检验，而 F 检验检验的则是一个整体模型的显著性。 评估模型的准确性 一旦我们接受了备则假设，即这个模型的线性关系是显著的。那我们就会想要量化模型拟合数据的程度。判断模型拟合的准确性主要有两个相关的量化指标：残差标准误和\(R^2\)统计量 残差标准误 我们之前已经提到过了残差标准误的计算方法。（残差标准误其实是对误差项的标准偏差的估计。大体而言，它是响应值偏离真正的回归直线的平均量。这段话不看也行╮（╯＿╰）╭。） 再次提下残差标准误的计算： \[ RSE = \sqrt{\frac{RSS}{n-2}} = \sqrt{\frac{\sum_{i=1}^n (y_i-\hat{y_i})^2}{n-2}} \] \(R^2\)统计量 RSE提供了一个模型对数据拟合的绝对测量方法。但其是一个绝对值，并不能很直观地展现模型拟合的能力。所以我们就会用\(R^2\)统计量来展现。 \[ R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS} \] 里面的每个元素我们前面都已经讲过了。总体而言，\(R^2\)统计量测量的是Y的变异中能被X所解释的部分所占比例。\(R^2\)接近1，说明回归能解释响应变量的大部分变异。 \(R^2\)统计量衡量了X和Y之间的线性关系。其实我们如果展开了\(R^2\)统计量的式子，会发现\(R^2=r^2\)（皮尔逊相关性）。可以说\(R^2是r^2\)的通用形式，因为相关系数只能用于单变量的情况，如果是多元线性回归，就只能用\(R^2\)统计量了。 相关性和一元线性回归关系 之前我们提到过相关性 \[ r=\frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}} \] 指代一下，就是 \[ r=\frac{L_{xy}}{\sqrt{L_{xx}L_{yy}}} \] 然后一元线性回归的\(\beta1\)是 \[ \hat{\beta_1}=\frac{L_{xy}}{L_{xx}} \] 那么 \[ r\sqrt{\frac{L_{yy}}{L_{xx}}}=\hat{\beta_1} \] 线性回归的R语言实现我放在后面的多元再说。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part14]]></title>
    <url>%2F2019%2F06%2F10%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part14%2F</url>
    <content type="text"><![CDATA[之前我们提到了如果做多次的假设检验，就要考虑多重比较矫正的问题了。那有没有只用做一次检验就可以搞定的方法呢。其实是有的，就是方差分析（ANOVA）了。不过由于ANOVA假设的是大家都是一样的，所以统计假设的意义不是那么的明确，还需要做事后的检验。 单因素ANOVA 首先把方差分析的假设性检验放出来（考试应该要写这个）： \[ H0:\mu_1=\mu_2=……=\mu_k \\ H1：\mu_1,\mu_2……\mu_k不全相等 \] 同时设定\(\alpha\) 值为0.05. 别忘了设\(\alpha\) 值为0.05 但其实H0和H1还有另一种写法，就是因素A对结果没有影响： \[ H0:\alpha_1=\alpha_2……=\alpha_k=0 \\ H1：\alpha_1，\alpha_2……不全为0 \] 这里面\(\alpha_k\) 代表的是处理的各个分组或者说水平。 ANOVA用的是 F 检验，即比较两种方差的大小。一种是组间方差，一种是组内方差。为什么这样就能检验各组均值是否是一样的呢。想象一下，我们总的变异应该来源于两部分，一部分是因素（比如某些处理，某些药物）导致的变异（组间变异），另一部分是随机抽样导致的抽样误差（组内变异）。如果大家都是来源于一个总体的，那么因素造成的变异就应该跟抽样误差的变异是差不多的。具体的推导建议大家去看书。 我这里把F检验的步骤列一下（考试应该也要写这个）。 我们假设总共有 k 组，每组有 m 个重复，总共为 n 个数。首先我们要把总平方和分解成组内平方和和组间平方和 \[ SS_{total} = SS_{between}+SS_{within} \] 总平方和为 \[ SS_{total} = \sum(x_{ij}-\bar{\bar{X}})^2 \] 组内平方和为 \[ SS_{between}=\sum_j \sum_i (\bar{X_j}-\bar{\bar{X}})^2 \] 当然组间平方和比较让人看得懂的写法应该写成下面的式子，不过老师PPT上是按上面这么写的，反正也不会让你算。。。。。： \[ SS_{between}=\sum_{j=1}^k m(\bar{X_j}-\bar{\bar{X}})^2 \] 组间平方和为 \[ SS_{within}=\sum_j \sum_i(x_{ij}-{\bar{X_j}})^2 \] 看得懂的版本 \[ SS_{within}=\sum_{j=1}^k\sum_{i=1}^{m}(x_{ij}-{\bar{X_j}})^2 \] 在计算完了组间和组内平方和之后，就可以计算 F 统计量了。 \[ F =\frac{MS_{between}}{MS_{within}}=\frac{SS_{between}/(k-1)}{SS_{within}/(n-k)} \] k-1 我们称为组间自由度 \(df_{between}\)，n-k 我们称为组内自由度 \(df_{within}\)。 当然，R里面只需要用到 aov 这个函数就可以了。不过在用 aov 这个函数之前，即做ANOVA分析之前。还需要做正态性检验，以及方差齐性检验。（就像之前讲过的 t-test 一样）。 关于这个流程，用一张PPT上的图表示： 首先检验正态性，用到的检验函数是 shapiro.test 即如果每组样本不是正态性的样本，那么就只能做非参数的ANOVA了。非参数用的函数为：kruskal.test 如果每组样本都是正态性的样本，那么接下来就要检验方差齐性。检验的函数是用的是 bartlett.test 如果方差不是齐性的，那么就要用 oneway.test()。在 oneway.test 那里设置 var.equal = FALSE（不过FASLE是默认的。。。。。。） 如果方差是齐性的，才可以最后用 aov 函数。 举个例子，用的是我们最一开始练习R操作时候的 test1数据集 12345678910111213141516171819202122232425# 读入数据，并把 seed 那列变成因子test1 &lt;- read.table("rawdata/test1.txt",header = T)test1$seed &lt;- factor(test1$seed)# 正态性检验，发现都是正态的# 假设检验为 H0：种子1的产量满足正态分布；H1：种子1的产量不满足正态分布。# 种子2,3假设同理shapiro.test(subset(test1,seed == 1)$yield)shapiro.test(subset(test1,seed == 2)$yield)shapiro.test(subset(test1,seed == 3)$yield)# 方差齐性检验，发现方差其实是齐性# H0：三组数据方差没有差别；H1：方差有差别bartlett.test(yield ~ seed, data = test1)# 都满足之后就可以用 aov 了，用summary检查结果&gt; test1_aov &lt;- aov(yield ~ seed, data = test1)&gt; summary(test1_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) seed 2 4364 2182.2 10.04 0.000586 ***Residuals 26 5649 217.3 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 可以看到aov输出的结果，把 \(SS_{between}\)、\(SS_{within}\) 、F等表示出来了。 我们之前提到过，方差分析并不能知道哪组跟哪组有差异。但如果我们想知道的话，就要用事后检验了。R 里面的函数是 TukeyHSD() 1234567891011&gt; TukeyHSD(test1_aov) Tukey multiple comparisons of means 95% family-wise confidence levelFit: aov(formula = yield ~ seed, data = test1)$seed diff lwr upr p adj2-1 -25.32727 -41.330375 -9.324171 0.00156303-1 -0.10000 -17.473293 17.273293 0.99988723-2 25.22727 8.208575 42.245971 0.0029482 稍微要注意的一点是，aov 函数要求输入的数据都是我们之前提到过的长数据，如果碰到了宽数据，记得要转换一下。 多因素ANOVA 设有两个因素A和B，两因素的ANOVA的总平方和分解为 \[ SS_{total}=SS_{A}+SS_{B}+SS_{AB}+SS_E \] 即总平方和可以分解为A因素造成的变异，B因素造成的变异，AB互作造成的变异以及抽样误差。具体的分解计算看PPT算了，太过于复杂，我觉得考试应该不用写。 两因素方差分析的假设为： 零假设有三个，分别是因素A（有l个水平）没有效果，因素B（有m个水平）没有效果，A和B互作因素没有效果 \[ H0_A：\alpha_1=\alpha_2=……=\alpha_l=0 \\ H0_B: \beta_1=\beta_2=……=\alpha_m=0 \\ H0_{AB}: ab_{ij}=0 \\ i=1,2……l;j=1,2……,m \] 备则假设就是不全为0。 两因素ANOVA的 F统计量有三个。见PPT图 再次举个例子，用的是我们之前讲过的 test4 的数据。A和B为两种因素，A有三个水平，B有2个水平。 关于 aov 函数的用法，可以去看看《R语言实战》第二版的201页。这里放出我感觉比较有用的一张截图。 12345678910111213141516171819202122232425262728293031323334353637383940# 读取和准备数据（这部分先前已经讲过了）test4 &lt;- read.table("rawdata/test4.txt",header = T)test4_long &lt;- gather(test4, key = temperature, value = weight, A1, A2, A3)test4_long$temperature &lt;- factor(test4_long$temperature)feed &lt;- c(rep("B1",10),rep("B2",10))test4_data &lt;- cbind(feed,test4_long)&gt; head(test4_data) feed temperature weight1 B1 A1 282.12 B1 A1 264.23 B1 A1 274.24 B1 A1 276.45 B1 A1 283.76 B1 A1 288.0# 正态性检验# 检验体重数据对于因素A和因素B是否是正态的shapiro.test(subset(test4_data, feed == "B1")$weight)shapiro.test(subset(test4_data, feed == "B2")$weight)shapiro.test(subset(test4_data, temperature == "A1")$weight)shapiro.test(subset(test4_data, temperature == "A2")$weight)shapiro.test(subset(test4_data, temperature == "A3")$weight)# 方差齐性检验bartlett.test(weight ~ feed, test4_data)bartlett.test(weight ~ temperature, test4_data)# ANOVA&gt; test4_aov &lt;- aov(weight ~ feed * temperature, data = test4_data)&gt; summary(test4_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) feed 1 127 127 1.589 0.213 temperature 2 9080 4540 56.809 5.22e-14 ***feed:temperature 2 17 9 0.108 0.897 Residuals 54 4316 80 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 可以看到有3个p-value。就可以看到温度对作用是显著的。 多因素也可以做事后检验。可以看到，事后检验也是分因素A，因素B，因素AB互作的。 12345678910111213141516171819202122232425262728293031323334&gt; test4_pos &lt;- TukeyHSD(test4_aov)&gt; test4_pos Tukey multiple comparisons of means 95% family-wise confidence levelFit: aov(formula = weight ~ feed * temperature, data = test4_data)$feed diff lwr upr p adjB2-B1 2.91 -1.717782 7.537782 0.2128406$temperature diff lwr upr p adjA2-A1 25.39 18.576905 32.203095 0.0000000A3-A1 26.75 19.936905 33.563095 0.0000000A3-A2 1.36 -5.453095 8.173095 0.8805321$`feed:temperature` diff lwr upr p adjB2:A1-B1:A1 1.87 -9.942078 13.682078 0.9970585B1:A2-B1:A1 25.09 13.277922 36.902078 0.0000009B2:A2-B1:A1 27.56 15.747922 39.372078 0.0000001B1:A3-B1:A1 25.49 13.677922 37.302078 0.0000006B2:A3-B1:A1 29.88 18.067922 41.692078 0.0000000B1:A2-B2:A1 23.22 11.407922 35.032078 0.0000050B2:A2-B2:A1 25.69 13.877922 37.502078 0.0000005B1:A3-B2:A1 23.62 11.807922 35.432078 0.0000035B2:A3-B2:A1 28.01 16.197922 39.822078 0.0000001B2:A2-B1:A2 2.47 -9.342078 14.282078 0.9892562B1:A3-B1:A2 0.40 -11.412078 12.212078 0.9999985B2:A3-B1:A2 4.79 -7.022078 16.602078 0.8358408B1:A3-B2:A2 -2.07 -13.882078 9.742078 0.9952518B2:A3-B2:A2 2.32 -9.492078 14.132078 0.9919360B2:A3-B1:A3 4.39 -7.422078 16.202078 0.8800277 缺失数据 对于缺失值，我感觉我们考试如果遇到的话，应该只要考虑直接去掉就可以了。举个例子 12345678910111213141516171819202122232425# 数据模拟&gt; dat &lt;- data.frame(V1 = 1:3,V2 = c(2,3,NA))&gt; dat V1 V21 1 22 2 33 3 NA# 检查缺失值&gt; is.na(dat) V1 V2[1,] FALSE FALSE[2,] FALSE FALSE[3,] FALSE TRUE&gt; table(is.na(dat))FALSE TRUE 5 1 # 凡是有缺失值的那一行就去掉&gt; na.omit(dat) V1 V21 1 22 2 3]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part13]]></title>
    <url>%2F2019%2F06%2F09%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part13%2F</url>
    <content type="text"><![CDATA[Power（统计功效） 关于power，我觉得下面这张图已经解释的很好了。 当 H0 是正确的时候，拒绝了H0，就是犯了第一类错误。当 H1 是正确的时候，拒绝了 H1，就是犯了第二类错误。然后相对应的，当 H1 是正确（H0是错误的时候）的时候，你接受了 H1（拒绝了H0），就是所谓的power（统计功效）了。 关于power的计算公式，我只是贴出来一下，大家考试的时候可以抄下，具体的我就不讲解了（当然，也是因为我不太懂power的具体计算╮（╯＿╰）╭）。 单样本的情况 已知样本的均值，标准差。我们假设的H1为 \(\mu=118**\)。如果我们想控制第二类错误为0.05，同时第一类错误为0.01**.那么我们需要多少样本量 1234567891011121314# 第二类错误是0.05，那么统计功效就是0.95# 已知了power，已知了第一类错误# 已知了效应大小pwr.t.test(d = (test4_mean - 118)/(test4_sd), sig.level = 0.01, type = "one.sample",power = 0.95)## ## One-sample t test power calculation ## ## n = 646.0381## d = 0.1664842## sig.level = 0.01## power = 0.95## alternative = two.sided 需要注意的是，对于效应大小（Cohen's d），我们用的是样本均值减去假设的总体均值，再除以标准差。因为大家可以看到，pwr的计算无关乎样本信息，所以对于效应值，我们只有除以标准差，才可以变成一个可以在各种样本中都使用的值。 公式为 \[ d=\frac{\bar{x}-\mu}{s} \] 两样本的情况 两样本的话，跟单样本差不多。只不过需要注意两点 type选择是paired 还是单纯的 two-samp。paired在其余条件相等情况下，power其实是更高的。 效应值的计算 \[ d=\frac{\mu_1 - \mu_2}{sd_{pooled}} \] \(sd_{polled}\)的计算是 \[ sd_{polled}=\sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}} \] 但在实际过程中，\(sd_{polled}\)比较方便的计算是 \[ sd_{polled}=\sqrt{\frac{s_1^2+s_2^2}{2}} \] 另外需要注意的是，我们之前的pwr是针对两个样本是一样大小的，如果两个样本的样本量不一样大，就用 pwr.t2n.test 1pwr.t2n.test(n1=, n2=, d=, sig.level=, power=, alternative=) 关于第一类错误，第二类错误，power等概念，大家有空可以去读一读《Fundamentals of Biostatistics》的213页左右。这本书真的很好，虽然我也没读几页。 当然，也推荐去读一读协和八公众号上的说人话的统计学系列，上面讲的也很清楚。 参考文章： The Cohen’s d Formula What Is And How To Calculate Cohen's d Effect size 非参数检验 之前的t检验等需要假定你的分布是正态分布。但如果不满足正态性的假设，我们就需要考虑非参数检验了。非参数检验通常检验的不是平均值，而是中位数，相比参数检验而言，比较的意义不是那么的明确。同时，非参数检验的power相比较而言，比较低。 关于非参数检验的方法，还是用一张PPT的图来表示： 但R里面的话，上面的非参数检验只要 wilcox.test() 一个函数就可以解决了 具体的操作相信大家在作业中都已经做过了，就不讲了。 稍微需要注意的一点是，非参数检验的原假设和备则假设是（双尾） \[ H0:M_1=M_2\\ H1:M_1 \ne M_2 \] 即上面我提到过的，是中位数不相等。 关于非参数检验，还是推荐去看看说人话的统计学系列。 多重矫正 其实一般来说，如果你做了多次的假设检验，就要考虑多重矫正的问题了。因为假设你单次检验的设定的阈值 \(\alpha\) 是0.05，那么你做了5次检验后，至少会犯一次错误的概率就是\(P=1-(1-0.05)^5\)。可以看到这个概率是很大的。所以对p-value进行矫正还是很重要的。 这里还是不放上原理了（还是因为我太菜了，多重矫正的原理还是没有清楚）。 R里面的多重矫正还是一个函数p.adjust()就可以了： 1p.adjust(p, method = p.adjust.methods, n = length(p)) 里面的method只要根据题目变换就行了。反正考试常见的题型就是出个表达量矩阵，然后让你用t.test，得到了p-value之后，让你矫正下p.adjust。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part12]]></title>
    <url>%2F2019%2F06%2F05%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part12%2F</url>
    <content type="text"><![CDATA[我感觉到后面应该会讲的比较省略了，公式模板什么的套的比较多，因为主要是用来做考试复习和速查的。大家如果有什么疑问，可以在下面提出来。在这一部分，我也会注重把解题的步骤写出来（好像写出来是有分的）。其实我们假设检验的步骤就是，建立H0和H1，然后确定分布，然后确定我们的样本值以及更极端值所占有的比率，如果比例太小，说明这个样本值不常见，就可以拒绝H0，接受H1。 样本均值比较 样本均值的比较我们一般会涉及到 Z检验 和 t检验。Z检验针对的是总体方差已知的情况，t检验针对的总体方差未知的情况。一般来说，t检验更为的常见。 使用t检验，前体条件必须是样本均值的抽样分布符合正态分布。如果总体是正态分布，那么小样本的样本均值抽样分布也可以符合正态分布。如果总体不是正态分布，那么只有样本量达到一定大小，才可以符合正态分布。但一般来说，我们的考试生物学数据是符合正态分布的，而且课上也不提检验正态性，所以我这里不说检验正态分布了。后面ANOVA就提到了检验正态性。。。。。 当然，某些生物学也是不符合正态性的，就要考虑用非参数检验了。 Z检验 Z检验就是根据样本值，得到样本值的Z-score，然后计算概率。 单样本均值比较，即与某个数字进行比较的话，就是 \[ z= \frac{\bar{x}-\mu}{\sigma/\sqrt{n}} \] 两样本的均值比较的话，就是 \[ z=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}} \] 举个例子： 假设我们已知总体身高符合正态分布，且标准差已知为5，那么我们的样本数据为……（这里放上一堆数字，总共为20）。那么我们想要检验总体均值是否8。 步骤为： 我们建立原假设和备则假设，并设置显著性\(\alpha=0.05\) \[ H_0:\mu=8\quad H_1:\mu \neq8 \] 然后计算p-value 123456789101112131415161718192021# 模拟数字&gt; data &lt;- rnorm(20,mean = 8,sd = 5)&gt; data [1] 10.688820 7.462011 6.457040 6.146526 20.790506 9.610317 3.614535 5.224481 [9] 16.044720 8.231625 5.929559 13.817802 8.168671 3.331038 7.902722 7.818987[17] -4.585604 5.304461 3.261386 11.483466# 计算样本均值和标准差&gt; mean(data)[1] 7.835154&gt; sd(data)[1] 5.286252# 计算z-score&gt; (mean(data)-8)*sqrt(20)/(sd(data))[1] -0.1394591# 计算p-value# 因为z-score &lt; 0，所以计算p-value是&gt; 2*pnorm(-0.1394591)[1] 0.8890874 由于p-value &gt; 0.05，所以接受H0。即认为总体均值是等于8的。 首先要注意单尾和双尾的问题，如果H1是不等于，就是双尾。H1是大于或者小于，就是单尾。单尾的话，p-value不用乘以2了。 z-score这里手算的话，要注意z-score的正负，如果是负的话是2*pnorm(z-score)。如果是正的话，就是2*（1-pnorm(z-score)） t检验 单样本的t检验 \[ t=\frac{\bar{x}-\mu}{s/\sqrt{n}} \] 配对样本的t检验 配对样本的t检验，本质上就是配对样本对应值之差的单样本检验。所以也是一样的公式 独立两样本的t检验——方差相等 \[ t = \frac{(x_1-x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_p^2}{n_1}+\frac{s_p^2}{n_2}}} \] \[ s_p^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2} \] t分布的自由度为 \[ df=n_1+n_2-2 \] 独立两样本的t检验——方差不相等 \[ t=\frac{(x_1-x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}} \] t分布的自由度为： \[ df = \frac{(s_1^2/n_1+s_2^2/n2)^2}{\frac{(s_1^2/n_1)^2}{n_1-1}+\frac{(s_2^2/n_2)^2}{n_2-1}} \] t分布的步骤还是跟z差不多的，只要注意写上H0和H1就行了。不过不同的是，需要记得去检验方差齐性。方差齐性的R函数是var.test。方差齐性检验完了，如果是齐性的，就在t.test 里面设置 var.equal=T。 举个例子（这里我不写H0，H1了） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 生成数据&gt; t_data1 &lt;- rnorm(20)&gt; t_data2 &lt;- rnorm(20)# 先确定是不是配对数据，我们先假设是配对的&gt; t.test(t_data1,t_data2,paired = T) Paired t-testdata: t_data1 and t_data2t = 0.82025, df = 19, p-value = 0.4222alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: -0.3972871 0.9093616sample estimates:mean of the differences 0.2560372 # 也可以假设是不配对的# 然后确定你的H1假设是单尾还是双尾，然后调整# 我们假设是双尾，即两者均值不等——双尾其实是默认值t.test(t_data1,t_data2,alternative = "two.sided")# 然后要做方差齐性检验（这里也要写H0和H1，即假设方差是否相等）&gt; var.test(t_data1,t_data2) F test to compare two variancesdata: t_data1 and t_data2F = 1.6712, num df = 19, denom df = 19, p-value = 0.2719alternative hypothesis: true ratio of variances is not equal to 195 percent confidence interval: 0.6614761 4.2221719sample estimates:ratio of variances 1.671187 # 做完方差齐性之后，根据结果，设置var.equal参数&gt; t.test(t_data1,t_data2,var.equal = T) Two Sample t-testdata: t_data1 and t_data2t = 0.77421, df = 38, p-value = 0.4436alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: -0.4134474 0.9255219sample estimates: mean of x mean of y 0.21364009 -0.04239716 样本方差比较 单样本方差比较 对于单样本的方差比较，我们用卡方分布。卡方统计量为 \[ \chi^2=\frac{(n-1)s^2}{\sigma^2} \] 例子就用PPT上这张图 PPT这里的零假设是方差等于35 只不过这里p值的计算可以利用R来做，不用查表 123# 还是双端&gt; 2*pchisq(2.103,9)[1] 0.02053599 关于不同情况下的双端计算，可以看这张PPT。 两样本方差比较 对于两样本的方差比较，我们用F检验。 F分布的定义为设随机变量 \(X_1 \sim \chi^2(m)\)，\(X_2 \sim \chi^2(n)\)，X1与X2独立。则称 \(F=\frac{X1/m}{X2/n}\)的分布是自由度为m与n的F分布，记为 \(F\sim F(m,n)\)。这个定义恰好适用与我们的两样本比较。 检验过程用PPT表示： 当然，在R里面，你直接用var.test就可以了。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part11]]></title>
    <url>%2F2019%2F06%2F05%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part11%2F</url>
    <content type="text"><![CDATA[中心极限定理 中心极限定理 假设我们有一个总体，我们从总体中取出一个大小为5的样本。我们可以利用这个样本均值、方差来估计总体的均值，方差。而如果我们不断地从总体中取出 n=5 的样本，然后每次都计算抽取样本的均值，就可以形成一个样本均值的抽样分布。 中心极限定理告诉我们，如果总体是呈现正态分布，或者样本的大小足够大，那么样本均值的抽样分布就会呈现正态分布。 可以见下图（图来源于Y叔的统计学笔记，文末给出链接） 可以看到，如果样本量足够大，哪怕总体是来源于一个再疯狂的分布，样本均值的抽样分布都会呈现一个正态分布，但如果样本量不够，则总体必须是正态分布，样本均值的抽样分布才是正态分布的。 多大算大呢，一般的thumb认为是30。当然，这也只是个经验。具体的大小还得依赖于你总体的分布。如果你总体的分布很像正态分布，自然样本量小点也可以达到效果。 标准误 对于单个样本而言，比如说我们取了5株苗。我们就会用标准差（standard deviation, SD）来衡量样本的离散程度。但对于我们上面提到的多个均值得到的分布（样本均值的抽样分布），我们也需要衡量分布的离散程度，这时候我们就会用标准误（Stand error，SE 或者说 standard error of the means，SEM）来衡量。样本均值抽样分布的标准误计算为： \[ \sigma_\bar{x}=SE=\frac{\sigma}{\sqrt{n}} \] 即用总体的标准差除以样本量的平方根。但通常来说，我们是不知道总体的方差的，所以通常会用样本的方差来估计，那么 \[ s_{\bar{x}}=\frac{s}{\sqrt{n}} \] 置信区间（confidence interval） 通常来说，我们会用总体参数的点估计（比如参数的均值）来代表我们对总体参数的估计。但实际上，度量一个点估计的精读更直观的方法就是给出未知参数的一个区间。我们通常会设定一个 \(\alpha\) 值，把 \(1-\alpha\) 叫做置信水平。比如我们通常会设置 95% 的置信水平。置信水平的频率解释就是，我们利用我们构建置信区间的方法，不断地重复构建置信区间，比如说构建1000次。这样我们就得到了1000个置信区间，每次得到的区间都是不一样的。置信区间是否包含我们总体参数（即真值）的结果是一个二元的，即包含或者不包含。这样最终差不多就会有950个置信区间包含了我们的真值，另外50个不包含真值。 可以看下面的图，每根线都是我们构建的置信区间。绿线代表包含了真值，红线代表没有包含真值。（图片来自：Data Analysis for the Life Sciences） 均值的置信区间 先放一段我个人认为的均值置信区间构建的原理，不保证正确。可以不看直接看后面置信区间的公式（公式考试的时候好像还是要写的）： 我们设置95%为置信水平。我们从总体中得到的了一个样本均值，这个样本均值是样本均值抽样分布（假设是正态分布）的一个点。我们可以认为这个样本均值点应该是在距离真值95%范围内的。双侧95%的那个阈值点就是1.96，所以样本均值距离真值应该是1.96个标准误之内的 \[ -1.96\le\frac{\mu-\bar{x}}{\sigma_{x}}\le1.96 \] \[ -1.96{\sigma_{x}} \le \mu-\bar{x} \le 1.96{\sigma_{x}} \] \[ -1.96{\sigma_{x}}+\bar{x} \le \mu \le 1.96{\sigma_{x}}+\bar{x} \] 现在讲公式： 样本均值分布呈现正态分布的情况下，可以使用正态分布和t分布来估计置信区间，用哪种方法，取决于总体参数 σ 是否已知。 知道总体标准差的情况下，我们使用正态分布 \[ (\bar{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) &lt; \mu &lt; (\bar{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) \] \(Z_{\alpha/2}\)就是所谓的critical value，跟你设置的置信水平有关系，比如你是95%的置信水平，双侧的话，就是一边在2.5%，一边在97.5%。那么就是-1.96和1.96了。 用R算critical value就是应用我们之前讲过的dpqr中的q了。 1234&gt; qnorm(0.975)[1] 1.959964&gt; qnorm(0.025)[1] -1.959964 不知道总体标准差的情况下，我们使用 t 分布 \[ (\bar{x}-t_{\alpha/2}\frac{s}{\sqrt{n}}) &lt; \mu &lt; (\bar{x}+t_{\alpha/2}\frac{s}{\sqrt{n}}) \] 之所以t分布的使用要满足样本均值分布呈现正态分布，是因为t分布的建立要求之一就是正态分布。具体可以去看概率论与数理统计的书。如果不满足这个条件，就不能使用t分布。 用 t 分布来计算置信区间的话，可以用R的t.test，会直接输出置信区间。 123456789101112131415&gt; data &lt;- rnorm(20)# 改下置信区间为90%&gt; t.test(data,conf.level = 0.9) One Sample t-testdata: datat = -0.59555, df = 19, p-value = 0.5585alternative hypothesis: true mean is not equal to 090 percent confidence interval: -0.5063434 0.2469069sample estimates: mean of x -0.1297183 有可能还会要你算边际误差（margin of error，E，也称误差幅度） 边际误差就是所谓的 \[ E=z_{\alpha/2}\sigma_{\bar{x}}=z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \] 其实就是用来构建置信区间的。 当然，如果总体标准差不知道的话，就用样本标准差代替，分布变成t分布。 \[ E=t_{\alpha/2}\frac{s}{\sqrt{n}} \] 可以看到，如果想要降低E，即缩短置信区间，最稳妥的方法就是增大n。即提高样本容量。 比例的置信区间 这部分来自于Y叔的统计笔记。我感觉写的很直观，就直接放了。 比例的置信区间也差不多，公式在下面 \[ (\hat{p} - E) &lt; p &lt; (\hat{p} + E) \] 其中 E 是边际误差，\(\hat{p}\) 是算出来的比例，而p是总体比例。E通过下面的式子 \[ E = z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \] 因为这种情况是符合二项分布的，而n有比较大，所以用正态分布来估计。 比如我们检测了829个成年人，51%反对修铁路。问总体上有大约多少是反对修铁路的。 首先，我们先检查这个二项分布是否符合正态分布的近似。发现，\(n\hat{p}=422.79 &gt; 5\)，\(n\hat{q}=406.21 &gt; 5\)。的确是可以用来近似的。 然后就计算 E： \[ E =1.96\sqrt{\frac{0.51*0.49}{829}} \] 就可以算出置信区间了。 方差和标准差的置信区间 我们如果是拿样本的标准差来估计总体的标准差的置信区间，就要用到卡方分布。 假设我们从正态分布的总体中每次抽出样本量为n的样本，计算样本的方差\(s^2\)。那么每次计算得到的\(\frac{(n-1)s^2}{\sigma^2}\)就会符合卡方分布。 \[ \chi^2 = \frac{(n-1)s^2}{\sigma^2} \] 因为卡方是不对称分布，所以置信区间也是不对称的，所以需要分别找出左侧和右侧的临界值。 假设我们抽取的n是100，那么自由度（degree of freedom）是99，我们要计算95%的置信区间，需要分别计算左侧0.025和右侧0.025的临界值： 1234&gt; qchisq(0.975,99)[1] 128.422&gt; qchisq(0.025,99)[1] 73.36108 这两个值被称为卡方左右值，\(\chi_{L}^2\)和\(\chi_{R}^2\)。那么标准差置信区间的计算就是 \[ \sqrt{\frac{(n-1)s^2}{\chi_{R}^2}} &lt; \sigma &lt; \sqrt{\frac{(n-1)s^2}{\chi_{L}^2}} \] 参考资料 Y叔的统计笔记]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part10]]></title>
    <url>%2F2019%2F06%2F03%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part10%2F</url>
    <content type="text"><![CDATA[前面一部分讲了些概率论的知识，这部分我们来讲讲课上讲过的随机变量及其分布。 可以把这一部分当作速查。 离散型随机变量 二项分布（Binomial Distributions） 如果记 X 为 n 重伯努利实验中成功（记为事件 A ）的次数，则 X 的可能取值为0，1……，n。记 p 为每次试验中 A 发生的概率，即 \(P(A)=p\)，则 \(P(\bar{A})=1-p\)。这个分布称为二项分布，记为\(X\sim{b(n,p)}\) 那么事件成功 k 次的概率就是 \[ P(X=k)=C_{n}^{k}p^k(1-p)^{n-k}\quad,k=0,1,……,n \] 关于组合数符号\(C_n^{k}和C_{k}^{n}\)写法一直有点争议，只要知道是怎么算就好。当然还有\(\binom{n}{k}\) 二项分布是一种常用的离散分布，比如： 检查 10 件产品，10 件产品中不合格的个数X服从二项分布 b（10，p），其中p为不合格率。 射击5次，5次命中次数Z服从二项分布b（5，p），其中p为射手的命中率。 二项分布的均值、方差（variance）、标准差为（Standard Deviation）： \[ \mu=np\\ \sigma^2=npq\\ \sigma=\sqrt{npq} \] 泊松分布（Poisson distribution） 泊松分布的概率分布列为： \[ P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda} \] 泊松分布只有一个参数，即 \(\lambda\)，\(\lambda &gt;0\)。记为 \(X\sim P(\lambda)\)。 泊松分布常与单位时间（或者单位面积、单位产品）等上的计数过程相联系，比如 一天中，来到某商场的顾客数目 某一服务设施在一定时间内受到的服务请求的次数 汽车站台的等候人数 泊松分布的均值（数学期望）和方差均是 \[ \mu=\sigma^2=\lambda \] 泊松分布还有一个非常实用的特性，即可以用泊松分布作为二项分布的一种近似。当二项分布n很大，p很小，而乘积 \(\lambda=np\) 的大小适中时候，可以用泊松分布近似。 根据课件上来说，一般是 \[ n &gt;= 100\\ np &lt;=10 \] 超几何分布（Hypergeometric distribution） 从一个有限总体中，进行不放回抽样往往会遇到超几何分布。 设有 N 件产品，其中有 M 件不合格品。若从中不放回地随机抽取 n 件，则其中含有的不合格的件数 X 服从超几何分布，记为 \(X\sim{h(n,N,M)}\)。超几何分布的概率分布列为 \[ P(X=k)=\frac{C_M^k C_{N-M}^{n-k}}{C_N^n} \] 超几何分布的数学期望和方差为： \[ \mu=n\frac{M}{N}\\ \sigma^2=\frac{nM(N-M)(N-n)}{N^2(N-1)} \] 当抽取个数远小于产品总数的时候，每次抽取后，总体中的不合格率 \(p=\frac{M}{N}\) 改变甚微，所以不放回的抽样就可以近似变成放回抽样。这时候超几何分布就可以用二项分布近似了。 跟我们相关的超几何分布的应用就是基因富集分析（enrichment analysis）。 基因富集常见的方法有 Fisher精确检验 超几何分布 二项分布 卡方检验 …… 关于基因富集的部分我们后面再讲。这里我只提下利用超几何分布来检验富集分析。 比如我们对根再生这个通路很感兴趣。我们想要知道这个通路在我们的差异基因中是不是显著富集的。我们得到了 2000 个差异基因，跟根再生通路相关的基因有50个。拟南芥全体基因有 25000 个，其中跟根再生通路相关的有 100 个。那么这里 25000 就是 N，100就是M。2000就是n，50就是k。然后我们就可以计算 p 值了。 当然，p值应该是要考虑加上极端值的累积概率。可能不单单是一个点的值。即应该用1-phyper而不是dphyper。后面差异富集部分一起讲吧。 连续型随机变量 正态分布（Normal Distributions） 一个随机变量如果是由大量微小的、独立的随机因素的叠加结果，那么这个变量一般都可以认为服从正态分布。比如人的身高、测量误差等。 正态分布的密度函数和分布函数太长了，就不写了，可以自己去翻阅PPT。正态分布记为 \(X\sim{N(\mu,\sigma^2)}\) 正态分布还可以转换成标准正态分布： 若随机变量 \(X\sim{N(\mu,\sigma^2)}\)，则 \(U=(X-\mu)/\sigma \sim {N(0,1)}\) 正态分布的应用应该就是后面要讲到的 t-test，所以这里就不讲了。 还有一点就是当前面的二项分布的 \[ np\ge5\\ nq\ge5 \] 就是用正态分布来近似二项分布， \[ \mu=np \\ \sigma=\sqrt{npq} \] 计算 分布函数、概率分布列、概率密度函数 分布函数就是累积分布函数（Cumulative Distribution Function，CDF），指的就是小于等于 a 的值出现概率的和。具有累积特性。比如对于标准正态分布而言，到负无穷到 0 为止的概率和就是0.5。常表示为： \[ F(a)=P(X \le a) \] 就像下图就是正态分布的累积分布函数图 概率分布列，或者说概率质量函数（probability mass function，PMF），就是针对离散型变量而言，离散型变量在特定取值上的概率。 概率密度函数（probability density function，PDF）就是针对连续型变量而言。因为连续型变量在特定取值上的概率肯定是0，所以对连续型变量使用概率分布列是没有意义的。概率密度函数曲线上的面积就是概率值。 下图就是正态分布的概率密度函数图 这个短暂篇幅不太好讲，如果想再深入，建议看书。 R实现 R的实现可以用《R语言实战》第二版的第90页的这张图表示： dpqr加上对应的分布缩写，就可以变成任一的概率函数了。让我们来稍微解释下dpqr d开头的密度函数应该是包含了离散型随机变量的概率分布列，连续型随机变量的概率密度函数。 比如我们想要算二项分布的概率分布列。以扔硬币为例，扔3次硬币，每次朝上的概率为0.5。 123456789# 0,1,2,3次朝上的概率&gt; dbinom(0,3,0.5)[1] 0.125&gt; dbinom(1,3,0.5)[1] 0.375&gt; dbinom(2,3,0.5)[1] 0.375&gt; dbinom(3,3,0.5)[1] 0.125 我们想要看看正态分布的概率密度，比如我们想要看标准正态分布，0那点的概率密度是多少。（看上面的图，应该是0.4左右） 12&gt; dnorm(0)[1] 0.3989423 p开头的分布函数就是我们之前提到过的累积分布函数。你可以想象成，在累计分布函数曲线上的x轴上，你设定一个值，那个值所对应y值（累积概率）是多少。还是前面两个例子。 我们想要看看，掷3次硬币，扔到小于等于1次正面的概率。 1234567# 就是0次正面+1次正面&gt; dbinom(0,3,0.5) + dbinom(1,3,0.5)[1] 0.5# 直接用p函数算。&gt; pbinom(1,3,0.5)[1] 0.5 我们想要看看从负无穷到0为止，总共的概率和。恰好就是一半的概率。 12&gt; pnorm(0)[1] 0.5 q开头的分位数函数可以想象成，在累积分布函数的y轴上，你设定一个y值，那个y值（累积概率）所对应的x值是多少。 感觉分位数对于连续型变量比较常见。比如我们想要知道标准正态分布的97.5%分位点是多少，即曲线下面积是0.975的时候，所对应的x值。我们也可以说，这个x值比97.5%的值都大。 12&gt; qnorm(0.975)[1] 1.959964 r开头的就是生成各种类型的随机数了。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part9]]></title>
    <url>%2F2019%2F06%2F01%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part9%2F</url>
    <content type="text"><![CDATA[从这一部分开始，我们正式开始进入生统课上的内容。我们首先先讲讲陈洛南老师的概率论的部分。 陈老师这一部分似乎考的不太多，我就提的少点了。 描述性统计 集中趋势 描述集中趋势的有平均数、中位数、众数、偏斜度。 对应的 R 代码为 123456789101112131415161718# 准备数据data &lt;- sample(1:10,4)&gt; data[1] 7 10 2 4# 均值&gt; mean(data)[1] 5.75# 中位数&gt; median(data)[1] 5.5# 众数（R里面似乎没有单独的众数，我觉得可以用table）&gt; table(data)data 2 4 7 10 1 1 1 1 当然，最方便还是用 summary 123&gt; summary(data) Min. 1st Qu. Median Mean 3rd Qu. Max. 2.00 3.50 5.50 5.75 7.75 10.00 离散程度 描述离散程度的有极差、方差、标准差、变异系数 代码为 123456789101112131415161718192021222324# 极差（最大值减去最小值）## 方法1&gt; max(data) - min(data)[1] 8## 方法2&gt; range(data)[1] 2 10&gt; diff(range(data))[1] 8# 方差&gt; var(data)[1] 12.25# 标准差&gt; sd(data)[1] 3.5# 变异系数# 变异系数是样本标准差除以样本均值再乘以100&gt; sd(data)*100/mean(data)[1] 60.86957 注意，R里面的 var 和 sd 对应计算的都是样本的方差和标准差，所以里面都是 n-1 。如果你想要计算总体的，可以再乘上 \(\frac{n-1}{n}\)。 概率 古典方法： 抽球问题看第一次作业就可以了。 概率性质： 当考虑概率的性质，比如加减的时候，只要画个维恩图就可以了。具体地看看第一次作业就可以了。 条件概率 条件概率： \[ P(A|B)=\frac{P(AB)}{P(B)} \] 全概率公式： \[ P(A)=\sum_{i=1}^{n}P(B_{i})P(A|B_{i}) \] 最简单的全概率公式： \[ P(A)=P(B)P(A|B)+P(\bar{B})P(A|\bar{B}) \] 比较偷懒，所以没打前提条件，具体地可以去看统计学的书。 独立性： 如果 A 和 Ｂ相互不影响，我们就说 A 和 B 是独立的。那么就会有 \[ P(A|B)=P(A)\\ P(B|A)=P(B)\\ P(AB)=P(A)P(B) \] 关于独立，可以去看看第一次作业的两个证明题。 条件概率计算的时候，只要注意把复杂的事件变成字母符号，然后按照公式一步步算就可以了。比如把下雨这个事件，称为 A 。带伞 这个事件称为 B，那么在下雨的情况下，带伞的概率就是 P(B|A) 了。 贝叶斯 贝叶斯公式： \[ P(B|A)=\frac{P(AB)}{P(A)}=\frac{P(A|B)P(B)}{P(B)P(A|B)+P(\bar{B})P(A|\bar{B})} \] 在贝叶斯公式中，我们称 P(B) 为 事件 B 的先验概率（priori probability），称 P(B|A) 为事件 B 的后验概率（ posterior probability），贝叶斯公式就是专门用来计算后验概率的。也是用来在已知结果的条件下，求出原因的概率。 我们可以举个例子。某地区居民的肝癌发病率为0.0004，现用甲胎蛋白法进行普查。医学研究表明，化验结果是存有错误的。已知患有肝癌的人，其化验结果 99% 呈现阳性，而没患肝癌的人其化验结果 99.9% 呈现阴性。现某人的检验结果呈现阳性，问其患肝癌的概率是多少。 记 B 为被检查者患有肝癌，A 为检查结果后为阳性。则 \[ P(B)=0.0004\quad P(\bar{B})=0.9996\\ P(A|B)=0.99\quad P(A|\bar{B})=1-0.999 \] 然后就可以利用贝叶斯公式： \[ P(B|A)=\frac{P(AB)}{P(A)}=\frac{P(A|B)P(B)}{P(B)P(A|B)+P(\bar{B})P(A|\bar{B})} \] 算出 P(B|A) = 0.284。可以看到真正患有癌症的患者只有不到 30%。那么我们该如何提高检测检验精度呢，一个方法就是复查，即提高人群中 P(B) 的比例。如果我们对首次检查得阳性的人群再进行复查，此时 P(B) = 0.284。再利用贝叶斯公式，就可以发现P(B|A) = 0.997了。 关于贝叶斯，陈老师上课还提到了一个有趣的点，即 假设我们的观测为 E（比如说我们的数据） 我们的假设为 R（比如说 基因 A 调控了 基因 B ） 我们通常做的 P-value 指的是 \(p(E|\bar{R})\) ，\(\bar{R}\) 是零假设。 但我们实际上想求得是，p(R|E)，所以真正的公式是 \[ p(R|E)=1-\frac{p(\bar{R})}{p(E)}p(E|\bar{R}) \] 所以，一定程度上，p-value越小，那么我们的 p(R|E) 就会越大了。 混淆矩阵 陈老师的课件里面出现了混淆矩阵，因为混淆矩阵在后面的统计检验、多重比较矫正、逻辑斯蒂回归等可能会用到，所以先提上来讲一讲。 先放上混淆矩阵： 混淆矩阵相关的定义为： 假阳性率（false positive rate）： 也称 I 型错误，即在没病的情况下，你能够预测出来病人是有病的比例，P（预测是有病|真实没病）。\(\frac{FP}{FP+TN}\) 假阴性率（false negative rate）： 即有病的情况下，你能够预测出来病人是没病的比例。\(\frac{FN}{TP+FN}\) 真阳性率： 即召回率（recall），称灵敏度，敏感性（sensitivity）。也称也是 1- II型错误，我感觉也可以称之为统计功效。即在有病的情况下，你能够预测出来病人有病的比例。\(\frac{TP}{TP+FN}\) 真阴性率： 即特异性（specificity），即没病的情况下，你能够预测出来病人是没病的比例。\(\frac{TN}{FP+TN}\) 准确度（accuracy）： 你预测都是对的比例，即病人有病的情况下，你成功预测他是有病的，病人没病的情况下，你成功预测他是没病的。\(\frac{TP+TN}{TP+TN+FP+FN}\) 精确度（precision ）： 也称预测阳性率。即你预测的有病病人中，有多少是真的有病的。\(\frac{TP}{FP+TP}\) 对于不同的机构，对于这些比例有着不同的要求。像对于医院来说，假阴性就必须非常非常低。 让我们带入数字来看下。假设我们我们用一种方法检验阿兹海默症，用的是 450 个病人，500 个正常人的样本。检验结果为。 假阳性率为：\(\frac{5}{500}\) 假阴性率：\(\frac{14}{450}\) 灵敏度：\(\frac{436}{450}\) 特异性：\(\frac{495}{500}\) 准确度：\(\frac{436+495}{950}\) 这里的灵敏度，就是对应我上面提到的：已知患有肝癌的人，其化验结果 99% 呈现阳性。而这里的特异性，就是对应我上面提到的：而没患肝癌的人其化验结果 99.9% 呈现阴性。 最后放两张我网上找来的图，以供大家自查： 参考资料： 概率论与数理统计教程第二版 特异度（specificity）与灵敏度（sensitivity） ROC和AUC]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part8]]></title>
    <url>%2F2019%2F05%2F23%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part8%2F</url>
    <content type="text"><![CDATA[我们前面讲完了数据读取、数据提取、数据读取、数据概述。这部分我们来讲讲跟剩下一些跟生统相关的函数与操作。 其实这部分我觉得掌握 order 函数就可以了，其他的 for、apply、function 我认为其实没啥必要掌握。因为生统数据一般都是才几行几列的数据，完全可以通过复制粘贴来完成这些操作。而这几个操作学习成本略大。反正生统考试也是代码抄到纸上。所以其他的函数我视情况再往上加。 排序函数 R 语言里面的排序相关的函数有三个，分别是 order、rank、sort。我们来看下这三个函数的使用。 1234567891011121314151617# 产生一个测试向量&gt; set.seed(19960521)&gt; test &lt;- sample(10, 5)&gt; test[1] 9 5 2 6 4# order&gt; order(test)[1] 3 5 2 4 1# rank&gt; rank(test)[1] 5 3 1 4 2# sort&gt; sort(test)[1] 2 4 5 6 9 大家看到结果可能会比较懵逼，让我们来一个个来看。 order order 实际上返回的是最小值，次小值，再次小值……次大值，最大值在原始数据中的位置。比如我们原始的数字是9,5,2,6,4。其中最小值是 2，那么 2 在原始数据中排在第 3 个位置。然后就可以 order 在结果中输出的第一个值就是 3，即代表原始数据中，最小值（2）在原始数据中的位置。次小值是4，在原始数据中排在第 5 个位置，那么order 函数输出的第二个值就是 5。 order 默认地排序是升序，即返回的是最小值，次小值，再次小值……次大值，最大值在原始数据中的位置。如果你在向量的前面加一个减号，那么就会变成降序，即返回的是最大值，次大值……次小值，最小值在原始数据中的位置。 12&gt; order(-test)[1] 1 4 2 5 3 大家看到在原始位置中的位置这个信息有没有感觉很有帮助。事实上，我们可以利用 order 返回给我们的信息来对数据进行排序。 12345&gt; test[order(test)][1] 2 4 5 6 9&gt; test[order(-test)][1] 9 6 5 4 2 order还可以对数据框、矩阵进行排序，还是拿我们之前病人的那个数据。不过我把 patientID 稍微更改一下。 1234567891011&gt; patientID &lt;- c(2, 2, 3, 1)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 2 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 1 52 Type1 Poor 我们可以根据病人的年龄进行排序。 1234567891011# 一步步来，先得到坐标索引&gt; order(patientdata$age)[1] 1 3 2 4# 再把坐标索引输进去&gt; patientdata[order(patientdata$age),] patientID age diabetes status1 2 25 Type1 Poor3 3 28 Type1 Excellent2 2 34 Type2 Improved4 1 52 Type1 Poor 我们还可以让数据框先根据病人的ID进行排序（升序）。排完序之后，然后再根据年龄进行排序（降序）。 1234567891011# 获取坐标索引&gt; order(patientdata$patientID,-patientdata$age)[1] 4 2 1 3# 再把坐标输进去&gt; patientdata[order(patientdata$patientID,-patientdata$age),] patientID age diabetes status4 1 52 Type1 Poor2 2 34 Type2 Improved1 2 25 Type1 Poor3 3 28 Type1 Excellent 我们可以看到，我们的病人数据框里面。ID是按升序进行排列的，然后再排age。可以看到ID 那边有 2 个数是相同的，即 2，2。单根据第一列排序是分不出先后的，那么我们就再根据 age 的顺序进行排列。 这种先根据什么，后根据什么排列对于处理生信相关的数据还是比较有用的。比如 peak 文件经常就是Chr，start，end，peakID这几列，我们就可以根据order来排列我们的Peak文件。但对于生统来说，可能只要掌握单列排序就行了。 之前有一题让我们提取出 p-value 最显著的前 10 个数据。我们就可以用 order 来做了。 12345678910111213141516171819202122# 有一大堆 p-value 值。&gt; head(test4_sig_result,20) gene12 gene37 gene49 gene62 gene95 gene101 gene104 gene129 0.0362923840 0.0121764651 0.0418676419 0.0329430116 0.0298363658 0.0105665102 0.0166156137 0.0017854211 gene131 gene135 gene152 gene177 gene178 gene186 gene217 gene239 0.0395431910 0.0439433653 0.0474609370 0.0340475106 0.0109452086 0.0170048804 0.0140824498 0.0087974170 gene250 gene277 gene279 gene282 0.0133802343 0.0007355859 0.0337127909 0.0280345683 ……茫茫多p-value# 提取出前10个## 先对这一大堆 p-value 进行排序test4_sig_result[order(test4_sig_result)]## 得到了有顺序的 p-value之后，利用 [] 提取&gt; test4_sig_result[order(test4_sig_result)][1:10] gene28801 gene27868 gene27438 gene21642 gene24019 gene12323 gene12962 gene28939 4.277344e-06 6.093302e-05 8.229606e-05 9.889894e-05 1.124717e-04 1.261658e-04 1.298200e-04 1.462493e-04 gene2387 gene18712 1.522920e-04 1.601297e-04 rank 12345&gt; test[1] 9 5 2 6 4&gt; rank(test)[1] 5 3 1 4 2 rank 实际上返回的是你原始数据中的每个值，在原始数据中的顺序（排名）。 打个比方，有 10 个人站成一排军训。order 函数会告诉教官，最矮的人站在哪里，次矮的人站在哪里，最高的人站在哪里。然后教官就可以根据 order 返回的信息，大声地说 XX号，站出来，排在第一位，XX号站出来，排在第二位。这样，教官一个个吼，最后这10个人就能从矮到高排好了。 而 rank 函数则是，有 10 个人站成一排军训，rank 函数则是会像一个检测器一样。每次经过一个人，就告诉教官，这个人在整个队列中，是排几矮的。就好比，检测器扫过第一个人，就会大声说出，这个人在这10个人中，是排在第2矮的。 比如我们这里数据，原始数据的顺序是9,5,2,6,4。9 在这 5 个数据中是最大的，就是第 5 小的。那么 rank 就会告诉我们 9，在原始数据中是排在第5位的。 sort 12345&gt; test[1] 9 5 2 6 4&gt; sort(test)[1] 2 4 5 6 9 sort 就相当于帮教官自动地排好了队列。 事实上，由于 order 不仅可以应用于向量，还可以应用于数据框，矩阵等等。所以应用场景也更多，我们生统也应该就用到它了。 for，function，apply 视情况而定。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part7]]></title>
    <url>%2F2019%2F05%2F19%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part7%2F</url>
    <content type="text"><![CDATA[到目前为止，我们已经讲完了如何读取数据，如何提取数据、如何整理数据，在这一部分，我们来讲讲如何对生统的数据有一个大致的描述。 这一部分参考了《R语言实战》的7.1部分 用数据的方式呈现 一般来说，常见的数据描述方式包括均值，方差，中位数，最大值，最小值等等。R 里面都有对应的函数来检验，比如 mean，var 等等。但为了快速地了解我们手头的数据，我们就可以用 summary 和 str 这两个函数。 我们以 test2 的数据集为例。 123456789101112131415161718192021222324252627282930# 读取test2 &lt;- read.table("rawdata/test2.txt",header = T)# head也是个查看数据的方式&gt; head(test2) control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.23# 使用Summary&gt; summary(test2) control low middle high Min. :17.22 Min. :18.64 Min. :22.29 Min. :24.07 1st Qu.:19.35 1st Qu.:20.39 1st Qu.:25.05 1st Qu.:29.21 Median :22.60 Median :22.69 Median :28.67 Median :32.63 Mean :21.98 Mean :23.23 Mean :28.13 Mean :32.84 3rd Qu.:23.96 3rd Qu.:25.69 3rd Qu.:29.95 3rd Qu.:36.14 Max. :27.21 Max. :29.67 Max. :35.12 Max. :39.76 # 使用str&gt; str(test2)'data.frame': 15 obs. of 4 variables: $ control: num 20.8 22.9 27.2 19.3 17.9 ... $ low : num 22.2 24.7 21.5 19.7 25.9 ... $ middle : num 28.6 28.7 25.3 30.3 23.1 ... $ high : num 31.9 37.9 39.8 27.9 29.6 ... 可以看到，summary 和 str 都是对每列的数据进行了描述。summary 偏向于描述数据的统计属性，比如最小值，最大值等等。str 偏向于描述数据的结构，比如我们可以看到数据总共是 15 行，4列。每列的数据都是数值型的数据。 让我们再来看看 test1 的数据集 123456789101112131415&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T)&gt; summary(test1) yield seed Min. :348 Min. :1.000 1st Qu.:362 1st Qu.:1.000 Median :376 Median :2.000 Mean :378 Mean :1.931 3rd Qu.:394 3rd Qu.:3.000 Max. :414 Max. :3.000 &gt; str(test1)'data.frame': 29 obs. of 2 variables: $ yield: int 383 406 351 400 390 361 394 395 414 382 ... $ seed : int 1 1 1 1 1 1 1 1 1 1 ... 当我们在用 Summary 和 str 看 test1 的数据集的时候，是不是感觉有那么一丝不太对。seed 那列的数据描述跟我们想的不太一样啊。事实上，还是因为没有把 seed 的那列变成因子型的变量导致的。让我们再次来做个转换。 12345678910111213&gt; test1$seed &lt;- factor(test1$seed)&gt; summary(test1) yield seed Min. :348 1:10 1st Qu.:362 2:11 Median :376 3: 8 Mean :378 3rd Qu.:394 Max. :414 &gt; str(test1)'data.frame': 29 obs. of 2 variables: $ yield: int 383 406 351 400 390 361 394 395 414 382 ... $ seed : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ... 这下子，是不是感觉好多了。 事实上，对于 test1 的数据集，还有一个问题就是在比较多组个体或观测时，关注的焦点经常是各组的描述性统计信息，而不是样本整体的描述性统计信息。 就像这里我们可能关注的是 test1 里面 seed1、seed2、seed3 这三种种子分别的数据类型，而不是整个 29 个观测的数据结果。关于分组变量，为了不加重大家的学习负担，我推荐可以用我们之前提到过的数据提取方法，从而分别地对数据进行描述。 因为生统的数据通常不会有多个变量这种复杂的数据结构，所以上面的方法已经足够了。如果碰到了多个变量，多个组，可以试试基本包的 aggregate() 函数 或者 tidyverse 包的 group_by 函数。 事实上，还有一种快速描述数据的函数，即 table 函数。 1234&gt; table(test1$seed) 1 2 3 10 11 8 可以看到table 可以快速地呈现各个处理所对应的重复数目。三种种子分别有10、11、8个重复。不过 table 似乎在列联表等中更为常见，这里先按下不表，后面在提到线性回归中可能会讲到。 用图画的方式呈现 人类是视觉动物，所以图画的呈现可能远比数据来的更直观。在生统中，我们通常遇见的还是 boxplot，即箱式图。箱式图对于数据格式的支持非常友好，其支持宽、长数据。 但值得注意的是，对于宽数据，直接把数据输入 boxplot 是没有问题的。因为你每列都代表一个单独的处理。虽然我不太推荐把宽数据作为画图的基本数据，因为其实在不符合各种画图包的格式，也不符合我们做线性回归、方差分析等的格式。这里 boxplot 能画可能只是一个个例。 对于长数据，我们就需要输入 formula，即告诉 boxplot 谁是 x 轴，谁是 y 轴。 让我们拿之前的 test1 和 test2数据来举例子。 12# test2的boxplotboxplot(test2) 123# test1的boxplot，我们以 yield 作为 y 轴，seed 作为 x 轴# 记得用 ？号看看 boxplot 的基本使用方法boxplot(yield ~ seed , data = test1) boxplot 的意义参见网上的解释 图片来源： Understanding Boxplots 你也可以画柱状图，用 hist 这个函数就行。这个上课应该都画过，不讲了。 顺便提一句，现在大家已经开始摈弃bar plot了，因为bar plot并不能给予我们太多的信息。相对应的，大家已经开始逐渐转向 box plot、dot plot（Scatter plot）、violin plot。如果对于画图有兴趣，大家可以去学学 ggplot2。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part6]]></title>
    <url>%2F2019%2F05%2F18%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part6%2F</url>
    <content type="text"><![CDATA[前面我们已经讲过了在生统课上会用到基本的数据结构以及怎么来提取我们想要的数据，这一部分我们来讲讲数据的清洗。 在生统课上，我们基本上会遇到两种数据结构。一种是我把其叫做宽数据，就比如我们在第五次生统作业中，碰到的第二题的数据。 12345678910&gt; test2 &lt;- read.table("rawdata/test2.txt",header = T)&gt; head(test2) control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.23 这种数据列名其实是不同的处理，每一列都对应不同处理下的值。 实际上，这种数据还不能称之为真正意义上的宽数据，这里拿来指代是为了方便与长数据区分。 另一种就是长数据，也就是我上一节翻来覆去提到过的数据结构。其每一行都是一个观测值，列名则是变量名。典型的就是我们在第五次生统作业中，碰到过的第一题的数据。 123456789&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T)&gt; head(test1) yield seed1 383 12 406 13 351 14 400 15 390 16 361 1 由于宽数据在线性回归，方差分析等分析中都无法被函数所识别，所以我们首先讲讲如何把宽数据转换成长数据。 gather函数转换 首先介绍的是 tidyr 包的 gather 函数。tidyr 包的安装就是我们之前讲过的方法 1install.packages("tidyr") 有点建议大家直接装 tidyverse 包，这是个各种包的合集，里面还包括了 ggplot2 等。不过有可能安不上 tidyverse ， 如果安装有问题，欢迎大家在下面提出问题。 gather 函数的使用非常方便，你只需要指定你转换后的 key 那列的列名，value 那列的列名，以及你需要转成长数据的那几列。我们以 test2 为例。 12345678910111213# 加载包library(tidyr)# 使用gahter&gt; test2_long &lt;- gather(test2, key = "Treatment_dose", value = "survive_time",control, low, middle, high)&gt; head(test2_long) Treatment_dose survive_time1 control 20.792 control 22.913 control 27.214 control 19.345 control 17.856 control 23.79 gather 函数你需要输入参数为 第一个参数是你的数据集 第二个参数是你新构建的关键列的名称（该列的内容由原先数据集的列名组成） 名字自己取，像我这里就取名为"Treatment_dose" 第三个参数是新构建的数值列的名称 名字还是自己取，我这里取名为"survive_time" 后面的几个参数都是你要用来构建关键列的那几个列名 这里就是control, low, middle, high，即原来的几个列名。 key 和 value 大家可能还是比较懵逼，但对于我们普通的生统数据，不需要太过于纠结其意义。 gather用法在转换的时候还要考虑uniq key的问题，但我们生统的数据应该也不需要考虑这一点。 也有人提到过用 reshape2 包的 melt 函数，或者基本包 transform 函数来转换。但我觉得没有 gather 这个函数直观，简单。还有，gather 转换的时候对于不等长数据的支持也比较好。就比如我们在第五次生统作业的第三题，药物 1 和 2 有 15 只小鼠，药物 3 只有 10 只老鼠。如果只是单纯地按我们之前的做法读入数据框，就会报错。 123&gt; test3 &lt;- read.table("rawdata/test3.txt",header = T)Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 11 did not have 3 elements 就是因为3列数据不等长，所以R才会报错。这时候，我们就可以设置一个参数 123456789101112131415161718&gt; test3 &lt;- read.table("rawdata/test3.txt",header = T,fill = T)&gt; test3 med1 med2 med31 40 50 602 10 20 303 35 45 1004 25 55 855 20 20 206 15 15 557 35 80 458 15 -10 309 -5 105 7710 30 75 10511 25 10 NA12 70 60 NA13 65 45 NA14 45 60 NA15 50 30 NA 然后就可以顺利地读入了，而且也可以顺利地用gather函数来整理成长数据。 1gather(test3,key = "different_med", value = "weight", med1,med2,med3) 会发现NA还是存在，但我们同样可以设置一个参数 1gather(test3,key = "different_med", value = "weight", med1,med2,med3,na.rm = T) 这样，就顺利地转换成我们需要的格式了。 基本包转换 其实，宽数据到长数据的转换，不一定需要特殊的包的函数，也可以用最基本的方法。尽管最基本的方法有些麻烦，但对于提高自己的数据转换能力还是很有帮助的。 rbind和cbind 在使用基本函数转换前，我们先来介绍两个我们以后可能会用到的函数，rbind 和 cbind。rbind是纵向合并，cbind是横向合并。具体操作我们来看一个例子 12345678910111213141516171819202122232425&gt; data1 &lt;- data.frame(A1 = sample(1:10,3),+ A2 = sample(1:10,3),+ A3 = sample(1:10,3))&gt; data1 A1 A2 A31 3 6 22 7 9 53 5 10 4&gt; data2 &lt;- data.frame(B1 = sample(10:20,3),+ B2 = sample(10:20,3),+ B3 = sample(10:20,3))&gt; data2 B1 B2 B31 15 17 142 13 14 113 17 15 12&gt; rbind(data1,data2)Error in match.names(clabs, names(xi)) : names do not match previous names&gt; cbind(data1,data2) A1 A2 A3 B1 B2 B31 3 6 2 15 17 142 7 9 5 13 14 113 5 10 4 17 15 12 可以看到rbind需要两个数据框有同样的变量（同样的列名），cbind则需要两个数据框的行数是一样的。 rbind尽管需要两个数据框有同样的变量，但顺序不一定要一样，比如 12345678910111213141516&gt; data3 &lt;- data.frame(A1 = sample(10:20,3),+ A3 = sample(10:20,3),+ A2 = sample(10:20,3))&gt; data3 A1 A3 A21 10 12 142 11 20 203 15 14 19&gt; rbind(data1,data3) A1 A2 A31 3 6 22 7 9 53 5 10 44 10 14 125 11 20 206 15 19 14 这里data1和data3的列名是一样的，但顺序是不一样的，但rbind还是可以合并。 关于rbind和cbind，《R语言实战》4.9也有提到，大家可以去看看 数据转换 介绍完了rbind和cbind，我们就可以来转换数据框了。我用test2做例子 1234567891011121314151617&gt; test2 control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.237 22.60 18.93 28.88 32.638 18.53 18.64 29.62 29.139 23.23 26.39 24.82 39.6210 20.14 25.49 34.64 36.1511 26.71 20.43 22.29 28.8512 19.36 22.69 29.22 24.0713 17.22 29.67 25.63 29.2914 24.13 20.36 35.12 35.2415 25.85 22.74 32.32 36.13 宽数据转换成长数据，本质上就像堆积木一样。你把每一列的数据拿出来，变成一块积木，然后你就一层层地堆起积木，最后就形成了长数据。是不是感觉特别像rbind干的事情？没错，我们这里就用rbind来构建长数据。 123456789control &lt;- data.frame(Treatment_dose = rep("control",15), survive_time = test2$control)low &lt;- data.frame(Treatment_dose = rep("low",15), survive_time = test2$low)middle &lt;- data.frame(Treatment_dose = rep("middle",15), survive_time = test2$middle)high &lt;- data.frame(Treatment_dose = rep("high",15), survive_time = test2$high)rbind(control,low,middle,high) 大家可能是一遍遍地打了以上的代码，机智的小伙伴可能还是复制粘贴的，然后把变量名改一下。不过，实际上我们可以用函数来解决这些重复操作的问题。函数这一部分就留待后面讲了。 双因素ANOVA的数据格式整理 在第五次生统作业的最后一题，我们需要考虑的是双因素ANOVA分析。但word文件里面的表格却不是一个双因素ANOVA的格式。让我们来看看如何利用上面讲过的内容，把它变成一个能让 aov 读入的双因素AONVA。 首先复制粘贴A1，A2，A3三列数据到txt文件中，然后读入R中。 123456789&gt; test4 &lt;- read.table("rawdata/test4.txt",header = T)&gt; head(test4) A1 A2 A31 282.1 296.7 300.12 264.2 318.0 307.53 274.2 295.3 294.24 276.4 292.8 312.05 283.7 304.5 300.26 288.0 305.9 292.6 然后转换成长数据框 1test4_long &lt;- gather(test4, key = temperature, value = weight, A1, A2, A3) 但这样还不够，这里只有单因素，即饲养温度这一列的信息，还没有饲料的信息。所以我们要自己加上一列饲料的信息。 123456789101112feed &lt;- c(rep("B1",10),rep("B2",10))test4_data &lt;- cbind(feed,test4_long)head(test4_data)&gt; head(test4_data) feed temperature weight1 B1 A1 282.12 B1 A1 264.23 B1 A1 274.24 B1 A1 276.45 B1 A1 283.76 B1 A1 288.0 这样就变成了双因素的ANOVA格式，可以顺利让aov函数读入了。 123456789&gt; test4_aov &lt;- aov(weight ~ feed * temperature, data = test4_data)&gt; summary(test4_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) feed 1 127 127 1.589 0.213 temperature 2 9080 4540 56.809 5.22e-14 ***feed:temperature 2 17 9 0.108 0.897 Residuals 54 4316 80 ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part5]]></title>
    <url>%2F2019%2F05%2F11%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part5%2F</url>
    <content type="text"><![CDATA[之前我们已经了解了如何去提取行、列数据。这部分我们讲讲如何筛选自己想要的数据。生统常见的一个数据提取问题就是，提取某某处理的那部分数据进行一些检验。比如，在有3种种子的数据中，提取出1号种子对应的数据。这时候，尽管我们可以根据1号种子的数字索引来提取，但这终归不是一个好的方法，因为一旦数据是打乱的，我们就无法知道正确的数字索引，从而进行提取了。所以，这时候，我们就应该使用我们之前讲过的逻辑运算符来进行操作。 顺便提下，我们之前在介绍数据框的时候，把行叫做观测，而把列叫做变量。所以我们在提取行的时候，就是在提取我们感兴趣的观测，而在提取列的时候，就是在提取我们感兴趣的变量。为什么我要翻来覆去地说这个呢，是因为我觉得以行作为观测，列作为变量是一个比较好的呈现数据的方式，也是后面很多我们生统要用到的包需要的格式。也是后面我们在说长宽数据转换时候要再次提到的一点。我们再来看一下糖尿病人的例子。 1234567891011&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor 这里是 4 行，就是 4 个观测值，4 个病人。这里有 4 列，4 个变量，就是用 4 个不同地指标去衡量了这些病人。当然，我们也会遇见不是这样格式的数据，比如我们在第五次生统作业上遇见的那个用药的数据集 123456789&gt; test2 &lt;- read.table("rawdata/test2.txt",header = T)&gt; head(test2) control low middle high1 20.79 22.22 28.56 31.932 22.91 24.74 28.67 37.943 27.21 21.53 25.28 39.764 19.34 19.66 30.28 27.945 17.85 25.89 23.13 29.656 23.79 29.10 23.47 34.23 这个数据集是 15 行，4 列。但我们并不能说我们做了 15 个观测，应用了 4 个变量。实际上，我们根据题目可知，总共是 60 只小鼠，只用了 1 个变量，即用药的浓度。你会发现这个数据集的每一行都不是同一只老鼠，但前面的糖尿病人数据集，每一行都是同一个病人，所以我们可以说每一行都是一个观测。 初次学 R 的人，对于这种数据的结构可能会感到困惑。不过不要紧，数据处理多了，就会慢慢清晰起来。 顺便提一下，现在生物学的数据跟传统社会学的数据有一个很大的不同就是，社会学的数据往往是低维度，高观测，而生物学的数据则恰好相反，是高维度，低观测的。这里的维度指的就是变量。举个例子，比如你要分发问卷给别人来统计大家对你的产品感不感兴趣，你可能在问卷上只有 2 个问题（2个变量，2个维度），但你却分发给了 1w 个人（1w 个 观测）。生物学的例子就好比，你对 100 个植株进行了 50w 个SNP位点的分析，这里就是 100 个观测，50w 的维度。数据结构的不同，就会导致分析方法的不同。 由于生统的数据列数最多也就 4,5 列，加上整列的提取并不需要逻辑运算符，所以后面的提取不涉及到列的提取。同时，为了让大家加深印象，我会交叉地用行以及观测这两个名词。 利用 [] 来提取感兴趣的观测 我们之前在向量里面提到过，如何提取符合条件的数据，这里运用的方法也是一样的，也是利用 which 或者 TRUE 来提取。不过在提取数据框数据的时候，我有一个小建议，就是分步完成你的提取任务。我们还是拿糖尿病人的数据集为例子。比如我们希望提取出年龄大于 30 岁的糖尿病人的数据。 12345678910111213141516171819202122# 先得到索引&gt; patientdata$age &gt; 30[1] FALSE TRUE FALSE TRUE&gt; which(patientdata$age &gt; 30)[1] 2 4# 把索引输入 [] 里面&gt; patientdata[patientdata$age &gt; 30,] patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor&gt; patientdata[which(patientdata$age &gt; 30),] patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor# 有时候嫌得到索引那步比较长，就可以把索引结果存为一个变量&gt; result &lt;- patientdata$age &gt; 30&gt; patientdata[result,] patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor patientdata$age &gt; 30 提取出来的索引值顺利能够放入数据框 [] 的逗号前面是因为我们之前提到过，数据框每列是等长的。想象下，我们有 4 个观测，我们我们用 patientdata$age 提取出来的，实际上是一串有 4 个值的向量，我们对向量进行了逻辑运算符，然后得到了 4 个 TRUE 或者 FALSE值，然后我们就可以把这些 TRUE 或者 FALSE 值和我们的观测一一对应。从而提出我们想要的观测。 事实上，在利用索引提取的时候，我还犯了个小错误，就是把索引输入到了错误的数据框里面，但并没有报错。 123456789&gt; test2[result,] control low middle high2 22.91 24.74 28.67 37.944 19.34 19.66 30.28 27.946 23.79 29.10 23.47 34.238 18.53 18.64 29.62 29.1310 20.14 25.49 34.64 36.1512 19.36 22.69 29.22 24.0714 24.13 20.36 35.12 35.24 这个故事告诉我们的是，索引得到的只是一串数字，他并不跟你产生这个索引结果的数据集有一毛钱的关系。 不要认为 R 的命令是黑箱，一步步地去拆解命令，你就可以很清晰地理解。 如果我们想要两个条件呢，即年龄大于30岁，且犯的是 Type I 型糖尿病呢。年龄大于 30 用的是 &gt; ，I 型糖尿病用的是等于 == ,那且是什么呢。就是我们之前提到的与或非了。 运算符 描述 x | y x或y x &amp; y x和（且）y 非的话是 !，不等于是 != 。不过我们估计是用不到的，所以我这里也就不讲了。 再次来提取我们想要的观测 123456789101112# 先得到索引&gt; patientdata$age &gt; 30[1] FALSE TRUE FALSE TRUE&gt; patientdata$diabetes == "Type1"[1] TRUE FALSE TRUE TRUE&gt; patientdata$age &gt; 30 &amp; patientdata$diabetes == "Type1"[1] FALSE FALSE FALSE TRUE# 提取&gt; patientdata[patientdata$age &gt; 30 &amp; patientdata$diabetes == "Type1",] patientID age diabetes status4 4 52 Type1 Poor 我们还可以在提取我们想要的观测的同时，提取一部分变量（列）出来 1234&gt; patientdata[patientdata$age &gt; 30,c("age","status")] age status2 34 Improved4 52 Poor 利用subset来提取 前面的那番操作大家可能会感觉写的有点长，那有没有一些简写呢，事实上是有的。你可以利用 R 基本包的 subset 函数来进行跟上面一模一样的操作。 别忘了用 ？ 来看看这个函数 有些人可能会提到用 attach 这个函数把数据框添加到 R 的搜索路径中，但实际上我不太推荐这样，因为一旦你要完成有许多个数据框的作业，而你又忘了detach，那么很有可能造成你不同数据框的不同变量之间的混淆。 subset 第一个要输入的参数是你的数据框，第二个要输入的参数是你对于观测（行）的筛选，可以用逻辑运算符串联，第三个可选择输入的是你要选择的列（变量）。跟之前一样的筛选条件，不过这次用的是 subset 函数。 123456789101112131415# 年龄大于30岁&gt; subset(patientdata, age &gt; 30) patientID age diabetes status2 2 34 Type2 Improved4 4 52 Type1 Poor# 年龄大于30，且 I 型糖尿病&gt; subset(patientdata, age &gt; 30 &amp; diabetes == "Type1") patientID age diabetes status4 4 52 Type1 Poor# 年龄大于30，且 I 型糖尿病的病人的年龄和病情&gt; subset(patientdata, age &gt; 30 &amp; diabetes == "Type1",c("age","status")) age status4 52 Poor]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part4]]></title>
    <url>%2F2019%2F05%2F10%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part4%2F</url>
    <content type="text"><![CDATA[上一节我们讲到了一些向量提取的操作。这一部分我们会讲一些数据框提取的操作。在R里面，数据框提取的基础操作跟向量很相似，还是以 [] 符号为基础。不过由于数据框是一个二维的数据结构，即有行与列，所以我们要以 dataframe[行索引, 列索引] 这种操作来提取数据框中的元素。 由于生统的数据一般只有两列，且比较长，可能不太适合演示，所以我这会用的还是之前糖尿病人的那个数据框。 1234567891011&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor 这次的内容参考了《R语言实战》的 4.10 部分，推荐大家可以去看一看。 数据框列的提取 根据坐标提取 跟我们之前介绍的向量读取差不多，也是可以利用坐标来读取。我们用坐标来提取第二列，即病人的年龄。 12&gt; patientdata[,2][1] 25 34 28 52 我们前面提到，逗号前面部分代表的是行索引（坐标可能更直白一点，但索引可能更正式一点，我还是用索引吧），逗号后面代表你要提取的列的索引。如果行的索引信息是缺失的话，那么就 R 就会默认你选择所有行。所以这里提取出了所有的 4 个病人的年龄。 根据列名提取 但这种坐标信息的提取是比较麻烦的，尤其是对于很多列的数据。你可能数不清你想要的那一列在第几列上。事实上，由于数据框的特殊数据格式，对于列的提取，也有了特殊的提取方式。因为数据框是一个二维数据结构，有行有列。意味着我们有行名和列名。所以在提取列的时候，我们也可以把列名放入原本数字索引在的地方，也可以达到跟数字索引提取同样的效果。 12&gt; patientdata[,"age"][1] 25 34 28 52 这里我们知道了我们的列名叫 age ，所以我们就把 age 加引号放入原本坐标在的那个地方，也顺利地提取出来了第二列，即病人的年龄。 $符号提取 数据框的提取还有一个简单的方式，就是在数据框后面加 $ 符号，然后就会自动跳出你的列名，你只要选择你想要的列名也可以顺利的提取出来。 12&gt; patientdata$age[1] 25 34 28 52 提取多列 多于多列数据的提取，还是用到我们上面提到的提取方法。 1234567891011121314151617# 数字索引提取&gt; patientdata[,c(2,4)] age status1 25 Poor2 34 Improved3 28 Excellent4 52 Poor# 列名提取&gt; patientdata[,c("age","status")] age status1 25 Poor2 34 Improved3 28 Excellent4 52 Poor# $符号可能不行 事实上，在提取列的时候，还有一个小问题，可能大家并没有发现。就是我们在提取单列的时候，得到的是一个向量型的结果，但我们在提取多列的时候，得到的是一个数据框型的结果。 12345&gt; class(patientdata[,"age"])[1] "numeric"&gt; class(patientdata[,c("age","status")])[1] "data.frame" 这是因为 R 会在你提取单列的时候，自动将得到的单列进行降维，将数据框变成向量。如果你并不想让数据进行降维，可以设置 drop = F。 123456789&gt; patientdata[,"age",drop = F] age1 252 343 284 52&gt; class(patientdata[,"age",drop = F])[1] "data.frame" 另一种在提取单列的时候，不降维的方法，就是你不遵循 [行索引，列索引] 这种格式，而是直接输入列索引，就像下面那样 12345678910111213141516&gt; patientdata[2] age1 252 343 284 52&gt; patientdata["age"] age1 252 343 284 52&gt; class(patientdata["age"])[1] "data.frame" 我个人不太推荐在生统做的时候用这个方式，还是建议老老实实按逗号的方式来提取，因为这样可能比较符合你的编程习惯。 但这种提出单列数据，自动降维的情况，对于我们生统是比较好的。因为像你后面做正态性检验等等，本质上你输入的应该是一串数值型的向量，而非是一个数据框。 数据框行的提取 行的提取跟列的提取很像，无非就是索引放在逗号前面。 123456789# 根据数字索引&gt; patientdata[1,] patientID age diabetes status1 1 25 Type1 Poor# 根据行名&gt; patientdata["1",] patientID age diabetes status1 1 25 Type1 Poor 大家可能会感到疑惑，1 和 "1" 难道不是一个东西么，事实上并不是。由于我们这里并没有给数据框赋予常见的那种行名，所以 R 会自动以数字即行号作为数据框的行名。所以 "1" 代表的其实是行名。这里为了更加清楚地展示，我们用人名来表示糖尿病人的行名。 1234567891011121314# rownames可以赋予行名rownames(patientdata) &lt;- c("Paul","James","Wade", "Antony")&gt; patientdata patientID age diabetes statusPaul 1 25 Type1 PoorJames 2 34 Type2 ImprovedWade 3 28 Type1 ExcellentAntony 4 52 Type1 Poor# 根据行名&gt; patientdata["Paul",] patientID age diabetes statusPaul 1 25 Type1 Poor 其实生统的作业不太会让你对某一行进行提取，更多的是提取出符合某一条件的几行来。这个就要涉及到我们之前讲到过的逻辑运算符了，这部分我们下一节再讲。 行列的提取 学会了提取列，也学会了提取行，行列就是你在 [] 里面，逗号前后都加上索引。但这个可能在生统中用处不大。 1234&gt; patientdata[1:2,2:3] age diabetes1 25 Type12 34 Type2]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part3]]></title>
    <url>%2F2019%2F05%2F09%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part3%2F</url>
    <content type="text"><![CDATA[因为生统中经常需要用到一些数据的提取，比如提取某一处理来做正态性检验等等。这些数据的提取本质上就是对某一行或者某一列的提取。所以这一部分我们来讲讲常见的数据提取。 R 里面的逻辑运算符 在讲数据提取之前，我们可能需要先了解一些逻辑运算符的基本知识。只有掌握了这些基本知识，才可以在后面灵活地提取出你想要的数据。 这一部分的内容参考了《R语言实战》的 4.3 部分，推荐大家去看看看 我们生统用到的逻辑运算符通常是大于，小于以及等于。符号分别是 运算符 描述 &lt; 小于 &lt;= 小于等于 &gt; 大于 &gt;= 大于等于 == 等于（注意等于并不是 = ，而是 == 。因为一个等号表达的是赋值或者传入参数） 当你利用逻辑运算符讲一个向量与数字进行比较的时候，R 就会返回给你 TRUE 或者 FALSE。 123&gt; vector_0 &lt;- c(1,2,3,4)&gt; vector_0 &gt; 2[1] FALSE FALSE TRUE TRUE 可以看到，凡是大于 2 的，都标明了 TRUE 。值得一提的是，等于不仅仅可以跟数字进行比较，还可以跟字符串进行比较。这在后面对数据框进行数据提取的时候，很有帮助。 12345&gt; vector_1 &lt;- c(rep("A",2),rep("B",5))&gt; vector_1[1] "A" "A" "B" "B" "B" "B" "B"&gt; vector_1 == "A"[1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE 实际上，R 里面还会有与、或、非等逻辑运算符。这对于数据框的提取也是很有帮助的，这个留待我们后面再讲。 向量的数据提取 讲完了逻辑运算符，我们就可以来提取数据了。我们之前介绍了两种生统常见的数据格式，一种是向量，另一种是数据框。我们这次先讲讲如何对向量来进行数据提取。 直接利用坐标提取 在 R 中最基本的数据提取手段就是利用 [] 这个符号。而在利用 [] 这个符号的时候，最简单的提取方式就是根据坐标进行提取了。我们先来尝试一下。 123456789101112131415161718# 创建一个向量&gt; vector_2 &lt;- c(1:10)&gt; vector_2 [1] 1 2 3 4 5 6 7 8 9 10 # 让我们提取第1个数据，注意 R 是以 1 开头的，而不是以 0 开头的。&gt; vector_2[1][1] 1# 提取第2,3,4个数据&gt; vector_2[2:4][1] 2 3 4# 提取第2,5个数据&gt; vector_2[2,5]Error in vector_2[2, 5] : incorrect number of dimensions&gt; vector_2[c(2,5)][1] 2 5 可以看到，我们在一开始提取 2,5 的时候，R 给了我们报错。是因为向量是一个一维的数据结构，而 [2,5] 这种提取适合的是数据框这种二维的数据结构，这一点我们在后面提取数据框数据的时候会提到。 简单来说，对于向量这种一维数据结构的提取，你并不能在 [] 里面使用逗号。所以，你如果想要提取不连续的坐标，就可以把不连续的坐标变成向量的形式放入 [] 里面。 利用which命令来提取 利用坐标的方式来提取有时候局限性会很大，因为有时候数据会很乱，利用坐标提取并没有什么用。比如下面的数据 1234# sample等命令我们会在后面生统常见的命令那边提到&gt; vector_3 &lt;- sample(1:100,10)&gt; vector_3 [1] 31 24 61 36 65 44 60 3 74 8 如果我们想要提取这里面大于60的数字，我们用肉眼观察，然后得到坐标的方式就比较麻烦。这时候我们就可以让 R 来代替我们找到那些大于 60 的数字的坐标。 这里我们用到的是 which 命令。 12&gt; which(vector_3 &gt; 60)[1] 3 5 9 这样我们就得到了大于 60 的数字的坐标了。然后再传入 [] 里面，这样就可以跟之前利用坐标一样来提取数据了。 12&gt; vector_3=3[which(vector_3 &gt; 60)][1] 61 65 74 利用TRUE和FALSE来进行提取 除了用 which命令来提取，我们还可以利用 TRUE 和 FALSE 来进行提取。 1234&gt; vector_3 &gt; 60 [1] FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE FALSE&gt; vector_3[vector_3 &gt; 60][1] 61 65 74 因为 TRUE 在 R 中和 T 是等价的，后面加参数的时候也是同理的。所以我在后面就会用 T 代表 TRUE了，FALSE 同理。 对于 TRUE 和 FALSE 这个类型的结果来说，有一个小彩蛋。就是我们可以把 T 和 F 传入 mean 和 sum 里面。 1234567# 统计有多少是大于 60 的。&gt; sum(vector_3 &gt; 60)[1] 3# 统计有百分之多少是大于 60 的。&gt; mean(vector_3 &gt; 60)[1] 0.3 可以看到，有 3 个数据是大于60，有 30% 的数据是大于60的。这对于大量数据的整体描述是一个非常好的小技巧。 参考文章： 《R语言实战》4.3 下一节我会讲讲如何对数据框进行提取操作。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part2]]></title>
    <url>%2F2019%2F05%2F08%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part2%2F</url>
    <content type="text"><![CDATA[R里面有许多数据类型，跟生统课相关的就是数值型，字符型，逻辑型了。而R也有很多数据结构，包括标量、向量、矩阵、数组、数据框和列表等。跟生统课上相关的就是向量、数据框这两种了。 后面的一些内容会借鉴《R语言实战》中的内容，推荐大家可以去看看这本书的2.1 ，2.2部分。 向量 向量是用于存储数值型、字符型或逻辑型数据的一维数组。 同一向量中无法混杂不同模式的数据。 即不能把数值型、字符串型、逻辑型的混起来放入同一向量中。 让我们来创建一个向量 123456789# 创建向量a &lt;- c(1, 2, 5, 3, 6, -2, 4)b &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)c &lt;- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE)# 查看向量的数据类型class(a)class(b)class(c) 对于数值型的向量创建，使用 : 可以帮助我们直接创建多个数字。这一点对于我们后面在数据框里面提取数值很有帮助。 12&gt; c(1:10) [1] 1 2 3 4 5 6 7 8 9 10 如果想要重复地创建某些值，就可以考虑 rep 函数。这一点对于后面我们给数据框添加列，或者添加列名行名可能会有帮助。 12345# 可以用?rep来查询其具体用法&gt; rep("A",3)[1] "A" "A" "A"&gt; rep(c("A","B"),3)[1] "A" "B" "A" "B" "A" "B" 如果想要间隔地创建数字，可以考虑用seq 1234# 隔3个数创建数字# 起始数字为1，终止数字为13，间隔为3个数字&gt; seq(1,13,3)[1] 1 4 7 10 13 数据框 数据框是一个二维数据结构，有行和列。一般来说，我们会将行表示观测，列表示变量 数据框可以放入不同类型的文件 一般来说，生统中的数据框是不需要自己创建的，只需要读入就行。用 read.table 读进来就已经是个数据框了。 123&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T)&gt; class(test1)[1] "data.frame" 12345678&gt; head(test1) yield seed1 383 12 406 13 351 14 400 15 390 16 361 1 我们可以对这个数据框进行一些探索。 首先看下这个数据框是几行几列的 123456&gt; dim(test1)[1] 29 2&gt; nrow(test1)[1] 29&gt; ncol(test1)[1] 2 发现是一个 29 X 2 的数据框。然后我们可以看下我们数据框的行名和列名是什么。 12345678# 提取行名&gt; rownames(test1) [1] "1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19"[20] "20" "21" "22" "23" "24" "25" "26" "27" "28" "29"# 提取列名&gt; colnames(test1)[1] "yield" "seed" 我们也可以对行名和列名进行更改 123&gt; colnames(test1) &lt;- c("A","B")&gt; colnames(test1)[1] "A" "B" 当然我们也可以自己来创建一个数据框。用到的是 data.frame 函数。 12345678910111213141516171819202122# R语言实战的例子&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status)&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor# 自己来一个例子&gt; data.frame(A = c(rep(1,2),rep(2,2),rep(3,2)), B = rep("test", 6)) A B1 1 test2 1 test3 2 test4 2 test5 3 test6 3 test 需要注意的是，数据框跟列表不一样，数据框里面的每一列都必须是等长的。像我这里就是 2 个 A ，2 个 B ， 2 个 C ，再加上 6 个 test 。 如果不等长就有可能会报错。也有可能不报错，用 NA 或者其他的值填充了。这个后面可能会提到。 因子 在我看来，因子的作用是为了对变量进行分类。就比如我们在做AONVA分析的时候，我们会做多种处理，那么我们就可以认为这些处理每个都是一类。拿上面的例子举例。 123456&gt; patientdata patientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 Poor 这里的糖尿病类型 Diabetes，有两种类型，分别是 Type1 和 Type2 。病情Status 有三种类型，分别是 poor、 improved、 excellent。所以这两列所含有的数据就是因子型的数据。 值得注意的是，R在构建数据框的时候，会自动将所有字符串类型的值转换成因子。我们可以看下 12345678&gt; patientdata$diabetes[1] Type1 Type2 Type1 Type1Levels: Type1 Type2&gt; patientdata$status[1] Poor Improved Excellent Poor Levels: Excellent Improved Poor&gt; patientdata$patientID[1] 1 2 3 4 如果这里有 Levels ，就代表这里的数据是因子。可以看到，patientID由于是数值型的变量，所以并没有自动地转换成因子。 我们同样也可以用 class 来看下类别。 12345&gt; class(patientdata$diabetes)[1] "factor"&gt; class(patientdata$patientID)[1] "numeric" 但有时候，字符串变量自动转换成因子也不是所有都对的，比如一开始我们有name这一列。 1234567891011121314151617181920&gt; patientID &lt;- c(1, 2, 3, 4)&gt; age &lt;- c(25, 34, 28, 52)&gt; diabetes &lt;- c("Type1", "Type2", "Type1", "Type1")&gt; status &lt;- c("Poor", "Improved", "Excellent", "Poor")&gt; names &lt;- c("Paul","James","Wade", "Antony")&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status,names)&gt; patientdata patientID age diabetes status names1 1 25 Type1 Poor Paul2 2 34 Type2 Improved James3 3 28 Type1 Excellent Wade4 4 52 Type1 Poor Antony# 看下 names 的类别&gt; class(patientdata$names)[1] "factor"&gt; patientdata$names[1] Paul James Wade AntonyLevels: Antony James Paul Wade 我们会发现 R 自动地将 names 这一列也变成了因子。但实际上，名字是独一无二的，并不是一个分类变量。所以，我们不应当将其变成一个 factor 。不过，你会发现，如果是这一列是后添加上去的，就不会自动转成因子。 123456789101112# 这个操作可以自动加上一列名为name_new的列patientdata$names_new &lt;- c("Paul","James","Wade", "Antony")&gt; patientdata patientID age diabetes status names names_new1 1 25 Type1 Poor Paul Paul2 2 34 Type2 Improved James James3 3 28 Type1 Excellent Wade Wade4 4 52 Type1 Poor Antony Antony&gt; patientdata$names_new[1] "Paul" "James" "Wade" "Antony" 让我们再来看下我们在第五次生统作业的第一题的数据。 1234&gt; class(test1$yield)[1] "integer"&gt; class(test1$seed)[1] "integer" 明明我们的seed代表的是处理类别，为什么却不是一个因子呢。因为 seed 那一列是数值型的变量，所以 R 并不会自动地将其转换成因子。但如果不转换成因子的话，就可能会在后续的分析中出现一些问题。所以我们可以用 factor 函数，来将其转换成因子。 123&gt; test1$seed &lt;- factor(test1$seed)&gt; class(test1$seed)[1] "factor" 如果想要R不自动地将字符串转换成因子，可以 123456&gt; # 读数据的时候，设置&gt; test1 &lt;- read.table("rawdata/test1.txt",header = T,stringsAsFactors = F)&gt; &gt; # 自己构建数据框的时候，设置&gt; patientdata &lt;- data.frame(patientID, age, diabetes, status,names,stringsAsFactors = F)&gt; 参考文章： 《R语言实战》第二章 如何理解R中因子(factor)的概念 中猴子的回答]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给女朋友写的生统资料_Part1]]></title>
    <url>%2F2019%2F05%2F08%2F%E7%BB%99%E5%A5%B3%E6%9C%8B%E5%8F%8B%E5%86%99%E7%9A%84%E7%94%9F%E7%BB%9F%E8%B5%84%E6%96%99-Part1%2F</url>
    <content type="text"><![CDATA[因为深感生统的节奏比较快，可能女票跟不上节奏，所以写了一个简略的manual，只针对生统的一些相关操作，不涉及高深的R操作。如果大家觉得还要加什么东西，可以在下面留言。 前期准备 语言问题: 对于 R 或者 R studio来说，我非常建议把语言更改成英文。这样，在你报错的时候，比较方便去搜索 工作路径： 对于有R studio的来说，频繁地切换 setwd 和 getwd 可能不是一个很好的选择。所以我比较推荐新建一个Project，这样你每次你的任务都会是独立的。新建完Project之后你就可以把作业相关的数据放在你的Project里面。在后面读取的时候，就不用切换 setwd 或者打一大串目录了。 保存问题： 我比较推荐的是在Tools-Global Options—General那里，将Save Worksapce to .RData on exit那里设置为Never。这可能会导致你每次打开你的Project，变量都还得重新打一遍。但这可以保证你的代码的可重复性。 镜像及安装问题： 生统课上的如果包都很小，所以镜像设置其实是无所谓的。 如果想安装某个包的话，使用如下代码 12## 以安装pwr包为例，注意加引号#install.packages(&quot;pwr&quot;) 数据读取 生统课上用到的文件一般给的都是 txt 或者 csv 文件，这意味一般着只需要使用 read.table 这个命令来读取文件就可以了。 让我们先来看一下read.table这个函数怎么用。 1?read.table 不懂的时候寻求谷歌或者?+命令，是一个很好的习惯 你会发现read.table()里面跟了一大堆东西，其中跟我们可能相关的是 file：代表你要读的文件路径 header：表达你是否要添加表头，默认值是FALSE，我们一般要设置为TRUE sep：sep代表是你用什么样的形式来分割你读取的文件，一般生统的文件可能会以空格，制表符，逗号来分割。分别对应sep = " "，sep = ","，sep = "\t" 我们来尝试读一个文件 12test1 &lt;- read.table(&quot;rawdata/test1.txt&quot;,header = T)head(test1) 1234567## yield seed## 1 383 1## 2 406 1## 3 351 1## 4 400 1## 5 390 1## 6 361 1 这里我们用了 header = T ，这样我们的数据就会有表头，或者说列名了。即 yield 和 seed。 TRUE和T是等价的，同理FALSE和F也是等价的。 csv是本质上是用逗号分割的文件，所以我们在读的时候加上 sep = "," 即可。 head代表的是你只输出你数据的前几行。同理，tail输出后几行。 再次提醒一遍，感觉不懂命令是什么时候用?或者谷歌。 你还可以用row.names=1，来将第一列当作行名。]]></content>
      <categories>
        <category>生物医学统计课</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MACS2的原理介绍]]></title>
    <url>%2F2019%2F02%2F17%2FMACS2%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[泊松分布 泊松分布是统计与概率中重要的离散分布之一，泊松分布表示在一定的时间或空间内出现的事件个数，比如某一服务设施在一定时间内受到的服务请求的次数、DNA序列的变异数、汽车站台的等候人数。根据MACS的论文中所描述的，Chip-Seq实验中全基因组的reads分布恰好是符合泊松分布的。 泊松分布的概率分布为 \[ P(X = k)=\frac{e^{-\lambda}\lambda^{k}}{k!} \] 其中e代表的是自然常数，而\(\lambda\)是单位时间（或单位面积）内随机事件的平均发生率，比如在一定时间内某一服务设施受到的请求次数是5次。 另外，泊松分布实际上只有一个参数，即\(\lambda\)，其方差和期望也是\(\lambda\)。同时，随着\(\lambda\)的增加，图像分布会趋于对称。 参考资料 二项分布、泊松分布、正态分布的关系 泊松分布和指数分布：10分钟教程 wiki_泊松分布 MACS的算法概览 Adjusting read position based on fragment size distribution Chip-Seq的主要过程为：交联——超声破碎——特异性识别——测序。所以我们测序得到的片段就是我们转录因子结合位点周围的片段。需要注意的一点是，MACS软件出现的年代是2008年，那时候的测序读长都很短，大约50bp左右，且以单端测序为主，并没有真实反应DNA-蛋白结合片段的长度。所以说，我们如果拿测得的50bp去做reads数目的堆积，势必会与真实的结合位置有一定的偏移。事实上，测序的短reads会在真实的结合位置两侧形成双峰，如下图所A示。这也是MACS双峰模型构建的理论基础。 值得一提的是，像转录因子一类的蛋白与DNA，其结合位点比较narrow，所以双峰模型的构建是比较合理的。但像图B所示的，一些蛋白与DNA会产生较宽的结合区域（诸如一些组蛋白修饰），这时候双峰就不那么显著了。 更为麻烦的是，有时候会有一些混合的结合位点模式，比如Polll蛋白，其会在启动子区域结合，也会覆盖整个基因区域。 为了衡量真实的测序片段大小，d，MACS会粗略地以2倍的超声破碎片段长度作为window来鉴定初步的富集区域。为了避免重复区域或者PCR导致极端富集区域的影响，MACS会随机挑选1000个区域作为模型peak构建区域。这些区域的reads富集程度是基因组背景的10-30倍。对于每个区域的模型peak，MACS都会分离出对比到正链和负链上的reads，然后分别计算出这些reads的位置。从而分别构建出这个区域内的正负链上的模型peak，正负链上模型peak顶点之间距离就记为d。在d确定之后，所有的reads都会朝着3'的方向横移（shift）d/2的距离，从而更好地模拟出蛋白-DNA结合位点。 在2012年的Identifying ChIP-seq enrichment using MACS这篇文章中，作者也提到对于一些过度破碎或者有着很宽的结合位点情况，可能会造成算出来的d很小。对于这种情况，我们一般建议用一个特定的片段长度，而非是预测出来的d。 注意shift和extend的区别，在2008年原始的MACS文章中，作者用的是shift，而到了12年的文章，作者写错，写成了extend。当然，在MACS2中，这两种情况都存在了。 Calculate peak enrichment using local background normalization 基于先前已经调整位置的reads，MACS会在全基因组范围内以2d长度的window来寻找那些有显著富集的区域。有重叠的window会融合成一个候选区域。因为会有许多因素影响不同范围内的reads富集程度，所以MACS用了动态的\(\lambda_{local}\)参数来对于reads数目的富集进行泊松分布的建模。即MACS并不会用一个常数\(\lambda\)，而是用一个会在不同区域有变化的\(\lambda_{local}\)。动态参数值定义为 \[ \lambda_{local}=max(\lambda_{BG},[\lambda_{region},\lambda_{1k}],\lambda_{5k},\lambda_{10k}) \] \(\lambda_{BG}\)来自于全基因组的计算，\(\lambda_{region}\)则来自在control中的对应区域，剩下的\(\lambda_x\)则来自control中，以得到的候选区域为中心，1k，5k，10k范围内的区域计算。见下图 lambda 如果control不在，则local值只是在Chip的样本中计算，而region和1k值也会被舍弃。同时如果Chip-Seq和control的样本测序深度不同，MACS会默认地把测序深度更深的样本缩放。 关于\(\lambda\)以及p值这一步的计算可能需要看源代码才可以了解了。 但根据MACS2的wiki来说，似乎p值和\(\lambda\)的计算都是以单个碱基为单位考虑的。 基于泊松分布的模型，我们就可以以单尾检验，计算出p值了。MACS默认以p=1 x 10-5为阈值。 Estimating the empirical false discovery rate by exchanging ChIP-seq and control samples 这里MACS用的Chip和control的置换，从而检验出FDR值我并没有看懂。不过MACS2用的已经是Benjamini-Hochberg方法了，还是比较好懂的。 参考资料： Evaluation of Algorithm Performance in ChIP-Seq Peak Detection Model-based Analysis of ChIP-Seq (MACS) Identifying ChIP-seq enrichment using MACS In-depth-NGS-Data-Analysis-Course MACS2中的一些参数介绍 -f/--format FORMAT 可以接受多种格式参数，默认使用AUTO来检测格式。但并不能检测“BAMPE”或者“BEDPE”格式，即双端测序格式。所以，当你的数据是双端测序数据时，你应该用BAMPE或者BEDPE参数。当你设置成双端参数的时候，MACS2就会跳过建模计算d的那一步，而是直接用片段的insert size来建立堆积。 --extsize 如果使用这个参数，那么MACS就会使用你设置的数值，来把reads从5‘—3’补齐到你指定的数值。这个参数只有当--nomodel参数设置了，或者MACS建模失败，--fix-bimodal开启的时候才可以用。 --shift shift参数会先于extsize参数执行。如果你设置的数值为正，reads会从5‘—3’偏移，而数值为负，reads会从3'—5‘偏移。当格式为BAMPE或者BEDPE的时候，不能设置参数。 --broad 会放宽cutoff的阈值，然后把临近的区域结合起来，形成较宽的peak区域。与broad-cutoff参数是一起的，broad-cutoff参数默认为q-value的参数，为0.1。 有趣的是，shift后面数值如果为正，则正负链的reads会朝着中心偏移，如果后面数值为负，则正负链的reads会各自远离中心，即正链reads向左，负链reads向右。 给个例子： 1234567891011Original Reads:chr1 500 550 read1 . (+)chr1 700 750 read2 . (-)--shift -100chr1 400 450 read1 . (+)chr1 800 850 read2 . (-)--extsize 200chr1 400 600 read1 . (+)chr1 650 850 read2 . (-) 参考资料： MACS_github google_group 如何使用MACS进行peak calling]]></content>
      <categories>
        <category>算法原理</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我刷过的统计学资料]]></title>
    <url>%2F2019%2F02%2F13%2F%E6%88%91%E5%88%B7%E8%BF%87%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[感觉大家的统计学习兴趣都很浓，我这里也把我之前刷的一些统计资料说下。 说人话的统计学 在协和八微信公众号上连载。我认为这套教程是在学完基本的统计学知识之后，非常好的进阶教程。在看完这套教程后，很多之前学过的模糊知识点都会逐渐明晰。值得一提的是，两位作者都是有丰富数据分析经验的学者。可能也正是这样，这套教程并不局限于书本内容，更多地从实际出发。 女士品茶 在没有基本的统计学知识之前，不建议读这本书。可能你读了半天，会发现这整本书就讲了XX干了XX事。但等你的统计学到一定程度，再看这本书，就会有种越看越开心的感觉吧。不过这本书对于统计学本身功底的提升可能极为有限。 可汗学院公开课：统计学 总共85集，每集大约10分钟不到，老师讲的非常有趣，非常适合统计学入门。里面几乎不涉及公式推导，就讲了统计学中最常见的 随机变量、均值方差标准差、统计图表、概率密度、二项分布、泊松分布、正态分布、大数定律、中心极限定理、样本和抽样分布、参数估计、置信区间、伯努利分布、假设检验和p值、方差分析、回归分析等内容。 基本上刷完你就知道了大致的概念了。不过由于每集比较短，里面的解释可能比较含糊，但非常适合初步地过一遍。 深入浅出统计学 可以说是图文并茂吧，但有点太过于概括了。感觉适合快速地扫一遍。就我个人而言，这种过多地图片可能影响我思考问题了。。。 欧姆社的漫画统计学 额，怎么说呢，感觉适合没接触过统计学的，可能不太适合研究生了。毕竟漫画太多了。 概率论与数理统计（茆诗松版） 值得强推！讲的非常非常细，公式推导非常地详细，可能刷过这本才是真正意义上地去学统计了。不过感觉有点难刷，我看到最后很多公式都是跳着看的。 看完这本书你才发现Fisher、Pearson有多厉害。 概率论与数理统计（陈希孺版） 也非常推荐！但不建议当入门统计的去看这本书，最好地状态应该是先去看茆诗松版的，然后感觉有些概念模糊，觉得讲的不清楚去对照着看陈希孺院士这本书，两相映证，可能效果更好。 统计学习导论-基于R应用 这本书虽然叫统计学习导论，但实际上跟我们常见的统计关系不大。更多地是好像是机器学习的一些基础内容，但胜在公式推导不多，所以还是能看进去一点的。推荐和李程的基因组学课程一起看，你就会知道这里面的知识对于生命科学的研究的重要性了。 李程的基因组学生信技能树也推过的。 这本书就像是常见的统计学教科学的后续延伸，因为统计教科学一般都是以线性回归结束的，而这本书恰好是以线性回归开头的。 刷完这些课程的一些感悟 真正有帮助地可能还是教科书而不是比较入门的书籍，因为教科学会有严谨的推导，而一些入门书籍可能为了趣味性会放弃一些严谨性，而只告诉你一些描述性质的东西。 对于一些优秀的偏概括性质或者历史传记性质的书籍，比如《女士品茶》，《统计学七支柱》，《赤裸裸的统计学》，《数理统计学简史》等等，可能并不应该作为入门读物，而是应该作为刷完教科学之后再去翻阅的书。不然你看完这些书，可能最终留下的结论还是某年某月，某某人干了个什么事。又或者你在津津有味地看书的时候，看到作者提出的公式，会立刻跳过，而不是细细琢磨这个公式在这个背景下的作用。 有道是纸上得来终觉浅，可能下一步我的计划就是去努力地将自己学到的统计学知识跟自己平常遇到的生命科学结合起来，来更好地去学以致用。]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mfuzz的使用]]></title>
    <url>%2F2019%2F01%2F06%2FMfuzz%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[最近在进行ATAC和RNA-Seq的联合分析，由于处理的材料是时间相关的，所以time-course也是一个可以分析的点。在一位刘潜老哥的帮助下，我找到了一篇靠谱熊 转录组时间序列数据处理 的文章，里面提到了mfuzz这个包。里面的软聚类的思想非常符合我的预期，然后就决定拿这个包进行我time-course的分析。为了更好的分析，我决定先翻译下这个包，了解下这个包的大致思想。 1 Overview 这部分是我偷懒的随便写的。。。。。。 感觉time-course的方法一般就是聚类。常见的聚类分为三种，分别是层次聚类（Hierarchical Clustering）、硬聚类（hard clustering）、软聚类（soft clustering）。层次聚类好像一般就是像热图那种。看了pheatmap的文档，感觉pheatmap就是层次聚类，当然你可以设置k_means，变成硬聚类。硬聚类常见的就是k-menas。软聚类就是我们这会要用到的这个包的核心思路。 2 Installation requirements 见Bioconductor的安装方法。 3 Data pre-processing 数据集是来源于酵母细胞循环表达数据。6178个基因，横跨160分钟的17个时间点。用的是芯片数据。 12345678&gt; head(yeast@assayData$exprs) cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 cdc28_50 cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 cdc28_120 cdc28_130 cdc28_140 cdc28_150 cdc28_160YDR132C 0.19 0.30 -0.29 0.29 -0.31 0.23 0.20 -0.08 0.19 -0.51 0.00 -0.31 0.11 -0.02 0.20 0.36 -0.54YMR012W -0.15 -0.15 -0.04 -0.28 -0.39 0.03 0.22 0.04 -0.15 0.37 0.47 -0.10 -0.09 NA -0.04 0.07 0.19YLR214W 0.38 0.30 -0.68 -0.52 -0.43 -0.13 -0.17 0.26 -0.03 -0.34 -0.01 -0.20 0.10 NA 0.45 0.40 0.63YLR116W 0.17 0.06 -0.21 0.19 0.33 0.44 0.46 0.38 -0.15 -0.03 0.04 -0.42 -0.15 0.02 NA -0.51 -0.61YDR203W 0.85 -0.10 -0.56 -0.31 -0.43 0.00 -0.34 0.17 0.40 -0.37 0.15 0.24 0.24 0.17 -0.12 -0.02 0.02YEL059C-A 0.45 0.20 0.06 0.10 -0.21 -0.08 -0.27 -0.01 -0.29 0.41 -0.08 -0.22 -0.27 NA -0.30 0.25 0.26 3.1 Missing value 第一步，去除那些有超过25%数据缺失的基因。注意这些数据缺失值应该是设为NA。 12yeast.r &lt;- filter.NA(yeast, thres=0.25)49 genes excluded. 这里就如上面的数据一样，一行即一个基因有16个时间点的数据，如果16个时间点里面有25%，即4个时间点都是NA，则剔除这个基因。 123456&gt; nrow(yeast)Features 3000 &gt; nrow(yeast.r)Features 2951 Fuzzy c-means就像其他聚类算法一样，其并不允许有缺失值的存在。所以我们会对剩下那些缺失值（16个数据点里面就缺了1个2个那种）进行填充。用的是对应基因的平均表达值。 对于RNA-Seq来说，你可以加上一些pseudocount，比如0.01。 1yeast.f &lt;- fill.NA(yeast.r,mode="mean") 当然，你也可以用（weighted） k-nearest neighbour method。（mode='knn'/'wknn'）。这些方法相比较而言比上面这种简单的方法要好，但需要耗费更多的算力。 3.2 Filtering 许多已经出版的聚类分析包含过滤的步骤，从而来去除那些表达相对比较低的，或者表达不怎么变化的。通常来说，比较受欢迎的就是样本的标准差作为阈值。 1tmp &lt;- filter.std(yeast.f,min.std=0) 然而在基因低表达到高表达的过程中，变化是非常平缓的。所以给定阈值筛选并不一定是可靠的，可能是非常武断。因为现在并没有很多有说服力的筛选手段，所以我们还是避免对基因数据做提前的筛选。这可以避免损失一些有生物学重大意义的基因。 比如1,2,4,10,12,13,15。看起来变化很大，但方差可能并不如你想象中的那么大。 Standardisation 由于聚类是在欧几里德空间中进行的，因此基因的表达值被标准化为平均值为零，标准差为1。该步骤确保了在欧几里得空间中具有相似表达模式的基因是相互接近的。 1yeast.s &lt;- standardise(yeast.f) 重要的是，Mfuzz认为输入的表达数据是完全经过前期数据标准化的。standardise 并不能代替标准化步骤。注意差异：标准化是为了让不同的样品间可以比较，而Mufzz中standardisation则是让转录本或者基因间可以比较。 4 Soft clustering of gene expression data 聚类可以用来解释基因表达的调控机制。众所周知的，基因的表达并不是开和关的，而是一个逐渐变化的过程。一个聚类算法应该展现出一个基因有多么的符合dominant cluster pattern。软聚类应该是一个非常好的方法，因为其可以利用membership \(μ_{ij}\)衡量一个基因 i跟cluster j的关系。 其实就是说基因A跟每个cluster都有关系，无非是membership score的值不一样而已。 软聚类的mfuzz函数基于的是e1071包的fuzzy c-means算法。对于软过滤而言，聚类中心点\(c_j\)来源于所有聚类成员的权重值。在图中的membership值可以用mfuzz.plot来展现。你也可以用mfuzz.plot2来看，其会有更多的选项。 值得注意的是，clustering只会基于表达矩阵，不会使用phenoData的任何信息。还有，在mfuzz中重复会被当作是独立的信息，所以他们应该提前被算好平均值，或者放进不同的ExpressionSet对象里面。 12&gt; cl &lt;- mfuzz(yeast.s,c=16,m=1.25)&gt; mfuzz.plot(yeast.s,cl=cl,mfrow=c(4,4),time.labels=seq(0,160,10)) 123456789101112131415161718192021222324# center代表的应该是你选择的16个中心点的表达模式## 感觉可以用来画图&gt; head(cl$centers,2) cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 cdc28_50 cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 cdc28_120 cdc28_130 cdc28_140 cdc28_150 cdc28_1601 0.1971169 -1.0925729 -1.6203551 -0.7961482 -0.33954720 -0.1567524 -0.05036767 0.08380756 0.5122518 0.3843354 0.4905732 0.437668149 0.4526805 0.3170533 0.2866267 0.2890568 0.60457312 -0.7393245 -0.5872038 0.2438611 -0.1883262 0.03321276 -1.0122666 -0.38203192 -0.47328266 -0.6289479 2.1494891 0.5371715 -0.001270464 -0.5672875 0.2288121 0.2331435 0.5896372 0.5646142# size代表的是各个聚类的基因数目 &gt; head(cl$size,2)[1] 175 244# cluster代表的是基因所属的membership score最高的那个簇&gt; head(cl$cluster,5)YDR132C YMR012W YLR214W YLR116W YDR203W 4 11 16 13 16 # membership代表的是每个基因对应16个簇的membership值&gt; head(cl$membership,5) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16YDR132C 0.014386854 0.0023657075 0.0478663840 0.47749014 0.007117060 0.070156647 0.010004348 0.051767933 0.01711759 0.028929516 0.0064926466 0.011933880 0.110039692 0.025007449 0.033967103 0.08535705YMR012W 0.082025023 0.1807366237 0.0088059459 0.00371571 0.012467968 0.002590922 0.127711735 0.007948114 0.06412901 0.004626184 0.3408675360 0.105343965 0.008278282 0.011112051 0.012020122 0.02762081YLR214W 0.130082565 0.0047176611 0.0041702207 0.06382311 0.001340277 0.004636745 0.013298969 0.043267912 0.21204674 0.016651050 0.0850329333 0.023956515 0.010243391 0.003459240 0.022271137 0.36100153YLR116W 0.002047923 0.0008467741 0.0412749579 0.01409627 0.002758020 0.034171711 0.001234725 0.002968499 0.00083482 0.003580831 0.0006540104 0.003016454 0.846273635 0.023517377 0.012508494 0.01021550YDR203W 0.083941355 0.0008482787 0.0008575124 0.02562416 0.001318053 0.004043468 0.001274160 0.032185727 0.12237271 0.002649867 0.0125075149 0.003545481 0.005389429 0.001504605 0.007198611 0.69473907 123mfuzz.plot(eset,cl,mfrow=c(1,1),colo,min.mem=0,time.labels,new.window=TRUE)colo可以设置颜色，min.mem可以设置membership的阈值 4.1 Setting of parameters for FCM clustering 对于fuzzy c-means来说，模糊值m和聚类数c必须提前设置好。对于m，我们应该选择一个可以防止随机数据聚类的值。值得注意的是，fuzzy 聚类可以遵守这样的准则，随机数据并不能被聚类。这相比于硬聚类（例如k-means）来说，是一个明显的优点。因为其即使在随机数据中，也可以检测到cluster。为了达到这一点，你可以使用下列选项： partcoef函数，来检测是否在某一特定的m设置下，随机数也会被聚类 或者直接计算 12&gt; m1 &lt;- mestimate(yeast.s)&gt; m1 # 1.15 设置一个合理的聚类值c是很有挑战性的，尤其是那些short time series，很有可能就会有overlapping clusters。我们可以设置一个最大的c值，大到最后出现了一个空的empty clusters（看 cselection函数） 12345678# 不太懂repeat值代表了什么&gt; cselection(yeast.s,m=1.25,crange=seq(4,32,4),repeats=5,visu=TRUE) c:4 c:8 c:12 c:16 c:20 c:24 c:28 c:32repeats:1 4 8 12 16 19 24 27 31repeats:2 4 8 12 16 20 23 28 30repeats:3 4 8 12 16 20 23 28 32repeats:4 4 8 12 16 20 24 28 31repeats:5 4 8 12 16 20 23 27 32 在cluster centroid之间最小距离\(D_{min}\) 也可以作为簇有效指数。在这里，我们可以检测不同的c值之间的\(D_{min}\)。我们可以预期D.min在达到最合适值之后，下降幅度会变低。你也可以选择 4.2 Cluster score Membership值也可以暗示两个向量之间的相关性。如果两个基因对于一个特定的cluster都有高的membership score，那么他们通常来说表达模式是相似的。我们对于高于阈值α的基因，叫做这个cluster的α-core。 membersip score的设置通常可以作为基因的后验筛选。我们可以用acore函数。 12tmp &lt;- acore(yeast.s,cl,min.acore = 0.5)# 生成的似乎是个列表，里面有16个。就可以知道每个簇里面含有的基因ID了。 5 Cluster stability FCM参数的变化也可以体现出cluster的稳健性。我们认为那些稳健的clusters具有某个特征，即在m的变化下，也只会展现出很小的变化。 12cl2 &lt;- mfuzz(yeast.s,c=16,m=1.35)mfuzz.plot(yeast.s,cl=cl2,mfrow=c(4,4),time.labels=seq(0,160,10)) 6 Global clustering structures 软聚类有趣的一点就是clusters之间的overlap或者coupling。在cluster k和l之间的coupling coefficient \(V_{kl}\) 可以定义为： \[ V_{kl}=\frac{1}{N}\sum^{N}_{i=1}{\mu_{ik}}{\mu_{il}} \] N是整个基因表达矩阵的数目。如果coupling值越低，说明两者的表达模式距离越远。如果越高，说明表达模式越相近。 12O &lt;- overlap(cl)Ptmp &lt;- overlap.plot(cl,over=O,thres=0.05) 7 Mfuzzgui - the graphical user interface for the Mfuzz pack-age mfuzz有图形化界面，不过我没去用。 小结 最近期末考试复习太忙了。。。。有空再加上点注意事项。]]></content>
      <categories>
        <category>软件的使用</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
      </tags>
  </entry>
</search>
